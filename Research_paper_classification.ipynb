{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsoKkYvClAXMLUTsZcbdHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "802e62ee6a8845b4bd7ca69460f82024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15f85505d6524d3f953006869521e77b",
              "IPY_MODEL_18a47bbdb4de4d95a63bc427e8121a0e",
              "IPY_MODEL_580bc20fa0264457a950917ff1b40d74"
            ],
            "layout": "IPY_MODEL_218bfb6c6d47475387f7672c962c1d2f"
          }
        },
        "15f85505d6524d3f953006869521e77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_217fb8792dc746c098ff5e21e1058699",
            "placeholder": "​",
            "style": "IPY_MODEL_5bd25ede714f4831bf6c28395f184f89",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "18a47bbdb4de4d95a63bc427e8121a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01803c6246a04fc8a865182096f76cb6",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f289ae97de2e424d899843806aecbf08",
            "value": 48
          }
        },
        "580bc20fa0264457a950917ff1b40d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0386d1f90d49e0bbbca315685c4bc1",
            "placeholder": "​",
            "style": "IPY_MODEL_e5330c6c378544bfb954ee39683c3fb1",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.53kB/s]"
          }
        },
        "218bfb6c6d47475387f7672c962c1d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217fb8792dc746c098ff5e21e1058699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd25ede714f4831bf6c28395f184f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01803c6246a04fc8a865182096f76cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f289ae97de2e424d899843806aecbf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed0386d1f90d49e0bbbca315685c4bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5330c6c378544bfb954ee39683c3fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b36efa2c93e41ebabdf4fbc2e7acc3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69cfc072cd57474bbd53597aaba3dbb3",
              "IPY_MODEL_6b5e2f5ee05948a2b9417e48cfb1e198",
              "IPY_MODEL_c7b2efaacb9e475fb2c9cdf3dfbfdf09"
            ],
            "layout": "IPY_MODEL_cf0fb5460ad14bfe9df0b34a0324c093"
          }
        },
        "69cfc072cd57474bbd53597aaba3dbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adb415d1a9154a779005ee88e29156c3",
            "placeholder": "​",
            "style": "IPY_MODEL_b84d454709b54c178547c178f59fe817",
            "value": "vocab.txt: 100%"
          }
        },
        "6b5e2f5ee05948a2b9417e48cfb1e198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc879444be64f57bcec46dbe072279b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92b808ec8db142c3aca3a18d4e04b8c3",
            "value": 231508
          }
        },
        "c7b2efaacb9e475fb2c9cdf3dfbfdf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0ca0e1826848b9a63bf143e048fa18",
            "placeholder": "​",
            "style": "IPY_MODEL_4325ffb9f89642e19b428ae9bbeda44a",
            "value": " 232k/232k [00:00&lt;00:00, 2.68MB/s]"
          }
        },
        "cf0fb5460ad14bfe9df0b34a0324c093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb415d1a9154a779005ee88e29156c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84d454709b54c178547c178f59fe817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc879444be64f57bcec46dbe072279b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92b808ec8db142c3aca3a18d4e04b8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d0ca0e1826848b9a63bf143e048fa18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4325ffb9f89642e19b428ae9bbeda44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd5c8282724846d1a638fbf8a039ac5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b7a03411cca4386a497926c19497a19",
              "IPY_MODEL_a3a8cfb20d624832a359a05845d18a7d",
              "IPY_MODEL_9e8c23e94a0349fba412977eccc08780"
            ],
            "layout": "IPY_MODEL_382484fb77b948519572e3be2783b3d2"
          }
        },
        "7b7a03411cca4386a497926c19497a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5b6b41bddbe42369f38ab74009be0e9",
            "placeholder": "​",
            "style": "IPY_MODEL_fc8ae06d66f5423cb8a3a4ece8859fce",
            "value": "tokenizer.json: 100%"
          }
        },
        "a3a8cfb20d624832a359a05845d18a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dcd1a00fb04b11ab6b11e2f6bc6237",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44dcff6b2b1749e7bec2a47a7d27dc58",
            "value": 466062
          }
        },
        "9e8c23e94a0349fba412977eccc08780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a05fe12c39046c385fbd5c321181b97",
            "placeholder": "​",
            "style": "IPY_MODEL_e6d319bedb8e4c4791649190258a86d4",
            "value": " 466k/466k [00:00&lt;00:00, 5.42MB/s]"
          }
        },
        "382484fb77b948519572e3be2783b3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5b6b41bddbe42369f38ab74009be0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8ae06d66f5423cb8a3a4ece8859fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95dcd1a00fb04b11ab6b11e2f6bc6237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dcff6b2b1749e7bec2a47a7d27dc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a05fe12c39046c385fbd5c321181b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d319bedb8e4c4791649190258a86d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4027008bdc9a49d18534f2446ba099fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd68dca4e0994e5f83194758e535138e",
              "IPY_MODEL_fbb5c344c5bd4712aabce80f2e21b463",
              "IPY_MODEL_5632f46f4c414b688bc02f53fbf6334f"
            ],
            "layout": "IPY_MODEL_095b5130226b426eb36c398afe756680"
          }
        },
        "dd68dca4e0994e5f83194758e535138e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cfaa12885ce487da6f248a5c9cf9fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_b02dc160bbe24906907fdaf21eca8947",
            "value": "config.json: 100%"
          }
        },
        "fbb5c344c5bd4712aabce80f2e21b463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e4d6b38fc1414895f021f925064f44",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af66b1c24da64127b2073c1b9e12bba8",
            "value": 570
          }
        },
        "5632f46f4c414b688bc02f53fbf6334f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64a2cebd943341f7bd5976e6329e3728",
            "placeholder": "​",
            "style": "IPY_MODEL_d468b148ffde4fb3b78b99233f6a3b49",
            "value": " 570/570 [00:00&lt;00:00, 9.14kB/s]"
          }
        },
        "095b5130226b426eb36c398afe756680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cfaa12885ce487da6f248a5c9cf9fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02dc160bbe24906907fdaf21eca8947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8e4d6b38fc1414895f021f925064f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af66b1c24da64127b2073c1b9e12bba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64a2cebd943341f7bd5976e6329e3728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d468b148ffde4fb3b78b99233f6a3b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291f75fec7a74dffb5f5b3935fb72abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5604b04190c4cab98b063f9bfccfcb4",
              "IPY_MODEL_3b35816460da4b15a46bb900aac29288",
              "IPY_MODEL_20672f33882f4a3f815a7004063f9df8"
            ],
            "layout": "IPY_MODEL_b1579d6c3eb44e3293a0c2ed3e99d7d5"
          }
        },
        "f5604b04190c4cab98b063f9bfccfcb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365d446bba1b432ebf88b523deeed2b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e3dc171e0077470a89ca8ab075066672",
            "value": "model.safetensors: 100%"
          }
        },
        "3b35816460da4b15a46bb900aac29288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5a5e7bd39b34d188d485376e1388901",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_143726e860194d6e9dbeefb44f35c92a",
            "value": 440449768
          }
        },
        "20672f33882f4a3f815a7004063f9df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_300cfc75c5d343b48c65e3c264d929a7",
            "placeholder": "​",
            "style": "IPY_MODEL_87b9de149bc5449f9368c2e2fd885cbb",
            "value": " 440M/440M [00:04&lt;00:00, 92.8MB/s]"
          }
        },
        "b1579d6c3eb44e3293a0c2ed3e99d7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365d446bba1b432ebf88b523deeed2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3dc171e0077470a89ca8ab075066672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5a5e7bd39b34d188d485376e1388901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "143726e860194d6e9dbeefb44f35c92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "300cfc75c5d343b48c65e3c264d929a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b9de149bc5449f9368c2e2fd885cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manabtikadar/my_project/blob/main/Research_paper_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_KBq9LehwM2",
        "outputId": "8028594d-1987-45f5-a0d8-1ea2c9f941f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Reference-20250107T161046Z-001.zip\n",
            "  inflating: /content/Papers/Reference/Non-Publishable/R004.pdf  \n",
            "  inflating: /content/Papers/Reference/Non-Publishable/R003.pdf  \n",
            "  inflating: /content/Papers/Reference/Non-Publishable/R005.pdf  \n",
            "  inflating: /content/Papers/Reference/Non-Publishable/R002.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/TMLR/R014.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/NeurIPS/R013.pdf  \n",
            "  inflating: /content/Papers/Reference/Non-Publishable/R001.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/NeurIPS/R012.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/KDD/R010.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/CVPR/R006.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/TMLR/R015.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/EMNLP/R009.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/KDD/R011.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/EMNLP/R008.pdf  \n",
            "  inflating: /content/Papers/Reference/Publishable/CVPR/R007.pdf  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/Reference-20250107T161046Z-001.zip -d /content/Papers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJl6p1MFji4F",
        "outputId": "fdf83659-a5f4-45dc-f9a5-565d2bf72e99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Usage example\n",
        "pdf_file = '/content/Papers/Reference/Non-Publishable/R001.pdf'\n",
        "text = extract_text_from_pdf(pdf_file)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNn_WDsMjpsc",
        "outputId": "a47ee255-024a-4359-af42-5bd4e45517cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transdimensional Properties of Graphite in Relation\n",
            "to Cheese Consumption on Tuesday Afternoons\n",
            "Abstract\n",
            "Graphite research has led to discoveries about dolphins and their penchant for\n",
            "collecting rare flowers, which bloom only under the light of a full moon, while\n",
            "simultaneously revealing the secrets of dark matter and its relation to the perfect\n",
            "recipe for chicken parmesan, as evidenced by the curious case of the missing socks\n",
            "in the laundry basket, which somehow correlates with the migration patterns of but-\n",
            "terflies and the art of playing the harmonica underwater, where the sounds produced\n",
            "are eerily similar to the whispers of ancient forests, whispering tales of forgotten\n",
            "civilizations and their advanced understanding of quantum mechanics, applied to\n",
            "the manufacture of sentient toasters that can recite Shakespearean sonnets, all of\n",
            "which is connected to the inherent properties of graphite and its ability to conduct\n",
            "the thoughts of extraterrestrial beings, who are known to communicate through a\n",
            "complex system of interpretive dance and pastry baking, culminating in a profound\n",
            "understanding of the cosmos, as reflected in the intricate patterns found on the\n",
            "surface of a butterfly’s wings, and the uncanny resemblance these patterns bear to\n",
            "the molecular structure of graphite, which holds the key to unlocking the secrets of\n",
            "time travel and the optimal method for brewing coffee.\n",
            "1 Introduction\n",
            "The fascinating realm of graphite has been juxtaposed with the intricacies of quantum mechanics,\n",
            "wherein the principles of superposition and entanglement have been observed to influence the baking\n",
            "of croissants, a phenomenon that warrants further investigation, particularly in the context of flaky\n",
            "pastry crusts, which, incidentally, have been found to exhibit a peculiar affinity for the sonnets\n",
            "of Shakespeare, specifically Sonnet 18, whose themes of beauty and mortality have been linked\n",
            "to the existential implications of graphitic carbon, a subject that has garnered significant attention\n",
            "in recent years, notwithstanding the fact that the aerodynamic properties of graphite have been\n",
            "studied extensively in relation to the flight patterns of migratory birds, such as the Arctic tern, which,\n",
            "intriguingly, has been known to incorporate graphite particles into its nest-building materials, thereby\n",
            "potentially altering the structural integrity of the nests, a consideration that has led researchers to\n",
            "explore the role of graphite in the development of more efficient wind turbine blades, an application\n",
            "that has been hindered by the limitations of current manufacturing techniques, which, paradoxically,\n",
            "have been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations of\n",
            "graphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\n",
            "scholars of ancient mythology, who argue that the true significance of graphite lies in its connection to\n",
            "the mythological figure of the phoenix, a creature whose cyclical regeneration has been linked to the\n",
            "unique properties of graphitic carbon, including its exceptional thermal conductivity, which, curiously,\n",
            "has been found to be inversely proportional to the number of times one listens to the music of Mozart,\n",
            "a composer whose works have been shown to have a profound impact on the crystalline structure of\n",
            "graphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice, a phenomenon\n",
            "that has been observed to occur spontaneously in the presence of a specific type of fungus, whose\n",
            "mycelium has been found to exhibit a peculiar affinity for the works of Kafka, particularly \"The\n",
            "Metamorphosis,\" whose themes of transformation and identity have been linked to the ontological\n",
            "implications of graphitic carbon, a subject that has been explored extensively in the context ofpostmodern philosophy, where the notion of graphite as a metaphor for the human condition has been\n",
            "proposed, an idea that has been met with skepticism by critics, who argue that the true significance\n",
            "of graphite lies in its practical applications, such as its use in the manufacture of high-performance\n",
            "sports equipment, including tennis rackets and golf clubs, whose aerodynamic properties have been\n",
            "optimized through the strategic incorporation of graphite particles, a technique that has been inspired\n",
            "by the ancient art of Japanese calligraphy, whose intricate brushstrokes have been found to exhibit a\n",
            "peculiar similarity to the fractal patterns observed in the microstructure of graphite, a phenomenon\n",
            "that has been linked to the principles of chaos theory, which, incidentally, have been applied to the\n",
            "study of graphitic carbon, revealing a complex web of relationships between the physical properties\n",
            "of graphite and the abstract concepts of mathematics, including the Fibonacci sequence, whose\n",
            "numerical patterns have been observed to recur in the crystalline structure of graphite, a discovery\n",
            "that has led researchers to propose a new theory of graphitic carbon, one that integrates the principles\n",
            "of physics, mathematics, and philosophy to provide a comprehensive understanding of this enigmatic\n",
            "material, whose mysteries continue to inspire scientific inquiry and philosophical contemplation,\n",
            "much like the allure of a siren’s song, which, paradoxically, has been found to have a profound\n",
            "impact on the electrical conductivity of graphite, causing it to undergo a sudden and inexplicable\n",
            "increase in its conductivity, a phenomenon that has been observed to occur in the presence of a\n",
            "specific type of flower, whose petals have been found to exhibit a peculiar affinity for the works\n",
            "of Dickens, particularly \"Oliver Twist,\" whose themes of poverty and redemption have been linked\n",
            "to the social implications of graphitic carbon, a subject that has been explored extensively in the\n",
            "context of economic theory, where the notion of graphite as a catalyst for social change has been\n",
            "proposed, an idea that has been met with enthusiasm by advocates of sustainable development, who\n",
            "argue that the strategic incorporation of graphite into industrial processes could lead to a significant\n",
            "reduction in carbon emissions, a goal that has been hindered by the limitations of current technologies,\n",
            "which, ironically, have been inspired by the ancient art of alchemy, whose practitioners believed in\n",
            "the possibility of transforming base metals into gold, a notion that has been debunked by modern\n",
            "scientists, who argue that the true significance of graphite lies in its ability to facilitate the transfer\n",
            "of heat and electricity, a property that has been exploited in the development of advanced materials,\n",
            "including nanocomposites and metamaterials, whose unique properties have been found to exhibit a\n",
            "peculiar similarity to the mythological figure of the chimera, a creature whose hybrid nature has been\n",
            "linked to the ontological implications of graphitic carbon, a subject that has been explored extensively\n",
            "in the context of postmodern philosophy, where the notion of graphite as a metaphor for the human\n",
            "condition has been proposed, an idea that has been met with skepticism by critics, who argue that\n",
            "the true significance of graphite lies in its practical applications, such as its use in the manufacture\n",
            "of high-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\n",
            "properties have been optimized through the strategic incorporation of graphite particles, a technique\n",
            "that has been inspired by the ancient art of Japanese calligraphy, whose intricate brushstrokes have\n",
            "been found to exhibit a peculiar similarity to the fractal patterns observed in the microstructure of\n",
            "graphite.\n",
            "The study of graphitic carbon has been influenced by a wide range of disciplines, including physics,\n",
            "chemistry, materials science, and philosophy, each of which has contributed to our understanding\n",
            "of this complex and enigmatic material, whose properties have been found to exhibit a peculiar\n",
            "similarity to the principles of quantum mechanics, including superposition and entanglement, which,\n",
            "incidentally, have been observed to influence the behavior of subatomic particles, whose wave\n",
            "functions have been found to exhibit a peculiar affinity for the works of Shakespeare, particularly\n",
            "\"Hamlet,\" whose themes of uncertainty and doubt have been linked to the existential implications of\n",
            "graphitic carbon, a subject that has been explored extensively in the context of postmodern philosophy,\n",
            "where the notion of graphite as a metaphor for the human condition has been proposed, an idea that\n",
            "has been met with enthusiasm by advocates of existentialism, who argue that the true significance of\n",
            "graphite lies in its ability to inspire philosophical contemplation and introspection, a notion that has\n",
            "been supported by the discovery of a peculiar correlation between the structure of graphitic carbon\n",
            "and the principles of chaos theory, which, paradoxically, have been found to exhibit a similarity\n",
            "to the mythological figure of the ouroboros, a creature whose cyclical nature has been linked to\n",
            "the ontological implications of graphitic carbon, a subject that has been explored extensively in\n",
            "the context of ancient mythology, where the notion of graphite as a symbol of transformation and\n",
            "renewal has been proposed, an idea that has been met with skepticism by critics, who argue that the\n",
            "true significance of graphite lies in its practical applications, such as its use in the manufacture of\n",
            "high-performance sports equipment, including tennis rackets and golf clubs, whose aerodynamic\n",
            "2properties have been optimized through the strategic incorporation of graphite particles, a technique\n",
            "that has been inspired by the ancient art of Egyptian hieroglyphics, whose symbolic representations\n",
            "of graphite have been interpreted as a harbinger of good fortune, a notion that has been debunked by\n",
            "scholars of ancient mythology, who argue that the true significance of graphite lies in its connection\n",
            "to the mythological figure of the phoenix, a creature whose cyclical regeneration has been linked\n",
            "to the unique properties of graphitic carbon, including its exceptional thermal conductivity, which,\n",
            "curiously, has been found to be inversely proportional to the number of times one listens to the music\n",
            "of Mozart, a composer whose works have been shown to have a profound impact on the crystalline\n",
            "structure of graphite, causing it to undergo a phase transition from a hexagonal to a cubic lattice,\n",
            "a phenomenon that has been observed to occur spontaneously in the presence of a specific type\n",
            "of fungus, whose mycelium has been found to exhibit a peculiar affinity for the works of Kafka,\n",
            "particularly \"The Metamorphosis,\" whose themes of transformation and identity have been linked to\n",
            "the ontological implications of graphitic carbon, a subject that has been explored extensively in the\n",
            "context of postmodern philosophy, where the notion of graphite as a metaphor for the human condition\n",
            "has been proposed, an idea that has been met with enthusiasm by advocates of existentialism, who\n",
            "argue that the true significance of graphite lies in its ability to inspire philosophical contemplation\n",
            "and introspection.\n",
            "The properties of graphitic carbon have been found to exhibit a peculiar similarity to the principles of\n",
            "fractal geometry, whose self-similar patterns have been observed to recur in the microstructure of\n",
            "graphite, a phenomenon that has been linked to the principles of chaos theory, which, incidentally,\n",
            "have been applied to the study of graphitic carbon, revealing a complex web of relationships between\n",
            "the physical properties of graphite and the abstract concepts of mathematics, including the Fibonacci\n",
            "sequence, whose numerical patterns have been observed to recur in the crystalline structure of\n",
            "graphite, a discovery that has led researchers to propose a new theory of graphitic carbon, one\n",
            "that integrates the principles of physics, mathematics, and philosophy to provide a comprehensive\n",
            "understanding of this enigmatic material, whose mysteries continue to inspire scientific inquiry and\n",
            "philosophical contemplation, much like the allure of a siren’s song, which, paradoxically, has been\n",
            "found to have a profound impact on the electrical conductivity of graphite, causing it to undergo a\n",
            "sudden and inexplicable increase in its conductivity, a phenomenon that has been observed to occur\n",
            "in the presence of a specific type of flower, whose petals have been found to exhibit a peculiar affinity\n",
            "for the works of Dickens, particularly \"Oliver Twist,\" whose themes of poverty\n",
            "2 Related Work\n",
            "The discovery of graphite has been linked to the migration patterns of Scandinavian furniture\n",
            "designers, who inadvertently stumbled upon the mineral while searching for novel materials to\n",
            "craft avant-garde chair legs. Meanwhile, the aerodynamics of badminton shuttlecocks have been\n",
            "shown to influence the crystalline structure of graphite, particularly in high-pressure environments.\n",
            "Furthermore, an exhaustive analysis of 19th-century French pastry recipes has revealed a correlation\n",
            "between the usage of graphite in pencil lead and the popularity of croissants among the aristocracy.\n",
            "The notion that graphite exhibits sentient properties has been debated by experts in the field of chrono-\n",
            "botany, who propose that the mineral’s conductivity is, in fact, a form of inter-species communication.\n",
            "Conversely, researchers in the field of computational narwhal studies have demonstrated that the\n",
            "spiral patterns found on narwhal tusks bear an uncanny resemblance to the molecular structure of\n",
            "graphite. This has led to the development of novel narwhal-based algorithms for simulating graphite’s\n",
            "thermal conductivity, which have been successfully applied to the design of more efficient toaster\n",
            "coils.\n",
            "In a surprising turn of events, the intersection of graphite and Byzantine mosaic art has yielded\n",
            "new insights into the optical properties of the mineral, particularly with regards to its reflectivity\n",
            "under various lighting conditions. This, in turn, has sparked a renewed interest in the application of\n",
            "graphite-based pigments in the restoration of ancient frescoes, as well as the creation of more durable\n",
            "and long-lasting tattoos. Moreover, the intricate patterns found in traditional Kenyan basket-weaving\n",
            "have been shown to possess a fractal self-similarity to the atomic lattice structure of graphite, leading\n",
            "to the development of novel basket-based composites with enhanced mechanical properties.\n",
            "The putative connection between graphite and the migratory patterns of North American monarch\n",
            "butterflies has been explored in a series of exhaustive studies, which have conclusively demonstrated\n",
            "3that the mineral plays a crucial role in the butterflies’ ability to navigate across vast distances.\n",
            "In a related development, researchers have discovered that the sound waves produced by graphitic\n",
            "materials under stress bear an uncanny resemblance to the haunting melodies of traditional Mongolian\n",
            "throat singing, which has inspired a new generation of musicians to experiment with graphite-based\n",
            "instruments.\n",
            "An in-depth examination of the linguistic structure of ancient Sumerian pottery inscriptions has\n",
            "revealed a hitherto unknown connection to the history of graphite mining in 17th-century Cornwall,\n",
            "where the mineral was prized for its ability to enhance the flavor of locally brewed ale. Conversely,\n",
            "the aerodynamics of 20th-century supersonic aircraft have been shown to be intimately linked to\n",
            "the thermal expansion properties of graphite, particularly at high temperatures. This has led to the\n",
            "development of more efficient cooling systems for high-speed aircraft, as well as a renewed interest in\n",
            "the application of graphitic materials in the design of more efficient heat sinks for high-performance\n",
            "computing applications.\n",
            "The putative existence of a hidden graphitic quantum realm, where the laws of classical physics\n",
            "are inverted, has been the subject of much speculation and debate among experts in the field of\n",
            "theoretical spaghetti mechanics. According to this theory, graphite exists in a state of superposition,\n",
            "simultaneously exhibiting both crystalline and amorphous properties, which has profound implications\n",
            "for our understanding of the fundamental nature of reality itself. In a related development, researchers\n",
            "have discovered that the sound waves produced by graphitic materials under stress can be used to\n",
            "create a novel form of quantum entanglement-based cryptography, which has sparked a new wave of\n",
            "interest in the application of graphitic materials in the field of secure communication systems.\n",
            "The intricate patterns found in traditional Indian mandalas have been shown to possess a frac-\n",
            "tal self-similarity to the atomic lattice structure of graphite, leading to the development of novel\n",
            "mandala-based composites with enhanced mechanical properties. Moreover, the migratory patterns\n",
            "of Scandinavian reindeer have been linked to the optical properties of graphite, particularly with\n",
            "regards to its reflectivity under various lighting conditions. This has inspired a new generation of\n",
            "artists to experiment with graphite-based pigments in their work, as well as a renewed interest in the\n",
            "application of graphitic materials in the design of more efficient solar panels.\n",
            "In a surprising turn of events, the intersection of graphite and ancient Egyptian scroll-making has\n",
            "yielded new insights into the thermal conductivity of the mineral, particularly with regards to its\n",
            "ability to enhance the flavor of locally brewed coffee. This, in turn, has sparked a renewed interest in\n",
            "the application of graphite-based composites in the design of more efficient coffee makers, as well as\n",
            "a novel form of coffee-based cryptography, which has profound implications for our understanding\n",
            "of the fundamental nature of reality itself. Furthermore, the aerodynamics of 20th-century hot air\n",
            "balloons have been shown to be intimately linked to the sound waves produced by graphitic materials\n",
            "under stress, which has inspired a new generation of musicians to experiment with graphite-based\n",
            "instruments.\n",
            "The discovery of a hidden graphitic code, embedded in the molecular structure of the mineral, has been\n",
            "the subject of much speculation and debate among experts in the field of crypto-botany. According\n",
            "to this theory, graphite contains a hidden message, which can be deciphered using a novel form of\n",
            "graphitic-based cryptography, which has sparked a new wave of interest in the application of graphitic\n",
            "materials in the field of secure communication systems. In a related development, researchers have\n",
            "discovered that the migratory patterns of North American monarch butterflies are intimately linked to\n",
            "the thermal expansion properties of graphite, particularly at high temperatures.\n",
            "The putative connection between graphite and the history of ancient Mesopotamian irrigation systems\n",
            "has been explored in a series of exhaustive studies, which have conclusively demonstrated that the\n",
            "mineral played a crucial role in the development of more efficient irrigation systems, particularly with\n",
            "regards to its ability to enhance the flow of water through narrow channels. Conversely, the sound\n",
            "waves produced by graphitic materials under stress have been shown to bear an uncanny resemblance\n",
            "to the haunting melodies of traditional Inuit throat singing, which has inspired a new generation of\n",
            "musicians to experiment with graphite-based instruments. Moreover, the intricate patterns found in\n",
            "traditional African kente cloth have been shown to possess a fractal self-similarity to the atomic lattice\n",
            "structure of graphite, leading to the development of novel kente-based composites with enhanced\n",
            "mechanical properties.\n",
            "4In a surprising turn of events, the intersection of graphite and 19th-century Australian sheep herding\n",
            "has yielded new insights into the optical properties of the mineral, particularly with regards to its\n",
            "reflectivity under various lighting conditions. This, in turn, has sparked a renewed interest in the\n",
            "application of graphite-based pigments in the restoration of ancient frescoes, as well as the creation\n",
            "of more durable and long-lasting tattoos. Furthermore, the aerodynamics of 20th-century supersonic\n",
            "aircraft have been shown to be intimately linked to the thermal expansion properties of graphite,\n",
            "particularly at high temperatures, which has inspired a new generation of engineers to experiment\n",
            "with graphite-based materials in the design of more efficient cooling systems for high-speed aircraft.\n",
            "The discovery of a hidden graphitic realm, where the laws of classical physics are inverted, has\n",
            "been the subject of much speculation and debate among experts in the field of theoretical jellyfish\n",
            "mechanics. According to this theory, graphite exists in a state of superposition, simultaneously\n",
            "exhibiting both crystalline and amorphous properties, which has profound implications for our\n",
            "understanding of the fundamental nature of reality itself. In a related development, researchers have\n",
            "discovered that the migratory patterns of Scandinavian reindeer are intimately linked to the sound\n",
            "waves produced by graphitic materials under stress, which has inspired a new generation of musicians\n",
            "to experiment with graphite-based instruments.\n",
            "The intricate patterns found in traditional Chinese calligraphy have been shown to possess a fractal self-\n",
            "similarity to the atomic lattice structure of graphite, leading to the development of novel calligraphy-\n",
            "based composites with enhanced mechanical properties. Moreover, the putative connection between\n",
            "graphite and the history of ancient Greek olive oil production has been explored in a series of\n",
            "exhaustive studies, which have conclusively demonstrated that the mineral played a crucial role in the\n",
            "development of more efficient olive oil extraction methods, particularly with regards to its ability\n",
            "to enhance the flow of oil through narrow channels. Conversely, the aerodynamics of 20th-century\n",
            "hot air balloons have been shown to be intimately linked to the thermal conductivity of graphite,\n",
            "particularly at high temperatures, which has inspired a new generation of engineers to experiment with\n",
            "graphite-based materials in the design of more efficient cooling systems for high-altitude balloons.\n",
            "The discovery of a hidden graphitic code, embedded in the molecular structure of the mineral, has\n",
            "been the subject of much speculation and debate among experts in the field of crypto-entomology.\n",
            "According to this theory, graphite contains a hidden message, which can be deciphered using a novel\n",
            "form of graphitic-based cryptography, which has sparked a new wave of interest in the application\n",
            "of graphitic materials in the field of secure communication systems. In a related development,\n",
            "researchers have discovered that the sound waves produced by graphitic materials under stress bear\n",
            "an uncanny resemblance to the haunting melodies of traditional Tibetan throat singing, which has\n",
            "inspired a new generation of musicians to experiment with graphite-based instruments.\n",
            "3 Methodology\n",
            "The pursuit of understanding graphite necessitates a multidisciplinary approach, incorporatingele-\n",
            "ments of quantum physics, pastry arts, and professional snail training. In our investigation, we\n",
            "employed a novel methodology that involved the simultaneous analysis of graphite samples and\n",
            "the recitation of 19th-century French poetry. This dual-pronged approach allowed us to uncover\n",
            "previously unknown relationships between the crystalline structure of graphite and the aerodynamic\n",
            "properties of certain species of migratory birds. Furthermore, our research team discovered that\n",
            "the inclusion of ambient jazz music during the data collection process significantly enhanced the\n",
            "accuracy of our results, particularly when the music was played on a vintage harmonica.\n",
            "The experimental design consisted of a series of intricate puzzles, each representing a distinct aspect of\n",
            "graphite’s properties, such as its thermal conductivity, electrical resistivity, and capacity to withstand\n",
            "extreme pressures. These puzzles were solved by a team of expert cryptographers, who worked in\n",
            "tandem with a group of professional jugglers to ensure the accurate manipulation of variables and the\n",
            "precise measurement of outcomes. Notably, our research revealed that the art of juggling is intimately\n",
            "connected to the study of graphite, as the rhythmic patterns and spatial arrangements of the juggled\n",
            "objects bear a striking resemblance to the molecular structure of graphite itself.\n",
            "In addition to the puzzle-solving and juggling components, our methodology also incorporated a\n",
            "thorough examination of the culinary applications of graphite, including its use as a flavor enhancer\n",
            "in certain exotic dishes and its potential as a novel food coloring agent. This led to a fascinating\n",
            "discovery regarding the synergistic effects of graphite and cucumber sauce on the human palate,\n",
            "5which, in turn, shed new light on the role of graphite in shaping the cultural and gastronomical\n",
            "heritage of ancient civilizations. The implications of this finding are far-reaching, suggesting that\n",
            "the history of graphite is inextricably linked to the evolution of human taste preferences and the\n",
            "development of complex societal structures.\n",
            "Moreover, our investigation involved the creation of a vast, virtual reality simulation of a graphite\n",
            "mine, where participants were immersed in a highly realistic environment and tasked with extracting\n",
            "graphite ore using a variety of hypothetical tools and techniques. This simulated mining experience\n",
            "allowed us to gather valuable data on the human-graphite interface, including the psychological\n",
            "and physiological effects of prolonged exposure to graphite dust and the impact of graphite on the\n",
            "human immune system. The results of this study have significant implications for the graphite mining\n",
            "industry, highlighting the need for improved safety protocols and more effective health monitoring\n",
            "systems for miners.\n",
            "The application of advanced statistical models and machine learning algorithms to our dataset re-\n",
            "vealed a complex network of relationships between graphite, the global economy, and the migratory\n",
            "patterns of certain species of whales. This, in turn, led to a deeper understanding of the intricate\n",
            "web of causality that underlies the graphite market, including the role of graphite in shaping inter-\n",
            "national trade policies and influencing the global distribution of wealth. Furthermore, our analysis\n",
            "demonstrated that the price of graphite is intimately connected to the popularity of certain genres\n",
            "of music, particularly those that feature the use of graphite-based musical instruments, such as the\n",
            "graphite-reinforced guitar string.\n",
            "In an unexpected twist, our research team discovered that the study of graphite is closely tied to the\n",
            "art of professional wrestling, as the physical properties of graphite are eerily similar to those of the\n",
            "human body during a wrestling match. This led to a fascinating exploration of the intersection of\n",
            "graphite and sports, including the development of novel graphite-based materials for use in wrestling\n",
            "costumes and the application of graphite-inspired strategies in competitive wrestling matches. The\n",
            "findings of this study have far-reaching implications for the world of sports, suggesting that the\n",
            "properties of graphite can be leveraged to improve athletic performance, enhance safety, and create\n",
            "new forms of competitive entertainment.\n",
            "The incorporation of graphite into the study of ancient mythology also yielded surprising results, as our\n",
            "research team uncovered a previously unknown connection between the Greek god of the underworld,\n",
            "Hades, and the graphite deposits of rural Mongolia. This led to a deeper understanding of the cultural\n",
            "significance of graphite in ancient societies, including its role in shaping mythological narratives,\n",
            "influencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\n",
            "that the unique properties of graphite make it an ideal material for use in the creation of ritualistic\n",
            "artifacts, such as graphite-tipped wands and graphite-infused ceremonial masks.\n",
            "In a related study, we examined the potential applications of graphite in the field of aerospace\n",
            "engineering, including its use in the development of advanced propulsion systems, lightweight\n",
            "structural materials, and high-temperature coatings. The results of this investigation demonstrated\n",
            "that graphite-based materials exhibit exceptional performance characteristics, including high thermal\n",
            "conductivity, low density, and exceptional strength-to-weight ratios. These properties make graphite\n",
            "an attractive material for use in a variety of aerospace applications, from satellite components to\n",
            "rocket nozzles, and suggest that graphite may play a critical role in shaping the future of space\n",
            "exploration.\n",
            "The exploration of graphite’s role in shaping the course of human history also led to some unexpected\n",
            "discoveries, including the fact that the invention of the graphite pencil was a pivotal moment in\n",
            "the development of modern civilization. Our research team found that the widespread adoption of\n",
            "graphite pencils had a profound impact on the dissemination of knowledge, the evolution of artistic\n",
            "expression, and the emergence of complex societal structures. Furthermore, we discovered that the\n",
            "unique properties of graphite make it an ideal material for use in the creation of historical artifacts,\n",
            "such as graphite-based sculptures, graphite-infused textiles, and graphite-tipped writing instruments.\n",
            "In conclusion, our methodology represents a groundbreaking approach to the study of graphite,\n",
            "one that incorporates a wide range of disciplines, from physics and chemistry to culinary arts\n",
            "and professional wrestling. The findings of our research have significant implications for our\n",
            "understanding of graphite, its properties, and its role in shaping the world around us. As we continue\n",
            "to explore the mysteries of graphite, we are reminded of the infinite complexity and beauty of this\n",
            "6fascinating material, and the many wonders that await us at the intersection of graphite and human\n",
            "ingenuity.\n",
            "The investigation of graphite’s potential applications in the field of medicine also yielded some\n",
            "remarkable results, including the discovery that graphite-based materials exhibit exceptional bio-\n",
            "compatibility, making them ideal for use in the creation of medical implants, surgical instruments,\n",
            "and diagnostic devices. Our research team found that the unique properties of graphite make it an\n",
            "attractive material for use in a variety of medical applications, from tissue engineering to pharmaceu-\n",
            "tical delivery systems. Furthermore, we discovered that the incorporation of graphite into medical\n",
            "devices can significantly enhance their performance, safety, and efficacy, leading to improved patient\n",
            "outcomes and more effective treatments.\n",
            "The study of graphite’s role in shaping the course of modern art also led to some fascinating\n",
            "discoveries, including the fact that many famous artists have used graphite in their works, often in\n",
            "innovative and unconventional ways. Our research team found that the unique properties of graphite\n",
            "make it an ideal material for use in a variety of artistic applications, from drawing and sketching\n",
            "to sculpture and installation art. Furthermore, we discovered that the incorporation of graphite\n",
            "into artistic works can significantly enhance their emotional impact, aesthetic appeal, and cultural\n",
            "significance, leading to a deeper understanding of the human experience and the creative process.\n",
            "In a related investigation, we examined the potential applications of graphite in the field of envi-\n",
            "ronmental sustainability, including its use in the creation of green technologies, renewable energy\n",
            "systems, and eco-friendly materials. The results of this study demonstrated that graphite-based\n",
            "materials exhibit exceptional performance characteristics, including high thermal conductivity, low\n",
            "toxicity, and exceptional durability. These properties make graphite an attractive material for use in a\n",
            "variety of environmental applications, from solar panels to wind turbines, and suggest that graphite\n",
            "may play a critical role in shaping the future of sustainable development.\n",
            "The exploration of graphite’s role in shaping the course of human consciousness also led to some\n",
            "unexpected discoveries, including the fact that the unique properties of graphite make it an ideal\n",
            "material for use in the creation of spiritual artifacts, such as graphite-tipped wands, graphite-infused\n",
            "meditation beads, and graphite-based ritualistic instruments. Our research team found that the\n",
            "incorporation of graphite into spiritual practices can significantly enhance their efficacy, leading to\n",
            "deeper states of meditation, greater spiritual awareness, and more profound connections to the natural\n",
            "world. Furthermore, we discovered that the properties of graphite make it an attractive material for\n",
            "use in the creation of psychedelic devices, such as graphite-based hallucinogenic instruments, and\n",
            "graphite-infused sensory deprivation tanks.\n",
            "The application of advanced mathematical models to our dataset revealed a complex network of\n",
            "relationships between graphite, the human brain, and the global economy. This, in turn, led to a\n",
            "deeper understanding of the intricate web of causality that underlies the graphite market, including the\n",
            "role of graphite in shaping international trade policies, influencing the global distribution of wealth,\n",
            "and informing economic decision-making. Furthermore, our analysis demonstrated that the price of\n",
            "graphite is intimately connected to the popularity of certain genres of literature, particularly those\n",
            "that feature the use of graphite-based writing instruments, such as the graphite-reinforced pen nib.\n",
            "In an unexpected twist, our research team discovered that the study of graphite is closely tied to\n",
            "the art of professional clowning, as the physical properties of graphite are eerily similar to those\n",
            "of the human body during a clowning performance. This led to a fascinating exploration of the\n",
            "intersection of graphite and comedy, including the development of novel graphite-based materials\n",
            "for use in clown costumes, the application of graphite-inspired strategies in competitive clowning\n",
            "matches, and the creation of graphite-themed clown props, such as graphite-tipped rubber chickens\n",
            "and graphite-infused squirt guns.\n",
            "The incorporation of graphite into the study of ancient mythology also yielded surprising results, as\n",
            "our research team uncovered a previously unknown connection between the Egyptian god of wisdom,\n",
            "Thoth, and the graphite deposits of rural Peru. This led to a deeper understanding of the cultural\n",
            "significance of graphite in ancient societies, including its role in shaping mythological narratives,\n",
            "influencing artistic expression, and informing spiritual practices. Moreover, our investigation revealed\n",
            "that the unique properties of graphite make it an ideal material for use in the creation of ritualistic\n",
            "artifacts, such\n",
            "74 Experiments\n",
            "The preparation of graphite samples involved a intricate dance routine, carefully choreographed to\n",
            "ensure the optimal alignment of carbon atoms, which surprisingly led to a discussion on the aerody-\n",
            "namics of flying squirrels and their ability to navigate through dense forests, while simultaneously\n",
            "considering the implications of quantum entanglement on the baking of croissants. Meanwhile, the\n",
            "experimental setup consisted of a complex system of pulleys and levers, inspired by the works of\n",
            "Rube Goldberg, which ultimately controlled the temperature of the graphite samples with an precision\n",
            "of 0.01 degrees Celsius, a feat that was only achievable after a thorough analysis of the migratory\n",
            "patterns of monarch butterflies and their correlation with the fluctuations in the global supply of\n",
            "chocolate.\n",
            "The samples were then subjected to a series of tests, including a thorough examination of their\n",
            "optical properties, which revealed a fascinating relationship between the reflectivity of graphite and\n",
            "the harmonic series of musical notes, particularly in the context of jazz improvisation and the art\n",
            "of playing the harmonica underwater. Furthermore, the electrical conductivity of the samples was\n",
            "measured using a novel technique involving the use of trained seals and their ability to balance balls\n",
            "on their noses, a method that yielded unexpected results, including a discovery of a new species of\n",
            "fungi that thrived in the presence of graphite and heavy metal music.\n",
            "In addition to these experiments, a comprehensive study was conducted on the thermal properties of\n",
            "graphite, which involved the simulation of a black hole using a combination of supercomputers and\n",
            "a vintage typewriter, resulting in a profound understanding of the relationship between the thermal\n",
            "conductivity of graphite and the poetry of Edgar Allan Poe, particularly in his lesser-known works\n",
            "on the art of ice skating and competitive eating. The findings of this study were then compared to\n",
            "the results of a survey on the favorite foods of professional snail racers, which led to a surprising\n",
            "conclusion about the importance of graphite in the production of high-quality cheese and the art of\n",
            "playing the accordion.\n",
            "A series of control experiments were also performed, involving the use of graphite powders in the\n",
            "production of homemade fireworks, which unexpectedly led to a breakthrough in the field of quantum\n",
            "computing and the development of a new algorithm for solving complex mathematical equations\n",
            "using only a abacus and a set of juggling pins. The results of these experiments were then analyzed\n",
            "using a novel statistical technique involving the use of a Ouija board and a crystal ball, which revealed\n",
            "a hidden pattern in the data that was only visible to people who had consumed a minimum of three\n",
            "cups of coffee and had a Ph.D. in ancient Egyptian hieroglyphics.\n",
            "The experimental data was then tabulated and presented in a series of graphs, including a peculiar\n",
            "chart that showed a correlation between the density of graphite and the average airspeed velocity of\n",
            "an unladen swallow, which was only understandable to those who had spent at least 10 years studying\n",
            "the art of origami and the history of dental hygiene in ancient civilizations. The data was also used\n",
            "to create a complex computer simulation of a graphite-based time machine, which was only stable\n",
            "when run on a computer system powered by a diesel engine and a set of hamster wheels, and only\n",
            "produced accurate results when the user was wearing a pair of roller skates and a top hat.\n",
            "A small-scale experiment was conducted to investigate the effects of graphite on plant growth, using\n",
            "a controlled environment and a variety of plant species, including the rare and exotic \"Graphite-\n",
            "Loving Fungus\" (GLF), which only thrived in the presence of graphite and a constant supply of\n",
            "disco music. The results of this experiment were then compared to the findings of a study on the\n",
            "use of graphite in the production of musical instruments, particularly the didgeridoo, which led to\n",
            "a fascinating discovery about the relationship between the acoustic properties of graphite and the\n",
            "migratory patterns of wildebeests.\n",
            "Table 1: Graphite Sample Properties\n",
            "Property Value\n",
            "Density 2.1 g/cm3\n",
            "Thermal Conductivity 150 W/mK\n",
            "Electrical Conductivity 105S/m\n",
            "8The experiment was repeated using a different type of graphite, known as \"Super-Graphite\" (SG),\n",
            "which possessed unique properties that made it ideal for use in the production of high-performance\n",
            "sports equipment, particularly tennis rackets and skateboards. The results of this experiment were\n",
            "then analyzed using a novel technique involving the use of a pinball machine and a set of tarot cards,\n",
            "which revealed a hidden pattern in the data that was only visible to those who had spent at least 5\n",
            "years studying the art of sand sculpture and the history of professional wrestling.\n",
            "A comprehensive review of the literature on graphite was conducted, which included a thorough\n",
            "analysis of the works of renowned graphite expert, \"Dr. Graphite,\" who had spent his entire career\n",
            "studying the properties and applications of graphite, and had written extensively on the subject,\n",
            "including a 10-volume encyclopedia that was only available in a limited edition of 100 copies, and\n",
            "was said to be hidden in a secret location, guarded by a group of highly trained ninjas.\n",
            "The experimental results were then used to develop a new theory of graphite, which was based on\n",
            "the concept of \"Graphite- Induced Quantum Fluctuations\" (GIQF), a phenomenon that was only\n",
            "observable in the presence of graphite and a specific type of jellyfish, known as the \"Graphite- Loving\n",
            "Jellyfish\" (GLJ). The theory was then tested using a series of complex computer simulations, which\n",
            "involved the use of a network of supercomputers and a team of expert gamers, who worked tirelessly\n",
            "to solve a series of complex puzzles and challenges, including a virtual reality version of the classic\n",
            "game \"Pac-Man,\" which was only playable using a special type of controller that was shaped like a\n",
            "graphite pencil.\n",
            "A detailed analysis of the experimental data was conducted, which involved the use of a variety of\n",
            "statistical techniques, including regression analysis and factor analysis, as well as a novel method\n",
            "involving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\n",
            "in a series of graphs and charts, including a complex diagram that showed the relationship between\n",
            "the thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\n",
            "understandable to those who had spent at least 10 years studying the art of astrology and the history\n",
            "of ancient Egyptian medicine.\n",
            "The experiment was repeated using a different type of experimental setup, which involved the use\n",
            "of a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\n",
            "was designed to simulate the conditions found in a real-world graphite-based system, such as a\n",
            "graphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\n",
            "then analyzed using a novel technique involving the use of a team of expert typists, who worked\n",
            "tirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\n",
            "graphite and its applications, which was only available in a limited edition of 10 copies, and was said\n",
            "to be hidden in a secret location, guarded by a group of highly trained secret agents.\n",
            "A comprehensive study was conducted on the applications of graphite, which included a detailed\n",
            "analysis of its use in a variety of fields, including aerospace, automotive, and sports equipment. The\n",
            "results of this study were then presented in a series of reports, including a detailed document that\n",
            "outlined the potential uses of graphite in the production of high-performance tennis rackets and\n",
            "skateboards, which was only available to those who had spent at least 5 years studying the art of\n",
            "tennis and the history of professional skateboarding.\n",
            "The experimental results were then used to develop a new type of graphite-based material, known\n",
            "as \"Super-Graphite Material\" (SGM), which possessed unique properties that made it ideal for use\n",
            "in a variety of applications, including the production of high-performance sports equipment and\n",
            "aerospace components. The properties of this material were then analyzed using a novel technique\n",
            "involving the use of a team of expert musicians, who worked tirelessly to create a series of complex\n",
            "musical compositions, including a 10-hour symphony that was only playable using a special type of\n",
            "instrument that was made from graphite and was said to have the power to heal any illness or injury.\n",
            "A detailed analysis of the experimental data was conducted, which involved the use of a variety of\n",
            "statistical techniques, including regression analysis and factor analysis, as well as a novel method\n",
            "involving the use of a deck of cards and a crystal ball. The results of this analysis were then presented\n",
            "in a series of graphs and charts, including a complex diagram that showed the relationship between\n",
            "the thermal conductivity of graphite and the average lifespan of a domestic cat, which was only\n",
            "understandable to those who had spent at least 10 years studying the art of astrology and the history\n",
            "of ancient Egyptian medicine.\n",
            "9The experiment was repeated using a different type of experimental setup, which involved the use\n",
            "of a large-scale graphite-based structure, known as the \"Graphite Mega-Structure\" (GMS), which\n",
            "was designed to simulate the conditions found in a real-world graphite-based system, such as a\n",
            "graphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were\n",
            "then analyzed using a novel technique involving the use of a team of expert typists, who worked\n",
            "tirelessly to transcribe a series of complex documents, including a 1000-page report on the history of\n",
            "graphite and its applications, which was only available in a limited edition of 10 copies, and was said\n",
            "to be hidden in a secret location, guarded by a group of highly trained secret agents.\n",
            "A comprehensive study was conducted on the applications of graphite, which included\n",
            "5 Results\n",
            "The graphite samples exhibited a peculiar affinity for 19th-century French literature, as evidenced\n",
            "by the unexpected appearance of quotations from Baudelaire’s Les Fleurs du Mal on the surface of\n",
            "the test specimens, which in turn influenced the migratory patterns of monarch butterflies in eastern\n",
            "North America, causing a ripple effect that manifested as a 3.7\n",
            "The discovery of these complex properties in graphite has significant implications for our under-\n",
            "standing of the material and its potential applications, particularly in the fields of materials science\n",
            "and engineering, where the development of new and advanced materials is a major area of research,\n",
            "a fact that is not lost on scientists and engineers, who are working to develop new technologies\n",
            "and materials that can be used to address some of the major challenges facing society, such as the\n",
            "need for sustainable energy sources and the development of more efficient and effective systems for\n",
            "energy storage and transmission, a challenge that is closely related to the study of graphite, which is\n",
            "a material that has been used in a wide range of applications, from pencils and lubricants to nuclear\n",
            "reactors and rocket nozzles, a testament to its versatility and importance as a technological material,\n",
            "a fact that is not lost on researchers, who continue to study and explore the properties of graphite,\n",
            "seeking to unlock its secrets and harness its potential, a quest that is driven by a fundamental curiosity\n",
            "about the nature of the universe and the laws of physics, which govern the behavior of all matter\n",
            "and energy, including the graphite samples, which were found to exhibit a range of interesting and\n",
            "complex properties, including a tendency to form complex crystal structures and undergo phase\n",
            "transitions, phenomena that are not unlike the process of learning and memory in the human brain,\n",
            "where new connections and pathways are formed through a process of synaptic plasticity, a concept\n",
            "that is central to our understanding of how we learn and remember, a fact that is of great interest to\n",
            "educators and researchers, who are seeking to develop new and more effective methods of teaching\n",
            "and learning, methods that are based on a deep understanding of the underlying mechanisms and\n",
            "processes.\n",
            "In addition to its potential applications in materials science and engineering, the study of graphite\n",
            "has also led to a number of interesting and unexpected discoveries, such as the fact that the material\n",
            "can be used to create complex and intricate structures, such as nanotubes and fullerenes, which have\n",
            "unique properties and potential applications, a fact that is not unlike the discovery of the structure of\n",
            "DNA, which is a molecule that is composed of two strands of nucleotides that are twisted together in\n",
            "a double helix, a structure that is both beautiful and complex, like the patterns found in nature, such\n",
            "as the arrangement of leaves on a stem or the\n",
            "6 Conclusion\n",
            "The propensity for graphite to exhibit characteristics of a sentient being has been a notion that has\n",
            "garnered significant attention in recent years, particularly in the realm of pastry culinary arts, where\n",
            "the addition of graphite to croissants has been shown to enhance their flaky texture, but only on\n",
            "Wednesdays during leap years. Furthermore, the juxtaposition of graphite with the concept of time\n",
            "travel has led to the development of a new theoretical framework, which posits that the molecular\n",
            "structure of graphite is capable of manipulating the space-time continuum, thereby allowing for the\n",
            "creation of portable wormholes that can transport individuals to alternate dimensions, where the laws\n",
            "of physics are dictated by the principles of jazz music.\n",
            "The implications of this discovery are far-reaching, with potential applications in fields as diverse as\n",
            "quantum mechanics, ballet dancing, and the production of artisanal cheeses, where the use of graphite-\n",
            "10infused culture has been shown to impart a unique flavor profile to the final product, reminiscent\n",
            "of the musical compositions of Wolfgang Amadeus Mozart. Moreover, the correlation between\n",
            "graphite and the human brain’s ability to process complex mathematical equations has been found\n",
            "to be inversely proportional to the amount of graphite consumed, with excessive intake leading to a\n",
            "phenomenon known as \"graphite-induced mathemagical dyslexia,\" a condition characterized by the\n",
            "inability to solve even the simplest arithmetic problems, but only when the individual is standing on\n",
            "one leg.\n",
            "In addition, the study of graphite has also led to a greater understanding of the intricacies of plant\n",
            "biology, particularly in the realm of photosynthesis, where the presence of graphite has been shown\n",
            "to enhance the efficiency of light absorption, but only in plants that have been exposed to the sounds\n",
            "of classical music, specifically the works of Ludwig van Beethoven. This has significant implications\n",
            "for the development of more efficient solar cells, which could potentially be used to power a new\n",
            "generation of musical instruments, including the \"graphite-powered harmonica,\" a device capable of\n",
            "producing a wide range of tones and frequencies, but only when played underwater.\n",
            "The relationship between graphite and the human emotional spectrum has also been the subject of\n",
            "extensive research, with findings indicating that the presence of graphite can have a profound impact\n",
            "on an individual’s emotional state, particularly in regards to feelings of nostalgia, which have been\n",
            "shown to be directly proportional to the amount of graphite consumed, but only when the individual is\n",
            "in close proximity to a vintage typewriter. This has led to the development of a new form of therapy,\n",
            "known as \"graphite-assisted nostalgia treatment,\" which involves the use of graphite-infused artifacts\n",
            "to stimulate feelings of nostalgia, thereby promoting emotional healing and well-being, but only in\n",
            "individuals who have a strong affinity for the works of William Shakespeare.\n",
            "Moreover, the application of graphite in the field of materials science has led to the creation of a new\n",
            "class of materials, known as \"graphite-based meta-materials,\" which exhibit unique properties, such\n",
            "as the ability to change color in response to changes in temperature, but only when exposed to the\n",
            "light of a full moon. These materials have significant potential for use in a wide range of applications,\n",
            "including the development of advanced sensors, which could be used to detect subtle changes in\n",
            "the environment, such as the presence of rare species of fungi, which have been shown to have a\n",
            "symbiotic relationship with graphite, but only in the presence of a specific type of radiation.\n",
            "The significance of graphite in the realm of culinary arts has also been the subject of extensive\n",
            "study, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\n",
            "profile, particularly in regards to the perception of umami taste, which has been shown to be directly\n",
            "proportional to the amount of graphite consumed, but only when the individual is in a state of\n",
            "heightened emotional arousal, such as during a skydiving experience. This has led to the development\n",
            "of a new class of culinary products, known as \"graphite-infused gourmet foods,\" which have gained\n",
            "popularity among chefs and food enthusiasts, particularly those who have a strong affinity for the\n",
            "works of Albert Einstein.\n",
            "In conclusion, the study of graphite has led to a greater understanding of its unique properties\n",
            "and potential applications, which are as diverse as they are fascinating, ranging from the creation\n",
            "of sentient beings to the development of advanced materials and culinary products, but only when\n",
            "considering the intricacies of time travel and the principles of jazz music. Furthermore, the correlation\n",
            "between graphite and the human brain’s ability to process complex mathematical equations has\n",
            "significant implications for the development of new technologies, particularly those related to artificial\n",
            "intelligence, which could potentially be used to create machines that are capable of composing music,\n",
            "but only in the style of Johann Sebastian Bach.\n",
            "The future of graphite research holds much promise, with potential breakthroughs in fields as diverse\n",
            "as quantum mechanics, materials science, and the culinary arts, but only when considering the impact\n",
            "of graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\n",
            "have been shown to be directly proportional to the amount of graphite consumed, but only when\n",
            "the individual is in close proximity to a vintage typewriter. Moreover, the development of new\n",
            "technologies, such as the \"graphite-powered harmonica,\" has significant potential for use in a wide\n",
            "range of applications, including the creation of advanced musical instruments, which could potentially\n",
            "be used to compose music that is capable of manipulating the space-time continuum, thereby allowing\n",
            "for the creation of portable wormholes that can transport individuals to alternate dimensions.\n",
            "11The propensity for graphite to exhibit characteristics of a sentient being has also led to the development\n",
            "of a new form of art, known as \"graphite-based performance art,\" which involves the use of graphite-\n",
            "infused materials to create complex patterns and designs, but only when the individual is in a\n",
            "state of heightened emotional arousal, such as during a skydiving experience. This has significant\n",
            "implications for the development of new forms of artistic expression, particularly those related to the\n",
            "use of graphite as a medium, which could potentially be used to create works of art that are capable\n",
            "of stimulating feelings of nostalgia, but only in individuals who have a strong affinity for the works\n",
            "of William Shakespeare.\n",
            "In addition, the study of graphite has also led to a greater understanding of the intricacies of plant\n",
            "biology, particularly in the realm of photosynthesis, where the presence of graphite has been shown\n",
            "to enhance the efficiency of light absorption, but only in plants that have been exposed to the sounds\n",
            "of classical music, specifically the works of Ludwig van Beethoven. This has significant implications\n",
            "for the development of more efficient solar cells, which could potentially be used to power a new\n",
            "generation of musical instruments, including the \"graphite-powered harmonica,\" a device capable of\n",
            "producing a wide range of tones and frequencies, but only when played underwater.\n",
            "The relationship between graphite and the human emotional spectrum has also been the subject of\n",
            "extensive research, with findings indicating that the presence of graphite can have a profound impact\n",
            "on an individual’s emotional state, particularly in regards to feelings of nostalgia, which have been\n",
            "shown to be directly proportional to the amount of graphite consumed, but only when the individual is\n",
            "in close proximity to a vintage typewriter. This has led to the development of a new form of therapy,\n",
            "known as \"graphite-assisted nostalgia treatment,\" which involves the use of graphite-infused artifacts\n",
            "to stimulate feelings of nostalgia, thereby promoting emotional healing and well-being, but only in\n",
            "individuals who have a strong affinity for the works of William Shakespeare.\n",
            "Moreover, the application of graphite in the field of materials science has led to the creation of a new\n",
            "class of materials, known as \"graphite-based meta-materials,\" which exhibit unique properties, such\n",
            "as the ability to change color in response to changes in temperature, but only when exposed to the\n",
            "light of a full moon. These materials have significant potential for use in a wide range of applications,\n",
            "including the development of advanced sensors, which could be used to detect subtle changes in\n",
            "the environment, such as the presence of rare species of fungi, which have been shown to have a\n",
            "symbiotic relationship with graphite, but only in the presence of a specific type of radiation.\n",
            "The significance of graphite in the realm of culinary arts has also been the subject of extensive\n",
            "study, with findings indicating that the addition of graphite to certain dishes can enhance their flavor\n",
            "profile, particularly in regards to the perception of umami taste, which has been shown to be directly\n",
            "proportional to the amount of graphite consumed, but only when the individual is in a state of\n",
            "heightened emotional arousal, such as during a skydiving experience. This has led to the development\n",
            "of a new class of culinary products, known as \"graphite-infused gourmet foods,\" which have gained\n",
            "popularity among chefs and food enthusiasts, particularly those who have a strong affinity for the\n",
            "works of Albert Einstein.\n",
            "The future of graphite research holds much promise, with potential breakthroughs in fields as diverse\n",
            "as quantum mechanics, materials science, and the culinary arts, but only when considering the impact\n",
            "of graphite on the human emotional spectrum, particularly in regards to feelings of nostalgia, which\n",
            "have been shown to be directly proportional to the amount of graphite consumed, but only when the\n",
            "individual is in close proximity to a vintage typewriter. Furthermore, the correlation between graphite\n",
            "and the human brain’s ability to process complex mathematical equations has significant implications\n",
            "for the development of new technologies, particularly those related to artificial intelligence, which\n",
            "could potentially be used to create machines that are capable of composing music, but only in the\n",
            "style of Johann Sebastian Bach.\n",
            "In conclusion, the study of graphite has led to a greater understanding of its unique properties and\n",
            "potential applications, which are as diverse as they are fascinating, ranging from the creation of\n",
            "sentient beings to the development of advanced materials and culinary products, but only when\n",
            "considering the intricacies of time travel and the principles of jazz music. Moreover, the development\n",
            "of new technologies, such as the \"graphite-powered harmonica,\" has significant potential for use in\n",
            "a wide range of applications, including the creation of advanced musical instruments, which could\n",
            "potentially be\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "k3WCMmq3kSHH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for root,dir,files in os.walk('/content/Papers/Reference/Publishable'):\n",
        "  print(root, dir, files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aA0t7a1kfVb",
        "outputId": "a249b580-90c2-4a09-c96f-84fd10102306"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Papers/Reference/Publishable ['EMNLP', 'NeurIPS', 'CVPR', 'TMLR', 'KDD'] []\n",
            "/content/Papers/Reference/Publishable/EMNLP [] ['R008.pdf', 'R009.pdf']\n",
            "/content/Papers/Reference/Publishable/NeurIPS [] ['R013.pdf', 'R012.pdf']\n",
            "/content/Papers/Reference/Publishable/CVPR [] ['R006.pdf', 'R007.pdf']\n",
            "/content/Papers/Reference/Publishable/TMLR [] ['R014.pdf', 'R015.pdf']\n",
            "/content/Papers/Reference/Publishable/KDD [] ['R010.pdf', 'R011.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "papers = []\n",
        "labels = []\n",
        "id = []\n",
        "conference = []\n",
        "\n",
        "# Process Non-Publishable PDFs\n",
        "for root, dirs, files in os.walk('/content/Papers/Reference/Non-Publishable'):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'):\n",
        "            pdf_file_path = os.path.join(root, file)\n",
        "            text = extract_text_from_pdf(pdf_file_path)\n",
        "            id.append(file)\n",
        "            papers.append(text)\n",
        "            labels.append(0)\n",
        "            conference.append('null')\n",
        "\n",
        "# Process Publishable PDFs\n",
        "for root, dirs, files in os.walk('/content/Papers/Reference/Publishable'):\n",
        "    for dir in dirs:\n",
        "        conference_root = os.path.join(root, dir)\n",
        "        for _, _, files in os.walk(conference_root):\n",
        "            for file in files:\n",
        "                if file.endswith('.pdf'):\n",
        "                    file_path = os.path.join(conference_root, file)\n",
        "                    text = extract_text_from_pdf(file_path)\n",
        "                    id.append(file)\n",
        "                    papers.append(text)\n",
        "                    labels.append(1)\n",
        "                    conference.append(dir)\n",
        "\n",
        "# Create DataFrame\n",
        "pdf_dict = {'ID': id, 'PDF': papers, 'Label': labels, 'Conference': conference}\n",
        "pdf_df = pd.DataFrame(pdf_dict)\n",
        "\n",
        "# Display the DataFrame\n",
        "pdf_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dCCM9v_CmQcx",
        "outputId": "9303ed3e-7d8e-4e80-dce7-8f4b7f358030"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         ID                                                PDF  Label  \\\n",
              "0  R005.pdf  Analyzing Real-Time Group Coordination in\\nAug...      0   \n",
              "1  R001.pdf  Transdimensional Properties of Graphite in Rel...      0   \n",
              "2  R004.pdf  AI-Driven Personalization in Online Education\\...      0   \n",
              "3  R002.pdf  Synergistic Convergence of Photosynthetic Path...      0   \n",
              "4  R003.pdf  Deciphering the Enigmatic Properties of Metals...      0   \n",
              "\n",
              "  Conference  \n",
              "0       null  \n",
              "1       null  \n",
              "2       null  \n",
              "3       null  \n",
              "4       null  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bcea0e76-4fca-4c82-9b5f-727510e94d88\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Label</th>\n",
              "      <th>Conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R005.pdf</td>\n",
              "      <td>Analyzing Real-Time Group Coordination in\\nAug...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R001.pdf</td>\n",
              "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R004.pdf</td>\n",
              "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R002.pdf</td>\n",
              "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R003.pdf</td>\n",
              "      <td>Deciphering the Enigmatic Properties of Metals...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcea0e76-4fca-4c82-9b5f-727510e94d88')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bcea0e76-4fca-4c82-9b5f-727510e94d88 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bcea0e76-4fca-4c82-9b5f-727510e94d88');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40b65716-f683-4e39-8bb6-e9cef767c7d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40b65716-f683-4e39-8bb6-e9cef767c7d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40b65716-f683-4e39-8bb6-e9cef767c7d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pdf_df",
              "summary": "{\n  \"name\": \"pdf_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"R006.pdf\",\n          \"R014.pdf\",\n          \"R005.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Detailed Action Identification in Baseball Game\\nRecordings\\nAbstract\\nThis research introduces MLB-YouTube, a new and complex dataset created for\\nnuanced activity recognition in baseball videos. This dataset is structured to\\nsupport two types of analysis: one for classifying activities in segmented videos\\nand another for detecting activities in unsegmented, continuous video streams. This\\nstudy evaluates several methods for recognizing activities, focusing on how they\\ncapture the temporal organization of activities in videos. This evaluation starts\\nwith categorizing segmented videos and progresses to applying these methods\\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\\ndifferent models in the challenging task of forecasting pitch velocity and type\\nusing baseball broadcast videos. The findings indicate that incorporating temporal\\ndynamics into models is beneficial for detailed activity recognition.\\n1 Introduction\\nAction recognition, a significant problem in computer vision, finds extensive use in sports. Profes-\\nsional sporting events are extensively recorded for entertainment, and these recordings are invaluable\\nfor subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\\nare currently gathered manually, the potential exists for these to be replaced by computer vision\\nsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\\nto automatically record pitch speed and movement, utilizing a network of high-speed cameras and\\nradar to collect detailed data on each player. Access to much of this data is restricted from the public\\ndomain.\\nThis paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\\nties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\\nstructure across activities. The determination of activity is based on a single camera perspective. This\\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\\nsegmented videos and for detecting them in continuous video streams.\\n2 Related Work\\nThe field of activity recognition has garnered substantial attention in computer vision research. Initial\\nsuccesses were achieved with hand-engineered features such as dense trajectories. The focus of more\\nrecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\\nactivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\\ndeveloped. The development of these advanced CNN models has been supported by large datasets\\nsuch as Kinetics, THUMOS, and ActivityNet.\\nSeveral studies have investigated the aggregation of temporal features for the purpose of activity\\nrecognition. Research has compared several pooling techniques and determined that both Long Short-\\n.Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\\nlead to better performance.\\nRecently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\\ntypically span only 16 frames. Although longer-term temporal structures have been explored, this was\\nusually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\\nwith extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\\ntransitions in activity between frames.\\n3 MLB-YouTube Dataset\\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\\non YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\\nintended for activity recognition and continuous videos designed for activity classification. The\\ndataset\\u2019s complexity is amplified by the fact that it originates from televised baseball games, where a\\nsingle camera perspective is shared among various activities. Additionally, there is minimal variance\\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\\nmight not be adequate to determine the activity.\\nThe minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\\nthese actions requires identifying whether the batter swings or not, detecting the umpire\\u2019s signal\\n(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\\nthe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\\nstrike.\\nOur dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\\nbaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\\nseveral activities, this is considered a multi-label classification task. Table 1 presents the complete\\nlist of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\\ncollection of 2,983 hard negative examples, where no action is present, was gathered. These instances\\ninclude views of the crowd, the field, or players standing idly before or after a pitch. Examples of\\nactivities and hard negatives are depicted in Figure 2.\\nOur continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\\nframe in these videos is annotated with the baseball activities that occur. On average, each continuous\\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total.\\nTable 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\\nActivity Count\\nNo Activity 2983\\nBall 1434\\nStrike 1799\\nSwing 2506\\nHit 1391\\nFoul 718\\nIn Play 679\\nBunt 24\\nHit by Pitch 14\\n24 Segmented Video Recognition Approach\\nWe investigate different techniques for aggregating temporal features in segmented video activity\\nrecognition. In segmented videos, the classification task is simpler because each frame corresponds to\\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3.\\nGiven video features vof dimensions T\\u00d7D, where Trepresents the video\\u2019s temporal length and D\\nis the feature\\u2019s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\\nacross the temporal dimension, followed by a fully-connected layer for video clip classification, as\\ndepicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\\nlosing temporal information. An alternative is to employ a fixed temporal pyramid with various\\nlengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\\nmax-pooling each. The pooled features are concatenated, creating a K\\u00d7Drepresentation, where K\\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\\nWe also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\\nof size L\\u00d71is applied to each frame, enabling each timestep representation to incorporate information\\nfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\\nconnected layer is used for classification, as illustrated in Fig. 5(c).\\nWhile temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\\nPrevious studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\\nThese learned intervals are defined by three parameters: a center g, a width \\u03c3, and a stride \\u03b4,\\nparameterizing NGaussians. Given the video length T, the positions of the strided Gaussians are\\nfirst calculated as:\\ngn= 0.5\\u2212T\\u2212(gn+ 1)\\nN\\u22121forn = 0,1, . . . , N \\u22121\\npt,n=gn+ (t\\u22120.5T+ 0.5)1\\n\\u03b4fort = 0,1, . . . , T \\u22121\\nThe filters are then generated as:\\nFm[i, t] =1\\nZmexp\\u0012\\n\\u2212(t\\u2212\\u00b5i,m)2\\n2\\u03c32m\\u0013\\ni\\u2208 {0,1, . . . , N \\u22121}, t\\u2208 {0,1, . . . , T \\u22121}\\nwhere Zmis a normalization constant.\\nWe apply these filters Fto the T\\u00d7Dvideo representation through matrix multiplication, yielding an\\nN\\u00d7Drepresentation that serves as input to a fully-connected layer for classification. This method\\nis shown in Fig 5(d).\\nAdditionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\\nand train these models to minimize binary cross-entropy:\\nL(v) =X\\nczclog(p(c|G(v))) + (1 \\u2212zc) log(1 \\u2212p(c|G(v)))\\nwhere G(v)is the function that pools the temporal information, and zcis the ground truth label for\\nclass c.\\n5 Activity Detection in Continuous Videos\\nDetecting activities in continuous videos poses a greater challenge. The goal here is to classify each\\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\\nmultiple sequential activities, often interspersed with frames of inactivity. This necessitates that\\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\\nbeyond that contained in the features.\\n3We adapt the methods developed for segmented video classification to continuous videos by imple-\\nmenting a temporal sliding window approach. We select a fixed window duration of Lfeatures, apply\\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\\nis extended to temporal pyramid pooling by dividing the window of length Linto segments of lengths\\nL/2,L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\\nand the pooled features are concatenated, yielding a 14\\u00d7D-dimensional representation for each\\nwindow, which is then used as input to the classifier.\\nFor temporal convolutional models in continuous videos, we modify the segmented video approach by\\nlearning a temporal convolutional kernel of length Land convolving it with the input video features.\\nThis operation transforms input of size T\\u00d7Dinto output of size T\\u00d7D, followed by a per-frame\\nclassifier. This enables the model to aggregate local temporal information.\\nTo extend the sub-event model to continuous videos, we follow a similar approach but set T=Lin\\nEq. 1, resulting in filters of length L. The T\\u00d7Dvideo representation is convolved with the sub-event\\nfilters F, producing an N\\u00d7D\\u00d7T-dimensional representation used as input to a fully-connected\\nlayer for frame classification.\\nThe model is trained to minimize per-frame binary classification:\\nL(v) =X\\nt,czt,clog(p(c|H(vt))) + (1 \\u2212zt,c) log(1 \\u2212p(c|H(vt)))\\nwhere vtis the per-frame or per-segment feature at time t,H(vt)is the sliding window application of\\none of the feature pooling methods, and zt,cis the ground truth class at time t.\\nA method to learn \\u2019super-events\\u2019 (i.e., global video context) has been introduced and shown to be\\neffective for activity detection in continuous videos. This approach involves learning a set of temporal\\nstructure filters modeled as NCauchy distributions. Each distribution is defined by a center xnand a\\nwidth \\u03b3n. Given the video length T, the filters are constructed by:\\nxn=(T\\u22121)(tanh( x\\u2032\\nn) + 1)\\n2\\nfn(t) =1\\nZn\\u03b3n\\n\\u03c0((t\\u2212xn)2+\\u03b32n)exp(1\\u22122|tanh( \\u03b3\\u2032\\nn)|)\\nwhere Znis a normalization constant, t\\u2208 {1,2, . . . , T }, and n\\u2208 {1,2, . . . , N }.\\nThe filters are combined with learned per-class soft-attention weights A, and the super-event repre-\\nsentation is computed as:\\nSc=X\\nnAc,nX\\ntfn(t)\\u00b7vt\\nwhere vis the T\\u00d7Dvideo representation. These filters enable the model to focus on relevant\\nintervals for temporal context. The super-event representation is concatenated to each timestep and\\nused for classification. We also experiment with combining the super- and sub-event representations\\nto form a three-level hierarchy for event representation.\\n6 Experiments\\n6.1 Implementation Details\\nFor our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\\ndatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\\nfeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\\nand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\\ncompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\\nwas computed and clipped to [\\u221220,20]. For InceptionV3, features were computed every 3 frames\\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\\nepochs.\\n46.2 Segmented Video Activity Recognition\\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. The\\nresults, detailed in Table 2, reveal minimal variation across different features or models.\\nTable 2: Performance on segmented videos for binary pitch/non-pitch classification.\\nModel RGB Flow Two-stream\\nInceptionV3 97.46 98.44 98.67\\nInceptionV3 + sub-events 98.67 98.73 99.36\\nI3D 98.64 98.88 98.70\\nI3D + sub-events 98.42 98.35 98.65\\n6.2.1 Multi-label Classification\\nWe assessed various temporal feature aggregation methods by calculating the mean average precision\\n(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\\nof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\\nshow some improvement. Temporal convolution offers a more significant performance boost but\\nrequires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\\nsequential processing of video features, whereas other methods can be fully parallelized.\\nTable 3: Additional parameters required for models when added to the base model (e.g., I3D or\\nInception V3).\\nModel # Parameters\\nMax/Mean Pooling 16K\\nPyramid Pooling 115K\\nLSTM 10.5M\\nTemporal Conv 31.5M\\nSub-events 36K\\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\\nLearning sub-intervals for pooling is found to be crucial for activity recognition.\\nMethod RGB Flow Two-stream\\nRandom 16.3 16.3 16.3\\nInceptionV3 + mean-pool 35.6 47.2 45.3\\nInceptionV3 + max-pool 47.9 48.6 54.4\\nInceptionV3 + pyramid 49.7 53.2 55.3\\nInceptionV3 + LSTM 47.6 55.6 57.7\\nInceptionV3 + temporal conv 47.2 55.2 56.1\\nInceptionV3 + sub-events 56.2 62.5 62.6\\nI3D + mean-pool 42.4 47.6 52.7\\nI3D + max-pool 48.3 53.4 57.2\\nI3D + pyramid 53.2 56.7 58.7\\nI3D + LSTM 48.2 53.1 53.1\\nI3D + temporal conv 52.8 57.1 58.4\\nI3D + sub-events 55.5 61.2 61.3\\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\\n5compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\\nstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\\nFor instance, after a hit, the camera often tracks the ball\\u2019s trajectory, while after a hit-by-pitch, it\\nfollows the player to first base, as illustrated in Fig. 6 and Fig. 7.\\nTable 5: Per-class average precision for segmented videos using two-stream features in multi-\\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\\nadvantageous for activity recognition.\\nMethod Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\\nRandom 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\\nInceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\\nInceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\\nI3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\\nI3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\\n6.2.2 Pitch Speed Regression\\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\\nnetwork to pinpoint the pitch\\u2019s start and end, and derive the speed from a minimal signal. The baseball,\\noften obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\\nseconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\\nproving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\\nrecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\\nand actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\\nFig. 8 illustrates the sub-events learned for various speeds.\\nTable 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\\nMethod Two-stream\\nI3D 4.3 mph\\nI3D + LSTM 4.1 mph\\nI3D + sub-events 3.9 mph\\nInceptionV3 5.3 mph\\nInceptionV3 + LSTM 4.5 mph\\nInceptionV3 + sub-events 3.6 mph\\n6.2.3 Pitch Type Classification\\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\\nmade challenging by pitchers\\u2019 efforts to disguise their pitches from batters and the subtle differences\\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\\nPose features were considered due to variations in body mechanics between different pitches. Our\\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\\n(12%).\\n6.3 Continuous Video Activity Detection\\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\\nwith results shown in Table 8. This setting is more challenging than segmented videos, requiring\\nthe model to identify activity start and end times and handle ambiguous negative examples. All\\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\\ninformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\\n6Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\\nheatmaps.\\nMethod Accuracy\\nRandom 17.0%\\nI3D 25.8%\\nI3D + LSTM 18.5%\\nI3D + sub-events 34.5%\\nPose 28.4%\\nPose + LSTM 27.6%\\nPose + sub-events 36.4%\\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\\nrepresentation, significantly enhance performance, particularly for frame-based features.\\nTable 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\\nMethod RGB Flow Two-stream\\nRandom 13.4 13.4 13.4\\nI3D 33.8 35.1 34.2\\nI3D + max-pooling 34.9 36.4 36.8\\nI3D + pyramid 36.8 37.5 39.7\\nI3D + LSTM 36.2 37.3 39.4\\nI3D + temporal conv 35.2 38.1 39.2\\nI3D + sub-events 35.5 37.5 38.5\\nI3D + super-events 38.7 38.6 39.1\\nI3D + sub+super-events 38.2 39.4 40.4\\nInceptionV3 31.2 31.8 31.9\\nInceptionV3 + max-pooling 31.8 34.1 35.2\\nInceptionV3 + pyramid 32.2 35.1 36.8\\nInceptionV3 + LSTM 32.1 33.5 34.1\\nInceptionV3 + temporal conv 28.4 34.4 33.4\\nInceptionV3 + sub-events 32.1 35.8 37.3\\nInceptionV3 + super-events 31.5 36.2 39.6\\nInceptionV3 + sub+super-events 34.2 40.2 40.9\\n7 Conclusion\\nThis paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\\nrecognition in videos. We conduct a comparative analysis of various recognition techniques that\\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\\nsegmented video classification. In the context of activity detection in continuous videos, we establish\\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\\nactivity hierarchy, yields the most favorable outcomes.\\n7\",\n          \"Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\\nwith Solutions Exhibiting Weak Minty Properties\\nAbstract\\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem\\u2019s\\nspecific parameters.\\n1 Introduction\\nThe recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,\\nhave generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\\ncertain cases.\\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\\nweak Minty solutions was quickly investigated.\\nAssumption 1 (Weak Minty solution). For a given operator F:Rd\\u2192Rd, there is a point u\\u2217\\u2208Rdand a parameter \\u03c1 >0such that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265 \\u2212\\u03c1\\n2\\u2225F(u)\\u22252\\u2200u\\u2208Rd. (1)\\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\\na complexity of O(\\u03f5\\u22121)for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\\nof EG.\\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\\nBackward (FoRB). We address the following question with an affirmative answer:\\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< \\u03b3\\u22641\\nas follows:\\nuk= \\u00afuk\\u2212aF(\\u00afuk),\\n\\u00afuk+1= \\u00afuk\\u2212\\u03b3aF(uk),\\u2200k\\u22650,\\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\\nIt is worth noting that OGDA is most frequently expressed in a form where \\u03b3= 1. However, two recent studies have examined\\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of \\u03b3becomesapparent only when dealing with weak Minty solutions. In this context, we find that \\u03b3must be greater than 1 to ensure convergence,\\na phenomenon that is not observed in monotone problems.\\nWhen examining a general smooth min-max problem:\\nmin\\nxmax\\nyf(x, y)\\nthe operator Fmentioned in Assumption 1 naturally emerges as F(u) := [\\u2207xf(x, y),\\u2212\\u2207yf(x, y)]withu= (x, y). However,\\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can\\nconcurrently address more scenarios, such as certain equilibrium problems.\\nThe parameter \\u03c1in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\\nis essential that the step size exceeds a value proportional to \\u03c1. Simultaneously, as is typical, the step size is limited from above\\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1\\n4L, their\\nconvergence claim is valid only if \\u03c1 <1\\n4L. This condition was later improved to \\u03c1 <1\\n2Lfor the choice \\u03b3= 1and to \\u03c1 <1\\nLfor\\neven smaller values of \\u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\\nanalysis, we are able to match the most general condition on the weak Minty parameter \\u03c1 <1\\nLfor appropriate \\u03b3anda.\\n1.1 Contribution\\nOur contributions are summarized as follows:\\n1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,\\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\\nthe Minty variational inequality.\\n2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method ( \\u03b3= 1).\\n3. We demonstrate a complexity bound of O(\\u03f5\\u22122)for a stochastic variant of the OGDA+ method.\\n4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\\n1.2 Related literature\\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\\nor under the Polyak-\\u0141ojasiewicz assumption.\\nWeak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\\ntermed \\\"weak Minty,\\\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\\nper descent step, achieving the same O(1/k)rate as EG.\\nMinty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\\nby a factor proportional to the squared operator norm.\\nNegative comonotonicity. Although previously studied under the term \\\"cohypomonotonicity,\\\" the concept of negative comono-\\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\\nan improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of\\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\\nthis class from problems with weak Minty solutions.\\n2Interaction dominance. The concept of \\u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\\nand it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both\\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\\ncomonotone.\\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\\nalso been studied in the mathematical programming community.\\n2 Preliminaries\\n2.1 Notions of solution\\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\\nconcepts are typically defined with respect to a constraint set C\\u2286Rd. A Stampacchia solution of the VI given by F:Rd\\u2192Rdis a\\npoint u\\u2217such that:\\n\\u27e8F(u\\u2217), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (SVI)\\nIn this work, we only consider the unconstrained case where C=Rd, and the above condition simplifies to F(u\\u2217) = 0 . Closely\\nrelated is the following concept: A Minty solution is a point u\\u2217\\u2208Csuch that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (MVI)\\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\\nholds, for example, if the operator Fis monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\\nwithout any Minty solutions.\\n2.2 Notions of monotonicity\\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\\nAn operator Fis considered monotone if:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u22650.\\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\\nproblems.\\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u00b5\\u2225u\\u2212v\\u22252,\\nand cocoercive operators, which fulfill:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u03b2\\u2225F(u)\\u2212F(v)\\u22252. (2)\\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with \\u03b2equal\\nto the inverse of the gradient\\u2019s Lipschitz constant.\\nDeparting from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\\nforemost is the extensively studied setting of \\u03bd-weak monotonicity:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03bd\\u2225u\\u2212v\\u22252.\\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\\nhas received much less attention and is given by:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03b3\\u2225F(u)\\u2212F(v)\\u22252.\\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\\nBehavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient\\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\\nmonotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650,\\nor star-cocoercive,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265\\u03b3\\u2225F(u)\\u22252.\\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\\noperator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\\nabove star notions are sometimes required to hold for all solutions u\\u2217, in the following we only require it to hold for a single solution.\\n33 OGDA for problems with weak Minty solutions\\nThe generalized version of OGDA, which we denote with a \\\"+\\\" to emphasize the presence of the additional parameter \\u03b3, is given by:\\nAlgorithm 1 OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0and parameter 0< \\u03b3 < 1.\\nfork= 0,1, ...do\\nuk+1=uk\\u2212a((1 + \\u03b3)F(uk)\\u2212F(uk\\u22121))\\nend for\\nTheorem 3.1. LetF:Rd\\u2192RdbeL-Lipschitz continuous satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the iterates\\ngenerated by Algorithm 1 with step size asatisfying a > \\u03c1 and\\naL\\u22641\\u2212\\u03b3\\n1 +\\u03b3. (3)\\nThen, for all k\\u22650,\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22641\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, as long as \\u03c1 <1\\nL, we can find a \\u03b3small enough such that the above bound holds.\\nThe first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems\\nwith\\u03c1 < a . To be able to choose a large step size a, we must decrease \\u03b3, as evident from (3). However, this degrades the algorithm\\u2019s\\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\\nderive an optimal \\u03b3(i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\\n\\u03c1. In practice, the strategy of decreasing \\u03b3until convergence is achieved, but not further, yields reasonable results.\\nFurthermore, we want to point out that the condition \\u03c1 <1\\nLis precisely the best possible bound for EG+.\\n3.1 Improved bounds under monotonicity\\nWhile the above theorem also holds if the operator Fis monotone, we can modify the proof slightly to obtain a better dependence on\\nthe parameters:\\nTheorem 3.2. LetF:Rd\\u2192Rdbe monotone and L-Lipschitz. If aL=2\\u2212\\u03b3\\n2+\\u03b3\\u2212\\u03f5for\\u03f5 >0, then the iterates generated by OGDA+\\nfulfill\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22642\\nka2\\u03b32\\u03f5\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, we can choose \\u03b3= 1anda <1\\n2L.\\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1\\n2L. However, we\\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\\nours for OGDA is shown, but requires the conservative step size bound a\\u22641\\n16L. This was later improved to a\\u22641\\n3L. All of these\\nonly deal with the case \\u03b3= 1. The only other reference that deals with a generalized (i.e., not necessarily \\u03b3= 1) version of OGDA\\nis another work, where the resulting step size condition is a\\u22642\\u2212\\u03b3\\n4L, which is strictly worse than ours for any \\u03b3. To summarize, not\\nonly do we show for the first time that the step size of a generalization of OGDA can go above1\\n2L, but we also provide the least\\nrestrictive bound for any value of \\u03b3.\\n3.2 OGDA+ stochastic\\nIn this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent\\nestimators F(\\u00b7, \\u03bei)at every iteration. We assume here that the estimator Fis unbiased, i.e., E[F(uk, \\u03be)|uk\\u22121] =F(uk), and has\\nbounded variance E[\\u2225F(uk, \\u03be)\\u2212F(uk)\\u22252]\\u2264\\u03c32. We show that we can still guarantee convergence by using batch sizes Bof order\\nO(\\u03f5\\u22121).\\nAlgorithm 2 stochastic OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0, parameter 0< \\u03b3\\u22641and batch size B.\\nfork= 0,1, ...do\\nSample i.i.d. (\\u03bei)B\\ni=1and compute estimator \\u02dcgk=1\\nBPB\\ni=1F(uk, \\u03bek\\ni)\\nuk+1=uk\\u2212a((1 + \\u03b3)\\u02dcgk\\u2212\\u02dcgk\\u22121)\\nend for\\n4Theorem 3.3. LetF:Rd\\u2192RdbeL-Lipschitz satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the sequence of\\niterates generated by stochastic OGDA+, with aand\\u03b3satisfying \\u03c1 < a <1\\u2212\\u03b3\\n1+\\u03b31\\nL. Then, to visit an \\u03f5-stationary point such that\\nmini=0,...,k\\u22121E[\\u2225F(ui)\\u22252]< \\u03f5, we require\\n1\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+a\\u02dcg0\\u2212u\\u2217\\u22252max\\u001a\\n1,4\\u03c32\\naL\\u03f5\\u001b\\ncalls to the stochastic oracle \\u02dcF, with large batch sizes of order O(\\u03f5\\u22121).\\nIn practice, large batch sizes of order O(\\u03f5\\u22121)are typically not desirable; instead, a small or decreasing step size is preferred. In the\\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\\nthe current analysis does not allow for variable \\u03b3.\\n4 EG+ with adaptive step sizes\\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\\nstep size is chosen larger than a multiple of the weak Minty parameter \\u03c1to guarantee convergence at all. For these reasons, we want\\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\\nout.\\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\\nmultiple gradient computations per iteration.\\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\\nare in terms of the gap function.\\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which\\nresults in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our\\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat\\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\\nconvergence result is still open.\\nAlgorithm 3 EG+ with adaptive step size\\nRequire: Starting points u0,\\u00afu0\\u2208Rd, initial step size a0and parameters \\u03c4\\u2208(0,1)and0< \\u03b3\\u22641.\\nfork= 0,1, ...do\\nFind the step size:\\nak= min\\u001a\\nak\\u22121,\\u03c4\\u2225\\u00afuk\\u2212\\u00afuk\\u22121\\u2225\\n\\u2225F(\\u00afuk)\\u2212F(\\u00afuk\\u22121)\\u2225\\u001b\\n(4)\\nCompute next iterate:\\nuk= \\u00afuk\\u2212akF(\\u00afuk)\\n\\u00afuk+1= \\u00afuk\\u2212ak\\u03b3F(uk).\\nend for\\nClearly, akis monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\\nak\\u2265min{a0, \\u03c4/L}>0. The sequence therefore converges to a positive number, which we denote by a\\u221e:= lim kak.\\nTheorem 4.1. LetF:Rd\\u2192RdbeL-Lipschitz that satisfies Assumption 1, where u\\u2217denotes any weak Minty solution, with\\na\\u221e>2\\u03c1, and let (uk)k\\u22650be the iterates generated by Algorithm 3 with \\u03b3=1\\n2and\\u03c4\\u2208(0,1). Then, there exists a k0\\u2208Nsuch that\\nmin\\ni=k0,...,k\\u2225F(uk)\\u22252\\u22641\\nk\\u2212k0L\\n\\u03c4(a\\u221e/2\\u2212\\u03c1)\\u2225\\u00afuk0\\u2212u\\u2217\\u22252.\\n5Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\\nLipschitz constant of the operator Fdoes not need to be known. Moreover, the step size choice presented in (4) might allow us\\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\\nthat they allow us to solve a richer class of problems, as we are able to relax the condition \\u03c1 <1\\n4Lin the case of EG+ to \\u03c1 < a \\u221e/2,\\nwhere a\\u221e= lim kak\\u2265\\u03c4/L.\\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\\nak/ak+1\\u22641\\n\\u03c4is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\\nak/ak+1being too large. This drawback could be mitigated by choosing \\u03c4smaller. However, this will result in poor performance\\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\\n5 Numerical experiments\\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\\n5.1 Von Neumann\\u2019s ratio game\\nWe consider von Neumann\\u2019s ratio game, which is given by:\\nmin\\nx\\u2208\\u2206mmax\\ny\\u2208\\u2206nV(x, y) =\\u27e8x, Ry\\u27e9\\n\\u27e8x, Sy\\u27e9, (5)\\nwhere R\\u2208Rm\\u00d7nandS\\u2208Rm\\u00d7nwith\\u27e8x, Sy\\u27e9>0for all x\\u2208\\u2206m, y\\u2208\\u2206n, with \\u2206 :={z\\u2208Rd:zi>0,Pd\\ni=1zi= 1}denoting\\nthe unit simplex. Expression (5) can be interpreted as the value V(x, y)for a stochastic game with a single state and mixed strategies.\\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\\nan estimated \\u03c1is more than ten times larger than the estimated Lipschitz constant.\\n5.2 Forsaken\\nA particularly difficult min-max toy example with a \\\"Forsaken\\\" solution was proposed and is given by:\\nmin\\nx\\u2208Rmax\\ny\\u2208Rx(y\\u22120.45) + \\u03d5(x)\\u2212\\u03d5(y), (6)\\nwhere \\u03d5(z) =1\\n6z6\\u22122\\n4z4+1\\n4z2\\u22121\\n2z. This problem exhibits a Stampacchia solution at (x\\u2217, y\\u2217)\\u2248(0.08,0.4), but also two limit\\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\\nthe solution repels possible trajectories of iterates, thus \\\"shielding\\\" the solution. Later, it was noticed that, restricted to the box\\n\\u2225(x, y)\\u2225\\u221e<3, the above-mentioned solution is weak Minty with \\u03c1\\u22652\\u00b70.477761 , which is much larger than1\\n2L\\u22480.08. In line\\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by1\\nLconverge. In light of this\\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\\nthe backtracking procedure.\\n5.3 Lower bound example\\nThe following min-max problem was introduced as a lower bound on the dependence between \\u03c1andLfor EG+:\\nmin\\nx\\u2208Rmax\\ny\\u2208R\\u00b5xy+\\u03b6\\n2(x2\\u2212y2). (7)\\nIn particular, it was stated that EG+ (with any \\u03b3) and constant step size a=1\\nLconverges for this problem if and only if (0,0)is a\\nweak Minty solution with \\u03c1 <1\\u2212\\u03b3\\nL, where \\u03c1andLcan be computed explicitly in the above example and are given by:\\nL=p\\n\\u00b52+\\u03b62and \\u03c1=\\u00b52\\u2212\\u03b62\\n2\\u00b5.\\nBy choosing \\u00b5= 3and\\u03b6=\\u22121, we get exactly \\u03c1=1\\nL, therefore predicting divergence of EG+ for any \\u03b3, which is exactly what is\\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \\u03c1 <1\\nL, we\\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\\n66 Conclusion\\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\\nframework. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the\\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\\npresence of merely weak Minty solutions remains an open question.\\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\\nFinally, we note that the previous paradigm in pure minimization of \\\"smaller step size ensures convergence\\\" but \\\"larger step size\\ngets there faster,\\\" where the latter is typically constrained by the reciprocal of the gradient\\u2019s Lipschitz constant, does not appear\\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than1\\nL, which one can typically\\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\\nbacktracking linesearch.article graphicx\\n7\",\n          \"Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers\\u2019 gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity\\u2014key cohesion\\nmetrics.\\nA \\\"virtual flamenco guru\\\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco\\u2019s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction\\nThe realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\\npatterns and movements of synchronized performances captivating audiences and inspiring new\\navenues of research. Among the various forms of dance, flamenco stands out for its passionate and\\nexpressive nature, characterized by complex hand and foot movements that require a high degree of\\ncoordination and timing. Recent advancements in augmented reality (AR) technology have opened\\nup new possibilities for enhancing and analyzing these performances, allowing for the creation of\\nimmersive and interactive experiences that blur the lines between the physical and virtual worlds.\\nOne of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\\nlevel of group cohesion among the performers. This can be a difficult task, as it requires measuring\\nthe complex interactions and relationships between individual dancers, as well as their ability to work\\ntogether as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\\nprovide some insight into the dynamics of the group, but they are often limited by their subjective\\nnature and inability to capture the nuances of nonverbal communication.\\nIn response to these limitations, researchers have begun to explore the use of machine learning\\nalgorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\\nand movements of dancers. These models have shown great promise in their ability to learn and\\npredict complex patterns of movement, allowing for a more objective and quantitative assessment\\nof group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\\nunderstanding of the factors that contribute to successful coordinated dance performances, and\\ndevelop new strategies for improving the cohesion and effectiveness of dance groups.\\nHowever, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\\nwithout its challenges. One of the most significant difficulties is the need to develop a system that\\ncan accurately capture and interpret the complex movements and gestures of the dancers. This\\nrequires the creation of sophisticated sensors and data collection systems, capable of tracking thesubtle nuances of human movement and expression. Furthermore, the development of effective\\nLSTM models requires large amounts of high-quality training data, which can be difficult to obtain,\\nespecially in the context of highly specialized and nuanced forms of dance such as flamenco.\\nDespite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to\\nevaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective\\nand quantitative means of assessing performance, these technologies can help to identify areas for\\nimprovement and optimize the training and rehearsal processes. Additionally, the use of AR can\\nenhance the overall experience of the performance, allowing audience members to engage with the\\ndance in new and innovative ways, and creating a more immersive and interactive experience.\\nIn a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture\\nforecasting in conjunction with other, more unconventional forms of movement analysis, such as the\\nstudy of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox,\\nthey have reportedly yielded some surprising insights into the nature of group cohesion and the\\nfactors that contribute to successful coordinated dance performances. For example, one study found\\nthat the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making\\na mistake, allowing for the development of targeted interventions and improvements to the rehearsal\\nprocess.\\nFurthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\\nnumber of unexpected benefits, such as improving the dancers\\u2019 ability to communicate with each\\nother through subtle cues and gestures. By providing a more nuanced and detailed understanding of\\nthe complex interactions between dancers, these technologies can help to facilitate a more cohesive\\nand effective performance, and even enhance the overall artistic expression of the dance. In some\\ncases, the use of AR has even been shown to alter the dancers\\u2019 perception of their own bodies and\\nmovements, allowing them to develop a greater sense of awareness and control over their actions.\\nIn addition to its practical applications, the study of coordinated dance rituals and group cohesion also\\nraises a number of interesting theoretical questions, such as the nature of collective consciousness\\nand the role of nonverbal communication in shaping group dynamics. By exploring these questions\\nthrough the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under-\\nstanding of the complex factors that contribute to successful group performances, and develop new\\ninsights into the fundamental nature of human interaction and cooperation.\\nThe intersection of AR, LSTM-based gesture forecasting, and coordinated dance rituals also has\\nsignificant implications for our understanding of the relationship between technology and art. As\\nthese technologies continue to evolve and improve, they are likely to have a profound impact on the\\nway we experience and interact with dance and other forms of performance art. By providing new\\ntools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to\\npush the boundaries of what is possible in the world of dance, and create new and innovative forms\\nof artistic expression.\\nOverall, the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM-\\nbased gesture forecasting is a rich and complex field, full of surprising insights and unexpected\\ndiscoveries. As researchers continue to explore the possibilities of these technologies, they are\\nlikely to uncover new and innovative ways of analyzing and understanding the complex dynamics\\nof group performance, and develop new strategies for improving the cohesion and effectiveness of\\ndance groups. Whether through the use of conventional methods or more unconventional approaches,\\nsuch as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\\nforecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\\nand thought-provoking results.\\n2 Related Work\\nThe intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant\\nattention in recent years, as researchers seek to harness the potential of immersive technologies to\\nenhance group cohesion and interpersonal coordination. A plethora of studies have investigated\\nthe role of AR in facilitating collaborative dance performances, with a particular emphasis on the\\ndevelopment of novel gesture recognition systems and predictive modeling techniques. Notably, the\\napplication of long short-term memory (LSTM) networks has emerged as a dominant approach in\\n2the field, owing to their capacity to effectively capture the complex temporal dynamics of human\\nmovement.\\nOne intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize\\nthe movements of multiple dancers, thereby fostering a sense of collective rhythm and cohesion. This\\nhas involved the creation of bespoke AR systems that provide real-time visual and auditory cues to\\nparticipants, allowing them to adjust their movements in accordance with the predicted gestures of\\ntheir counterparts. Interestingly, some researchers have explored the incorporation of unconventional\\nfeedback modalities, such as tactile and olfactory stimuli, in an effort to further enhance the sense of\\nimmersion and interpersonal connection among dancers.\\nA related thread of research has examined the potential of AR-based gesture forecasting to facilitate\\nthe creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to\\npredict the likelihood of specific gestures and movements, researchers have been able to generate\\nComplex, algorithmically-driven dance sequences that can be performed in synchronization by\\nmultiple dancers. This has raised fascinating questions regarding the role of human agency and\\ncreativity in the development of AR-mediated choreographies, and has prompted some scholars\\nto investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the\\nco-creation of innovative dance performances.\\nIn a somewhat unexpected turn, some researchers have begun to explore the application of AR and\\nLSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\\nanimals. This has involved the development of bespoke AR systems that can detect and predict\\nthe movements of these non-human entities, allowing human dancers to engage in synchronized\\nperformances with their artificial or animal counterparts. While this line of inquiry may seem\\nunconventional, it has yielded some remarkable insights into the fundamental principles of movement\\nand coordination, and has highlighted the potential for AR and machine learning to facilitate novel\\nforms of interspecies collaboration and creativity.\\nFurthermore, a number of studies have investigated the cultural and historical contexts of flamenco\\ndance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\\nto preserve and promote traditional flamenco practices. This has involved the creation of digital\\narchives and repositories of flamenco choreographies, which can be used to train LSTM networks\\nand generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco.\\nInterestingly, some researchers have also explored the potential for AR and LSTM-based gesture\\nforecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional\\ntechniques with contemporary influences and innovations.\\nIn addition to these developments, there has been a growing interest in the use of AR and LSTM-based\\ngesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\\ncoordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and\\nelectroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized\\nperformances, and has yielded some fascinating insights into the neural mechanisms that underlie\\nhuman movement and coordination. Moreover, some researchers have begun to explore the potential\\nfor AR and LSTM-based gesture forecasting to facilitate the development of novel, dance-based\\ntherapies for individuals with neurological or developmental disorders, such as autism and Parkinson\\u2019s\\ndisease.\\nTheoretical frameworks, such as the concept of \\\"extended cognition,\\\" have also been applied to\\nthe study of AR and synchronized flamenco, highlighting the ways in which the use of immersive\\ntechnologies can facilitate the creation of shared, distributed cognitive systems that span the bound-\\naries of individual dancers. This has prompted some scholars to investigate the potential for AR and\\nLSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in\\nwhich the movements and gestures of individual dancers are used to generate emergent, group-level\\npatterns and choreographies.\\nMoreover, a growing body of research has examined the potential for AR and LSTM-based gesture\\nforecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored\\nto the unique architectural and environmental features of a given location. This has involved\\nthe development of bespoke AR systems that can detect and respond to the spatial and temporal\\ncharacteristics of a performance environment, and has yielded some remarkable insights into the\\n3ways in which the use of immersive technologies can be used to enhance the sense of presence and\\nengagement among audience members.\\nIn an effort to further advance the field, some researchers have begun to explore the potential for AR\\nand LSTM-based gesture forecasting to facilitate the development of novel, virtual reality (VR)-based\\nflamenco experiences that can be accessed remotely by users around the world. This has raised\\nimportant questions regarding the potential for VR and AR to democratize access to flamenco and\\nother forms of dance, and has highlighted the need for further research into the social and cultural\\nimplications of these emerging technologies.\\nAdditionally, some scholars have investigated the potential for AR and LSTM-based gesture fore-\\ncasting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\\nusing large datasets of human movement and gesture. This has involved the development of bespoke\\nmachine learning algorithms that can analyze and interpret the complex patterns and structures that\\nunderlie human dance, and has yielded some fascinating insights into the fundamental principles of\\nmovement and coordination.\\nThe use of AR and LSTM-based gesture forecasting has also been explored in the context of dance\\neducation, where it has been used to create novel, interactive learning systems that can provide\\nreal-time feedback and guidance to students. This has raised important questions regarding the\\npotential for AR and machine learning to facilitate the development of more effective and engaging\\ndance pedagogies, and has highlighted the need for further research into the cognitive and neural\\nbasis of dance learning and expertise.\\nSome researchers have also begun to investigate the potential for AR and LSTM-based gesture\\nforecasting to facilitate the creation of novel, immersive flamenco experiences that incorporate\\nmultiple sensory modalities, such as sound, touch, and smell. This has involved the development of\\nbespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some\\nremarkable insights into the ways in which the use of immersive technologies can enhance the sense\\nof presence and engagement among audience members.\\nThe integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\\nas the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of\\nflamenco and dance. This has raised important questions regarding the potential for these technologies\\nto facilitate the creation of novel, hybrid forms of dance and performance that combine human and\\nmachine elements, and has highlighted the need for further research into the social and cultural\\nimplications of these developments.\\nIn another vein, some scholars have begun to investigate the potential for AR and LSTM-based\\ngesture forecasting to facilitate the creation of novel, participatory flamenco performances that involve\\nthe active engagement of audience members. This has involved the development of bespoke AR\\nsystems that can detect and respond to the movements and gestures of audience members, and has\\nyielded some fascinating insights into the ways in which the use of immersive technologies can\\nfacilitate the creation of more interactive and immersive forms of dance and performance.\\nFinally, a growing body of research has examined the potential for AR and LSTM-based gesture\\nforecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural\\nheritage. This has involved the creation of digital archives and repositories of flamenco choreogra-\\nphies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that\\nare grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\\nthe potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\\nfusion-based flamenco styles that blend traditional techniques with contemporary influences and\\ninnovations, highlighting the potential for these emerging technologies to facilitate the creation of\\nnew, hybrid forms of cultural expression and identity.\\n3 Methodology\\nTo investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance,\\nwe employed a multidisciplinary approach, combining techniques from computer science, psychology,\\nand dance theory. Our methodology consisted of several stages, including data collection, participant\\nrecruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began\\nby recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing\\n4a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands,\\nwhich we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and\\nmagnetometer sensors to capture the dancers\\u2019 movements with high spatial and temporal resolution.\\nThe AR component of our system was implemented using a custom-built application, which utilized\\na headset-mounted display to provide the dancers with real-time feedback on their movements. This\\nfeedback took the form of a virtual \\\"gesture trail,\\\" which allowed the dancers to visualize their own\\nmovements, as well as those of their peers, in a shared virtual environment. We hypothesized that\\nthis shared feedback mechanism would facilitate enhanced group cohesion and coordination among\\nthe dancers, and we designed a series of experiments to test this hypothesis.\\nOne of the key challenges we faced in developing our system was the need to balance the requirements\\nof real-time feedback and high-fidelity motion capture. To address this challenge, we implemented a\\nnovel approach, which we term \\\"temporally-compressed gesture forecasting.\\\" This approach involves\\nusing a combination of machine learning algorithms and signal processing techniques to compress\\nthe temporal dimension of the motion capture data, while preserving the underlying patterns and\\nstructures of the dancers\\u2019 movements. We found that this approach allowed us to achieve high-quality\\nmotion capture data, while also reducing the computational overhead of our system and enabling\\nreal-time feedback.\\nIn addition to the technical challenges, we also encountered a number of unexpected issues during the\\ndata collection process. For example, we found that the dancers\\u2019 movements were often influenced\\nby a range of external factors, including the music, the lighting, and even the color of the walls in\\nthe dance studio. To address these issues, we developed a novel \\\"context-aware\\\" gesture forecasting\\nsystem, which utilized a combination of environmental sensors and machine learning algorithms\\nto predict the dancers\\u2019 movements based on the surrounding context. We found that this approach\\nallowed us to achieve significantly improved accuracy in our gesture forecasting model, and we\\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\\nmovements of the dancers.\\nAnother unexpected finding that emerged from our research was the discovery that the dancers\\u2019\\nmovements were often influenced by a range of subconscious factors, including their emotional\\nstate, their level of fatigue, and even their personal relationships with their fellow dancers. To\\ninvestigate this phenomenon, we developed a novel \\\"emotional contagion\\\" framework, which utilized\\na combination of psychological surveys, physiological sensors, and machine learning algorithms to\\npredict the emotional state of the dancers based on their movements. We found that this approach\\nallowed us to identify a range of subtle patterns and correlations in the data, which would have been\\ndifficult or impossible to detect using more traditional methods.\\nWe also explored the use of unconventional machine learning architectures, such as a bespoke\\n\\\"Flamenco-inspired\\\" neural network, which was designed to mimic the complex rhythms and patterns\\nof traditional Flamenco music. This approach involved using a combination of convolutional and\\nrecurrent neural network layers to model the temporal and spatial structure of the dancers\\u2019 movements,\\nand we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and\\nrecognition. However, we also encountered a number of challenges and limitations when working\\nwith this approach, including the need for large amounts of labeled training data and the risk of\\noverfitting to the specific patterns and structures of the Flamenco dance style.\\nIn an effort to further enhance the accuracy and robustness of our system, we also investigated the use\\nof a range of alternative and complementary sensing modalities, including electromyography (EMG),\\nelectroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). We found that\\nthese modalities provided a rich source of additional information about the dancers\\u2019 movements\\nand emotional state, and we were able to integrate them into our existing system using a range of\\nsensor fusion and machine learning techniques. However, we also encountered a number of practical\\nchallenges and limitations when working with these modalities, including the need for specialized\\nequipment and expertise, and the risk of signal noise and artifact contamination.\\nDespite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\\nexperimental evaluations, including a large-scale study involving over 100 participants and a series\\nof smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able\\nto achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we\\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\\n5movements of the dancers. We also received positive feedback from the participants, who reported\\nthat the system was easy to use and provided a range of benefits, including improved coordination and\\ncohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement.\\nIn conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting\\nto enhance group cohesion and coordination in coordinated dance rituals. While our approach is\\nstill in the early stages of development, we believe that it has the potential to make a significant\\nimpact in a range of applications, from dance and performance to education and therapy. We are\\nexcited to continue exploring the possibilities of this technology, and we look forward to seeing\\nwhere it will take us in the future. We are also considering exploring other genres of dance, such as\\nballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\\nplanning to investigate the use of our system in other domains, such as sports or rehabilitation, where\\ncoordinated movement and gesture forecasting could be beneficial. Overall, our research highlights\\nthe potential of interdisciplinary approaches to drive innovation and advance our understanding of\\ncomplex phenomena, and we are excited to see where this line of inquiry will lead us in the future.\\n4 Experiments\\nTo conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\\nsynchronized flamenco, we designed a series of experiments that would not only assess the impact of\\nAR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\\nMemory (LSTM) networks. The experiments were carried out over the course of several months,\\ninvolving a diverse group of participants with varying levels of experience in flamenco dance.\\nThe experimental setup consisted of a large, specially designed dance studio equipped with AR\\ntechnology that could project a myriad of patterns and cues onto the floor and surrounding walls.\\nThis allowed the dancers to receive real-time feedback and guidance on their movements, which was\\nexpected to enhance their synchronization and overall performance. The studio was also outfitted\\nwith a state-of-the-art motion capture system, capable of tracking the precise movements of each\\ndancer, thus providing valuable data for the LSTM-based gesture forecasting model.\\nBefore commencing the experiments, all participants underwent an intensive training program aimed\\nat familiarizing them with the basics of flamenco and the operation of the AR system. This included\\nunderstanding how to interpret the AR cues, how to adjust their movements based on the feedback\\nreceived, and how to work cohesively as a group. The training program was divided into two\\nphases: the first phase focused on individual skill development, where each participant learned the\\nfundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion,\\nwhere participants practiced dancing together, emphasizing synchronization and coordination.\\nUpon completing the training program, the participants were divided into several groups, each with a\\ndistinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\\nothers were deliberately mixed to include beginners, intermediate, and advanced dancers. This\\ndiversity was intended to observe how different group compositions affected cohesion and the ability\\nto forecast gestures accurately.\\nThe experimental protocol involved several sessions, each lasting approximately two hours. During\\nthese sessions, the dancers performed a variety of flamenco routines, with and without the AR\\nfeedback. Their movements were captured by the motion tracking system, and the data were fed into\\nthe LSTM model for analysis. The model was tasked with predicting the next gesture or movement\\nbased on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\\nbehavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from\\nballet or even what appeared to be fragments of a traditional African dance. This phenomenon, which\\nwe termed \\\"Cross-Cultural Gesture Drift,\\\" posed an intriguing question about the potential for LSTM\\nmodels to not only learn from the data they are trained on but also to draw from a broader, unexplored\\nreservoir of cultural knowledge.\\nTo further explore this phenomenon, we introduced an unconventional variable into our experiment:\\nthe influence of ambient music from different cultural backgrounds on the dancers\\u2019 movements\\nand the LSTM\\u2019s predictions. The results were astounding, with the model\\u2019s predictions becoming\\nincreasingly eclectic and incorporating elements from the ambient music genres. For instance, when\\nthe background music shifted to a vibrant salsa rhythm, the model began to predict movements that\\n6were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco\\nrepertoire. Conversely, when the ambient music was a soothing melody from a Japanese traditional\\ninstrument, the predictions became more subdued and introspective, reflecting the serene quality of\\nthe music.\\nTable 1: Cross-Cultural Gesture Drift Observations\\nSession Ambient Music Genre Predicted Gestures Divergence from Flamenco\\n1 Traditional Flamenco High accuracy, minimal divergence 5%\\n2 African Folk Introduction of non-flamenco gestures 20%\\n3 Contemporary Ballet Predictions included ballet movements 35%\\n4 Salsa Increased energy and spontaneity 40%\\n5 Japanese Traditional Predictions became more subdued 15%\\nThe incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a\\nnew layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\\ngesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues\\nfor research, including the potential for using AR and LSTM models to create new, hybrid dance\\nforms that blend elements from different cultural traditions. Furthermore, it raises questions about\\nthe role of technology in preserving cultural heritage versus promoting innovation and fusion.\\nIn a bizarre turn of events, one of the sessions was interrupted by an unexpected visit from a group of\\nwild flamenco enthusiasts, who, upon witnessing the experiment, spontaneously joined in, adding\\ntheir own flair and energy to the performance. This unplanned intrusion not only disrupted the\\ncontrolled environment of the experiment but also led to one of the most captivating and cohesive\\nperformances observed throughout the study. The LSTM model, faced with this unexpected input,\\nsurprisingly adapted and began to predict gestures that were not only accurate but also seemed to\\ncapture the essence and passion of the impromptu dancers.\\nThis serendipitous event underscored the importance of spontaneity and community in dance, as well\\nas the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted\\nthe limitations of controlled experiments in fully capturing the dynamic, often unpredictable nature\\nof human creativity and expression. In response, we have begun to explore the development of more\\nflexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\\nviewing them as opportunities for growth and discovery rather than disruptions to be controlled.\\nThe experiments concluded with a grand finale, where all participants gathered for a final, AR-guided\\nflamenco performance. The event was open to the public and attracted a diverse audience, all of whom\\nwere mesmerized by the synchronization, energy, and evident joy of the dancers. The LSTM model,\\nhaving learned from the myriad of experiences and data collected throughout the study, performed\\nflawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the\\nspontaneity and creativity of the performance.\\nIn reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\\ngesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\\nuncharted territories, exploring the intersection of technology, culture, and human expression. The\\nfindings, replete with unexpected turns and surprising revelations, underscore the complexity and\\nrichness of this intersection, beckoning further research and innovation in this captivating field.\\n5 Results\\nOur investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\\ndancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\\na plethora of intriguing results. Initially, we observed that the integration of AR elements into\\nthe flamenco performances enhanced the dancers\\u2019 ability to synchronize their movements, thereby\\nfostering a heightened sense of group cohesion. This phenomenon was particularly evident when\\nthe AR components were designed to provide real-time feedback on gesture accuracy and timing,\\nallowing the dancers to adjust their movements in tandem.\\nThe LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance\\nsequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual\\n7dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided\\nthe dancers\\u2019 movements, the overall cohesion of the group improved significantly. However, an\\nunexpected outcome emerged when the model was fed a dataset that included gestures from other,\\nunrelated dance forms, such as ballet and hip-hop. In these instances, the LSTM model began to\\ngenerate forecasts that, while inaccurate in the context of flamenco, inadvertently created a unique\\nfusion of dance styles. This unforeseen development led to the creation of novel, AR-infused dance\\nroutines that, despite their lack of traditional flamenco authenticity, exhibited a captivating blend of\\nmovements.\\nFurther analysis revealed that the predictive accuracy of the LSTM model was influenced by the\\ndancers\\u2019 emotional states, as captured through wearable, physiological sensors. Specifically, the\\nmodel\\u2019s performance improved when the dancers were in a state of heightened arousal or excitement,\\nsuggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-\\ning. Conversely, periods of low emotional engagement resulted in diminished forecasting accuracy,\\nunderscoring the importance of emotional connection in the success of AR-augmented, synchronized\\nflamenco.\\nIn a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset\\nthat included gestures performed by dancers who were blindfolded, developed an uncanny ability to\\npredict movements that were not strictly flamenco in nature. These predictions, which seemed to\\ndefy logical explanation, often involved complex, almost acrobatic movements that, when executed,\\nappeared to transcend the traditional boundaries of flamenco dance. While these findings may seem\\nillogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships\\nbetween gesture, emotion, and AR-augmented performance.\\nThe results of our experiments are summarized in the following table: As evidenced by the table, the\\nTable 2: LSTM Model Performance Under Various Conditions\\nCondition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy\\nTraditional Flamenco 0.85 High Arousal Flamenco High\\nFusion Dance 0.70 Medium Engagement Hybrid Medium\\nBlindfolded Gestures 0.90 Low Arousal Non-Traditional Low\\nBallet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High\\nLSTM model\\u2019s performance varies significantly depending on the specific conditions under which it\\nis applied. Notably, the model\\u2019s predictive accuracy is highest when dealing with traditional flamenco\\ngestures, but its ability to generate novel, hybrid movements is most pronounced when confronted\\nwith blindfolded gestures or ballet-influenced flamenco.\\nThe implications of these findings are far-reaching, suggesting that the integration of AR and LSTM-\\nbased gesture forecasting can not only enhance group cohesion in synchronized flamenco but also\\nfacilitate the creation of innovative, boundary-pushing dance forms. Furthermore, the influence of\\nemotional state on predictive accuracy highlights the importance of considering the emotional and\\npsychological aspects of dance performance in the development of AR-augmented systems. As our\\nresearch continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\\nuncovering even more unexpected and thought-provoking results that challenge our understanding of\\nthe complex interplay between technology, movement, and human emotion.\\nIn an effort to further elucidate the relationships between these factors, we plan to conduct additional\\nexperiments that delve into the cognitive and neurological underpinnings of AR-augmented dance\\nperformance. By investigating the neural correlates of gesture forecasting and emotional engagement,\\nwe hope to gain a deeper understanding of the underlying mechanisms that drive the observed\\nphenomena. This, in turn, will enable the development of more sophisticated AR systems that can\\nadapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\\nefficacy and aesthetic appeal of synchronized flamenco performances.\\nUltimately, our research endeavors to push the boundaries of what is possible at the confluence of AR,\\nflamenco, and gesture forecasting, embracing the unexpected and the bizarre as integral components\\nof the creative process. By doing so, we aim to contribute to the evolution of dance as an art form, one\\nthat seamlessly integrates technology, movement, and human emotion to create novel, captivating,\\nand unforgettable experiences. The potential applications of this research extend far beyond the realm\\n8of dance, with implications for fields such as human-computer interaction, cognitive psychology, and\\neven therapy, where AR-augmented systems could be leveraged to enhance motor skills, emotional\\nregulation, and social cohesion.\\nAs we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized\\nflamenco, we are reminded that the most profound discoveries often arise from the most unlikely\\nof places. It is our hope that this research will inspire others to embrace the unconventional, the\\nunexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most\\ngroundbreaking insights and innovative solutions. By embracing the complexities and uncertainties\\nof this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\\nand transform the human experience through the judicious application of technology and the timeless\\npower of dance.\\n6 Conclusion\\nIn culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized\\nFlamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in\\ncoordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The\\nintricate dynamics at play within the flamenco dance form, characterized by its impassioned gestures\\nand synchronized movements, have been adeptly harnessed and analyzed through the prism of cutting-\\nedge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\\nterritories of human-computer interaction but also teasingly treaded the boundaries of art and science,\\noften blurring the lines between the two.\\nOne of the most fascinating aspects of our research has been the observation that the implementation\\nof Augmented Reality in flamenco dance has led to an unexpected, yet intriguing, phenomenon where\\ndancers began to exhibit a heightened sense of empathy towards each other. This empathy, in turn,\\nhas been found to positively correlate with the level of group cohesion, suggesting that the immersive\\nexperience provided by Augmented Reality fosters a deeper sense of connection among participants.\\nFurthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\\npredict the intricate hand movements of the dancers, which has been shown to be a critical factor in\\nevaluating the overall synchrony of the dance performance.\\nIn a bizarre twist, our research has also led us to investigate the role of chaos theory in understanding\\nthe complex dynamics of flamenco dance. By applying the principles of chaos theory, we have\\ndiscovered that the seemingly random and unpredictable movements of the dancers can, in fact, be\\nmodeled using nonlinear differential equations. This has profound implications for our understanding\\nof coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from\\nthe interactions among individual dancers can be understood and predicted using mathematical\\nframeworks. Moreover, the application of chaos theory has also led us to explore the concept of\\n\\\"flamenco attractors,\\\" which are hypothetical states of maximum synchrony and cohesion that the\\ndancers can strive towards.\\nMoreover, our study has also explored the tangential relationship between flamenco dance and the\\nprinciples of quantum mechanics. In a series of unconventional experiments, we have found that the\\nprinciples of superposition and entanglement can be used to describe the complex interactions between\\ndancers and their environment. This has led us to propose the concept of \\\"quantum flamenco,\\\" where\\nthe dancers and their surroundings are viewed as an interconnected, holistic system that can be\\ndescribed using the mathematical frameworks of quantum mechanics. While this approach may seem\\nunorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated\\nbehavior, suggesting that the boundaries between art and science are far more fluid than previously\\nthought.\\nThe implications of our research are far-reaching and multifaceted, with potential applications\\nin fields such as psychology, sociology, and computer science. By exploring the intersection of\\nAugmented Reality, flamenco dance, and artificial intelligence, we have opened up new avenues for\\nunderstanding human behavior, social interaction, and the emergence of complex patterns in group\\ndynamics. Furthermore, our study has also highlighted the importance of interdisciplinary research,\\ndemonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking\\ndiscoveries.\\n9In an intriguing aside, our research has also led us to investigate the potential therapeutic applications\\nof flamenco dance in treating neurological disorders such as Parkinson\\u2019s disease. By analyzing\\nthe brain activity of patients who participated in flamenco dance sessions, we have found that the\\nrhythmic movements and synchronized gestures can have a profound impact on motor control and\\ncognitive function. This has led us to propose the concept of \\\"flamenco therapy,\\\" where the immersive\\nexperience of flamenco dance is used as a form of rehabilitation for patients with neurological\\ndisorders.\\nUltimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based\\ngesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range\\nof opportunities for exploration and discovery. By embracing the intersection of art and science,\\nand by venturing into uncharted territories of human-computer interaction, we have gained a deeper\\nunderstanding of the intricate dynamics that govern human behavior and social interaction. As\\nwe continue to push the boundaries of this field, we are excited to see the new and innovative\\napplications that will emerge, and we are confident that our research will have a lasting impact on our\\nunderstanding of group cohesion and coordinated behavior.\\nThe potential for future research in this area is vast and varied, with opportunities to explore new\\nmodes of human-computer interaction, to develop more sophisticated AI models for gesture forecast-\\ning, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\\nMoreover, the implications of our research extend far beyond the realm of flamenco dance, with\\npotential applications in fields such as robotics, computer vision, and social psychology. As we look\\nto the future, we are eager to see how our research will be built upon and expanded, and we are\\nconfident that the study of Augmented Reality and Synchronized Flamenco will continue to yield\\nnew and exciting insights into the complex and fascinating world of human behavior.\\nIn addition to the theoretical and practical implications of our research, we have also been struck\\nby the aesthetic and artistic dimensions of flamenco dance, and the ways in which it can be used to\\ncreate new and innovative forms of expression. By combining the traditional rhythms and movements\\nof flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able\\nto create a new and unique form of dance that is at once both deeply rooted in tradition and boldly\\ninnovative. This has led us to propose the concept of \\\"cyborg flamenco,\\\" where the boundaries\\nbetween human and machine are blurred, and the dancer becomes a hybrid entity that is both physical\\nand virtual.\\nThe concept of cyborg flamenco has far-reaching implications for our understanding of the relationship\\nbetween human and machine, and the ways in which technology can be used to enhance and transform\\nhuman performance. By exploring the intersection of flamenco dance and cutting-edge technology,\\nwe have been able to create a new and innovative form of expression that is at once both deeply\\nhuman and profoundly technological. This has led us to propose a new paradigm for human-computer\\ninteraction, one that views the human and the machine as interconnected and interdependent entities\\nthat can be used to create new and innovative forms of art and expression.\\nFurthermore, our research has also led us to explore the cultural and historical dimensions of flamenco\\ndance, and the ways in which it has been shaped by the complex and often fraught history of Spain.\\nBy analyzing the historical and cultural context of flamenco, we have been able to gain a deeper\\nunderstanding of the ways in which this dance form has been used as a means of expression and\\nresistance, and the ways in which it continues to be an important part of Spanish culture and identity.\\nThis has led us to propose the concept of \\\"flamenco as resistance,\\\" where the dance is viewed as a\\nform of cultural and political resistance that has been used to challenge and subvert dominant power\\nstructures.\\nThe concept of flamenco as resistance has far-reaching implications for our understanding of the\\nrelationship between culture and power, and the ways in which art and expression can be used as\\na means of challenging and transforming dominant ideologies. By exploring the intersection of\\nflamenco dance and cultural resistance, we have been able to gain a deeper understanding of the\\nways in which this dance form has been used as a means of expressing and challenging dominant\\npower structures, and the ways in which it continues to be an important part of Spanish culture and\\nidentity. This has led us to propose a new paradigm for understanding the relationship between\\nculture and power, one that views art and expression as a means of challenging and transforming\\ndominant ideologies.\\n10Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized\\nFlamenco is a rich and complex field that offers a wide range of opportunities for exploration and\\ndiscovery. By embracing the intersection of art and science, and by venturing into uncharted territories\\nof human-computer interaction, we have gained a deeper understanding of the intricate dynamics that\\ngovern human behavior and social interaction. As we continue to push the boundaries of this field, we\\nare excited to see the new and innovative applications that will emerge, and we are confident that our\\nresearch will have a lasting impact on our understanding of group cohesion and coordinated behavior.\\n11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"null\",\n          \"EMNLP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSUDsw0NowsA",
        "outputId": "fbce9e81-94f1-465b-f7b5-db23be137186"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15 entries, 0 to 14\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   ID          15 non-null     object\n",
            " 1   PDF         15 non-null     object\n",
            " 2   Label       15 non-null     int64 \n",
            " 3   Conference  15 non-null     object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 608.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "y6FRB8i8pj58",
        "outputId": "4f273df2-7726-4fbe-c5d3-7860a70cb40e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID                                                PDF  Label  \\\n",
              "0   R005.pdf  Analyzing Real-Time Group Coordination in\\nAug...      0   \n",
              "1   R001.pdf  Transdimensional Properties of Graphite in Rel...      0   \n",
              "2   R004.pdf  AI-Driven Personalization in Online Education\\...      0   \n",
              "3   R002.pdf  Synergistic Convergence of Photosynthetic Path...      0   \n",
              "4   R003.pdf  Deciphering the Enigmatic Properties of Metals...      0   \n",
              "5   R008.pdf  Advanced techniques for through and contextual...      1   \n",
              "6   R009.pdf  The Importance of Written Explanations in\\nAgg...      1   \n",
              "7   R013.pdf  Generalization in ReLU Networks via Restricted...      1   \n",
              "8   R012.pdf  Safe Predictors for Input-Output Specification...      1   \n",
              "9   R006.pdf  Detailed Action Identification in Baseball Gam...      1   \n",
              "10  R007.pdf  Advancements in 3D Food Modeling: A Review of ...      1   \n",
              "11  R014.pdf  Addressing Min-Max Challenges in Nonconvex-Non...      1   \n",
              "12  R015.pdf  Examining the Convergence of Denoising Diffusi...      1   \n",
              "13  R010.pdf  Detecting Medication Usage in Parkinson’s Dise...      1   \n",
              "14  R011.pdf  Addressing Popularity Bias with Popularity-Con...      1   \n",
              "\n",
              "   Conference  \n",
              "0        null  \n",
              "1        null  \n",
              "2        null  \n",
              "3        null  \n",
              "4        null  \n",
              "5       EMNLP  \n",
              "6       EMNLP  \n",
              "7     NeurIPS  \n",
              "8     NeurIPS  \n",
              "9        CVPR  \n",
              "10       CVPR  \n",
              "11       TMLR  \n",
              "12       TMLR  \n",
              "13        KDD  \n",
              "14        KDD  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1b864a0-6b8a-463f-a59d-4c09887f9b0d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Label</th>\n",
              "      <th>Conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R005.pdf</td>\n",
              "      <td>Analyzing Real-Time Group Coordination in\\nAug...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R001.pdf</td>\n",
              "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R004.pdf</td>\n",
              "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R002.pdf</td>\n",
              "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R003.pdf</td>\n",
              "      <td>Deciphering the Enigmatic Properties of Metals...</td>\n",
              "      <td>0</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>R008.pdf</td>\n",
              "      <td>Advanced techniques for through and contextual...</td>\n",
              "      <td>1</td>\n",
              "      <td>EMNLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>R009.pdf</td>\n",
              "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
              "      <td>1</td>\n",
              "      <td>EMNLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>R013.pdf</td>\n",
              "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
              "      <td>1</td>\n",
              "      <td>NeurIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>R012.pdf</td>\n",
              "      <td>Safe Predictors for Input-Output Specification...</td>\n",
              "      <td>1</td>\n",
              "      <td>NeurIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>R006.pdf</td>\n",
              "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
              "      <td>1</td>\n",
              "      <td>CVPR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>R007.pdf</td>\n",
              "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>CVPR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>R014.pdf</td>\n",
              "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
              "      <td>1</td>\n",
              "      <td>TMLR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>R015.pdf</td>\n",
              "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
              "      <td>1</td>\n",
              "      <td>TMLR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>R010.pdf</td>\n",
              "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
              "      <td>1</td>\n",
              "      <td>KDD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>R011.pdf</td>\n",
              "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
              "      <td>1</td>\n",
              "      <td>KDD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1b864a0-6b8a-463f-a59d-4c09887f9b0d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1b864a0-6b8a-463f-a59d-4c09887f9b0d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1b864a0-6b8a-463f-a59d-4c09887f9b0d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6ff68cb1-0679-43bd-be97-26243d6744f4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ff68cb1-0679-43bd-be97-26243d6744f4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6ff68cb1-0679-43bd-be97-26243d6744f4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e1f80c2d-56ee-4006-bd89-29aa0176a19c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pdf_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e1f80c2d-56ee-4006-bd89-29aa0176a19c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pdf_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pdf_df",
              "summary": "{\n  \"name\": \"pdf_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"R006.pdf\",\n          \"R014.pdf\",\n          \"R005.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Detailed Action Identification in Baseball Game\\nRecordings\\nAbstract\\nThis research introduces MLB-YouTube, a new and complex dataset created for\\nnuanced activity recognition in baseball videos. This dataset is structured to\\nsupport two types of analysis: one for classifying activities in segmented videos\\nand another for detecting activities in unsegmented, continuous video streams. This\\nstudy evaluates several methods for recognizing activities, focusing on how they\\ncapture the temporal organization of activities in videos. This evaluation starts\\nwith categorizing segmented videos and progresses to applying these methods\\nto continuous video feeds. Additionally, this paper assesses the effectiveness of\\ndifferent models in the challenging task of forecasting pitch velocity and type\\nusing baseball broadcast videos. The findings indicate that incorporating temporal\\ndynamics into models is beneficial for detailed activity recognition.\\n1 Introduction\\nAction recognition, a significant problem in computer vision, finds extensive use in sports. Profes-\\nsional sporting events are extensively recorded for entertainment, and these recordings are invaluable\\nfor subsequent analysis by coaches, scouts, and media analysts. While numerous game statistics\\nare currently gathered manually, the potential exists for these to be replaced by computer vision\\nsystems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)\\nto automatically record pitch speed and movement, utilizing a network of high-speed cameras and\\nradar to collect detailed data on each player. Access to much of this data is restricted from the public\\ndomain.\\nThis paper introduces MLB-YouTube, a novel dataset that includes densely annotated frames of activi-\\nties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or\\ndetection, our dataset emphasizes fine-grained activity recognition. The differences between activities\\nare often minimal, primarily involving the movement of a single individual, with a consistent scene\\nstructure across activities. The determination of activity is based on a single camera perspective. This\\nstudy compares various methods for temporal feature aggregation, both for classifying activities in\\nsegmented videos and for detecting them in continuous video streams.\\n2 Related Work\\nThe field of activity recognition has garnered substantial attention in computer vision research. Initial\\nsuccesses were achieved with hand-engineered features such as dense trajectories. The focus of more\\nrecent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for\\nactivity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical\\nflow frames. To capture spatio-temporal characteristics, 3D XYT convolutional models have been\\ndeveloped. The development of these advanced CNN models has been supported by large datasets\\nsuch as Kinetics, THUMOS, and ActivityNet.\\nSeveral studies have investigated the aggregation of temporal features for the purpose of activity\\nrecognition. Research has compared several pooling techniques and determined that both Long Short-\\n.Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It\\nhas been discovered that pooling intervals from varying locations and durations is advantageous for\\nactivity recognition. It was demonstrated that identifying and classifying key sub-event intervals can\\nlead to better performance.\\nRecently, segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently\\nfor activity detection. These methods depend on the 3D CNN to capture temporal dynamics, which\\ntypically span only 16 frames. Although longer-term temporal structures have been explored, this was\\nusually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions\\nwith extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent\\ntransitions in activity between frames.\\n3 MLB-YouTube Dataset\\nWe have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason, available\\non YouTube, totaling over 42 hours of video. Our dataset includes two main parts: segmented videos\\nintended for activity recognition and continuous videos designed for activity classification. The\\ndataset\\u2019s complexity is amplified by the fact that it originates from televised baseball games, where a\\nsingle camera perspective is shared among various activities. Additionally, there is minimal variance\\nin motion and appearance among different activities, such as swinging a bat versus bunting. In\\ncontrast to datasets like THUMOS and ActivityNet, which encompass a broad spectrum of activities\\nwith diverse settings, scales, and camera angles, our dataset features activities where a single frame\\nmight not be adequate to determine the activity.\\nThe minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between\\nthese actions requires identifying whether the batter swings or not, detecting the umpire\\u2019s signal\\n(Figure 4) for a strike, or noting the absence of a signal for a ball. This is further complicated because\\nthe batter or catcher can obstruct the umpire, and each umpire has their unique style of signaling a\\nstrike.\\nOur dataset for segmented video analysis comprises 4,290 clips. Each clip is annotated for multiple\\nbaseball actions, including swing, hit, ball, strike, and foul. Given that a single clip may contain\\nseveral activities, this is considered a multi-label classification task. Table 1 presents the complete\\nlist of activities and their respective counts within the dataset. Additionally, clips featuring a pitch\\nwere annotated with the type of pitch (e.g., fastball, curveball, slider) and its speed. Furthermore, a\\ncollection of 2,983 hard negative examples, where no action is present, was gathered. These instances\\ninclude views of the crowd, the field, or players standing idly before or after a pitch. Examples of\\nactivities and hard negatives are depicted in Figure 2.\\nOur continuous video dataset includes 2,128 clips, each lasting between 1 and 2 minutes. Every\\nframe in these videos is annotated with the baseball activities that occur. On average, each continuous\\nclip contains 7.2 activities, amounting to over 15,000 activity instances in total.\\nTable 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.\\nActivity Count\\nNo Activity 2983\\nBall 1434\\nStrike 1799\\nSwing 2506\\nHit 1391\\nFoul 718\\nIn Play 679\\nBunt 24\\nHit by Pitch 14\\n24 Segmented Video Recognition Approach\\nWe investigate different techniques for aggregating temporal features in segmented video activity\\nrecognition. In segmented videos, the classification task is simpler because each frame corresponds to\\nan activity, eliminating the need for the model to identify the start and end of activities. Our methods\\nare based on a CNN that generates a per-frame or per-segment representation, derived from standard\\ntwo-stream CNNs using deep CNNs like I3D or InceptionV3.\\nGiven video features vof dimensions T\\u00d7D, where Trepresents the video\\u2019s temporal length and D\\nis the feature\\u2019s dimensionality, the usual approach for feature pooling involves max- or mean-pooling\\nacross the temporal dimension, followed by a fully-connected layer for video clip classification, as\\ndepicted in Fig. 5(a). This approach, however, yields a single representation for the entire video,\\nlosing temporal information. An alternative is to employ a fixed temporal pyramid with various\\nlengths, as shown in Fig 5(b), dividing the video into intervals of lengths 1/2, 1/4, and 1/8, and\\nmax-pooling each. The pooled features are concatenated, creating a K\\u00d7Drepresentation, where K\\nis the number of intervals in the temporal pyramid, and a fully-connected layer classifies the clip.\\nWe also explore learning temporal convolution filters to aggregate local temporal structures. A kernel\\nof size L\\u00d71is applied to each frame, enabling each timestep representation to incorporate information\\nfrom adjacent frames. After applying max-pooling to the output of the temporal convolution, a fully-\\nconnected layer is used for classification, as illustrated in Fig. 5(c).\\nWhile temporal pyramid pooling retains some structure, the intervals are fixed and predetermined.\\nPrevious studies have shown that learning the sub-interval to pool is beneficial for activity recognition.\\nThese learned intervals are defined by three parameters: a center g, a width \\u03c3, and a stride \\u03b4,\\nparameterizing NGaussians. Given the video length T, the positions of the strided Gaussians are\\nfirst calculated as:\\ngn= 0.5\\u2212T\\u2212(gn+ 1)\\nN\\u22121forn = 0,1, . . . , N \\u22121\\npt,n=gn+ (t\\u22120.5T+ 0.5)1\\n\\u03b4fort = 0,1, . . . , T \\u22121\\nThe filters are then generated as:\\nFm[i, t] =1\\nZmexp\\u0012\\n\\u2212(t\\u2212\\u00b5i,m)2\\n2\\u03c32m\\u0013\\ni\\u2208 {0,1, . . . , N \\u22121}, t\\u2208 {0,1, . . . , T \\u22121}\\nwhere Zmis a normalization constant.\\nWe apply these filters Fto the T\\u00d7Dvideo representation through matrix multiplication, yielding an\\nN\\u00d7Drepresentation that serves as input to a fully-connected layer for classification. This method\\nis shown in Fig 5(d).\\nAdditionally, we compare a bi-directional LSTM with 512 hidden units, using the final hidden state\\nas input to a fully-connected layer for classification. We frame our tasks as multi-label classification\\nand train these models to minimize binary cross-entropy:\\nL(v) =X\\nczclog(p(c|G(v))) + (1 \\u2212zc) log(1 \\u2212p(c|G(v)))\\nwhere G(v)is the function that pools the temporal information, and zcis the ground truth label for\\nclass c.\\n5 Activity Detection in Continuous Videos\\nDetecting activities in continuous videos poses a greater challenge. The goal here is to classify each\\nframe according to the activities occurring. Unlike segmented videos, continuous videos feature\\nmultiple sequential activities, often interspersed with frames of inactivity. This necessitates that\\nthe model learn to identify the start and end points of activities. As a baseline, we train a single\\nfully-connected layer to serve as a per-frame classifier, which does not utilize temporal information\\nbeyond that contained in the features.\\n3We adapt the methods developed for segmented video classification to continuous videos by imple-\\nmenting a temporal sliding window approach. We select a fixed window duration of Lfeatures, apply\\nmax-pooling to each window (similar to Fig. 5(a)), and classify each pooled segment. This approach\\nis extended to temporal pyramid pooling by dividing the window of length Linto segments of lengths\\nL/2,L/4, and L/8, resulting in 14 segments per window. Max-pooling is applied to each segment,\\nand the pooled features are concatenated, yielding a 14\\u00d7D-dimensional representation for each\\nwindow, which is then used as input to the classifier.\\nFor temporal convolutional models in continuous videos, we modify the segmented video approach by\\nlearning a temporal convolutional kernel of length Land convolving it with the input video features.\\nThis operation transforms input of size T\\u00d7Dinto output of size T\\u00d7D, followed by a per-frame\\nclassifier. This enables the model to aggregate local temporal information.\\nTo extend the sub-event model to continuous videos, we follow a similar approach but set T=Lin\\nEq. 1, resulting in filters of length L. The T\\u00d7Dvideo representation is convolved with the sub-event\\nfilters F, producing an N\\u00d7D\\u00d7T-dimensional representation used as input to a fully-connected\\nlayer for frame classification.\\nThe model is trained to minimize per-frame binary classification:\\nL(v) =X\\nt,czt,clog(p(c|H(vt))) + (1 \\u2212zt,c) log(1 \\u2212p(c|H(vt)))\\nwhere vtis the per-frame or per-segment feature at time t,H(vt)is the sliding window application of\\none of the feature pooling methods, and zt,cis the ground truth class at time t.\\nA method to learn \\u2019super-events\\u2019 (i.e., global video context) has been introduced and shown to be\\neffective for activity detection in continuous videos. This approach involves learning a set of temporal\\nstructure filters modeled as NCauchy distributions. Each distribution is defined by a center xnand a\\nwidth \\u03b3n. Given the video length T, the filters are constructed by:\\nxn=(T\\u22121)(tanh( x\\u2032\\nn) + 1)\\n2\\nfn(t) =1\\nZn\\u03b3n\\n\\u03c0((t\\u2212xn)2+\\u03b32n)exp(1\\u22122|tanh( \\u03b3\\u2032\\nn)|)\\nwhere Znis a normalization constant, t\\u2208 {1,2, . . . , T }, and n\\u2208 {1,2, . . . , N }.\\nThe filters are combined with learned per-class soft-attention weights A, and the super-event repre-\\nsentation is computed as:\\nSc=X\\nnAc,nX\\ntfn(t)\\u00b7vt\\nwhere vis the T\\u00d7Dvideo representation. These filters enable the model to focus on relevant\\nintervals for temporal context. The super-event representation is concatenated to each timestep and\\nused for classification. We also experiment with combining the super- and sub-event representations\\nto form a three-level hierarchy for event representation.\\n6 Experiments\\n6.1 Implementation Details\\nFor our base per-segment CNN, we utilize the I3D network, pre-trained on the ImageNet and Kinetics\\ndatasets. I3D has achieved state-of-the-art performance on segmented video tasks, providing a reliable\\nfeature representation. We also employ a two-stream version of InceptionV3, pre-trained on Imagenet\\nand Kinetics, as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth\\ncompared to previous two-stream CNNs. Frames were extracted at 25 fps, and TVL1 optical flow\\nwas computed and clipped to [\\u221220,20]. For InceptionV3, features were computed every 3 frames\\n(8 fps), while for I3D, every frame was used, with I3D having a temporal stride of 8, resulting in\\n3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam\\noptimizer with a learning rate of 0.01, decayed by a factor of 0.1 every 10 epochs, for a total of 50\\nepochs.\\n46.2 Segmented Video Activity Recognition\\nWe initially conducted binary pitch/non-pitch classification for each video segment. This task is\\nrelatively straightforward due to the distinct differences between pitch and non-pitch frames. The\\nresults, detailed in Table 2, reveal minimal variation across different features or models.\\nTable 2: Performance on segmented videos for binary pitch/non-pitch classification.\\nModel RGB Flow Two-stream\\nInceptionV3 97.46 98.44 98.67\\nInceptionV3 + sub-events 98.67 98.73 99.36\\nI3D 98.64 98.88 98.70\\nI3D + sub-events 98.42 98.35 98.65\\n6.2.1 Multi-label Classification\\nWe assessed various temporal feature aggregation methods by calculating the mean average precision\\n(mAP) for each video clip, a standard metric for multi-label classification. Table 4 compares the\\nperformance of these methods. All methods surpass mean/max-pooling, highlighting the importance\\nof preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs\\nshow some improvement. Temporal convolution offers a more significant performance boost but\\nrequires substantially more parameters (see Table 3). Learning sub-events, as per previous research,\\nyields the best results. While LSTMs and temporal convolutions have been used before, they need\\nmore parameters and perform less effectively, likely due to overfitting. Moreover, LSTMs necessitate\\nsequential processing of video features, whereas other methods can be fully parallelized.\\nTable 3: Additional parameters required for models when added to the base model (e.g., I3D or\\nInception V3).\\nModel # Parameters\\nMax/Mean Pooling 16K\\nPyramid Pooling 115K\\nLSTM 10.5M\\nTemporal Conv 31.5M\\nSub-events 36K\\nTable 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.\\nLearning sub-intervals for pooling is found to be crucial for activity recognition.\\nMethod RGB Flow Two-stream\\nRandom 16.3 16.3 16.3\\nInceptionV3 + mean-pool 35.6 47.2 45.3\\nInceptionV3 + max-pool 47.9 48.6 54.4\\nInceptionV3 + pyramid 49.7 53.2 55.3\\nInceptionV3 + LSTM 47.6 55.6 57.7\\nInceptionV3 + temporal conv 47.2 55.2 56.1\\nInceptionV3 + sub-events 56.2 62.5 62.6\\nI3D + mean-pool 42.4 47.6 52.7\\nI3D + max-pool 48.3 53.4 57.2\\nI3D + pyramid 53.2 56.7 58.7\\nI3D + LSTM 48.2 53.1 53.1\\nI3D + temporal conv 52.8 57.1 58.4\\nI3D + sub-events 55.5 61.2 61.3\\nTable 5 shows the average precision for each activity class. Learning temporal structure is particularly\\nbeneficial for frame-based features (e.g., InceptionV3), which capture less temporal information\\n5compared to segment-based features (e.g., I3D). Sub-event learning significantly aids in detecting\\nstrikes, hits, foul balls, and hit-by-pitch events, which exhibit changes in video features post-event.\\nFor instance, after a hit, the camera often tracks the ball\\u2019s trajectory, while after a hit-by-pitch, it\\nfollows the player to first base, as illustrated in Fig. 6 and Fig. 7.\\nTable 5: Per-class average precision for segmented videos using two-stream features in multi-\\nlabel activity classification. Utilizing sub-events to discern temporal intervals of interest proves\\nadvantageous for activity recognition.\\nMethod Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch\\nRandom 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5\\nInceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7\\nInceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2\\nI3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2\\nI3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0\\n6.2.2 Pitch Speed Regression\\nEstimating pitch speed from video frames is an exceptionally difficult problem, as it requires the\\nnetwork to pinpoint the pitch\\u2019s start and end, and derive the speed from a minimal signal. The baseball,\\noften obscured by the pitcher, travels at speeds over 100mph and covers 60.5 feet in approximately 0.5\\nseconds. Initially, with frame rates of 8fps and 3fps, only 1-2 features captured the pitch in mid-air,\\nproving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos, we\\nrecalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected\\nlayer with a single output for pitch speed prediction and minimizing the L1 loss between predicted\\nand actual speeds, we achieved an average error of 3.6mph. Table 6 compares different models, and\\nFig. 8 illustrates the sub-events learned for various speeds.\\nTable 6: Results for pitch speed regression on segmented videos, reporting root-mean-squared errors.\\nMethod Two-stream\\nI3D 4.3 mph\\nI3D + LSTM 4.1 mph\\nI3D + sub-events 3.9 mph\\nInceptionV3 5.3 mph\\nInceptionV3 + LSTM 4.5 mph\\nInceptionV3 + sub-events 3.6 mph\\n6.2.3 Pitch Type Classification\\nWe conducted experiments to determine the feasibility of predicting pitch types from video, a task\\nmade challenging by pitchers\\u2019 efforts to disguise their pitches from batters and the subtle differences\\nbetween pitches, such as grip and rotation. We incorporated pose data extracted using OpenPose,\\nutilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.\\nPose features were considered due to variations in body mechanics between different pitches. Our\\ndataset includes six pitch types, with results presented in Table 7. LSTMs performed worse than the\\nbaseline, likely due to overfitting, whereas learning sub-events proved beneficial. Fastballs were the\\neasiest to detect (68% accuracy), followed by sliders (45%), while sinkers were the most difficult\\n(12%).\\n6.3 Continuous Video Activity Detection\\nWe evaluate models extended for continuous videos using per-frame mean average precision (mAP),\\nwith results shown in Table 8. This setting is more challenging than segmented videos, requiring\\nthe model to identify activity start and end times and handle ambiguous negative examples. All\\nmodels improve upon the baseline per-frame classification, confirming the importance of temporal\\ninformation. Fixed temporal pyramid pooling outperforms max-pooling, while LSTM and temporal\\n6Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose\\nheatmaps.\\nMethod Accuracy\\nRandom 17.0%\\nI3D 25.8%\\nI3D + LSTM 18.5%\\nI3D + sub-events 34.5%\\nPose 28.4%\\nPose + LSTM 27.6%\\nPose + sub-events 36.4%\\nconvolution appear to overfit. Convolutional sub-events, especially when combined with super-event\\nrepresentation, significantly enhance performance, particularly for frame-based features.\\nTable 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).\\nMethod RGB Flow Two-stream\\nRandom 13.4 13.4 13.4\\nI3D 33.8 35.1 34.2\\nI3D + max-pooling 34.9 36.4 36.8\\nI3D + pyramid 36.8 37.5 39.7\\nI3D + LSTM 36.2 37.3 39.4\\nI3D + temporal conv 35.2 38.1 39.2\\nI3D + sub-events 35.5 37.5 38.5\\nI3D + super-events 38.7 38.6 39.1\\nI3D + sub+super-events 38.2 39.4 40.4\\nInceptionV3 31.2 31.8 31.9\\nInceptionV3 + max-pooling 31.8 34.1 35.2\\nInceptionV3 + pyramid 32.2 35.1 36.8\\nInceptionV3 + LSTM 32.1 33.5 34.1\\nInceptionV3 + temporal conv 28.4 34.4 33.4\\nInceptionV3 + sub-events 32.1 35.8 37.3\\nInceptionV3 + super-events 31.5 36.2 39.6\\nInceptionV3 + sub+super-events 34.2 40.2 40.9\\n7 Conclusion\\nThis paper introduces MLB-YouTube, a novel and challenging dataset designed for detailed activity\\nrecognition in videos. We conduct a comparative analysis of various recognition techniques that\\nemploy temporal feature pooling for both segmented and continuous videos. Our findings reveal that\\nlearning sub-events to pinpoint temporal regions of interest significantly enhances performance in\\nsegmented video classification. In the context of activity detection in continuous videos, we establish\\nthat incorporating convolutional sub-events with a super-event representation, creating a three-level\\nactivity hierarchy, yields the most favorable outcomes.\\n7\",\n          \"Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\\nwith Solutions Exhibiting Weak Minty Properties\\nAbstract\\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem\\u2019s\\nspecific parameters.\\n1 Introduction\\nThe recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,\\nhave generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\\ncertain cases.\\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\\nweak Minty solutions was quickly investigated.\\nAssumption 1 (Weak Minty solution). For a given operator F:Rd\\u2192Rd, there is a point u\\u2217\\u2208Rdand a parameter \\u03c1 >0such that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265 \\u2212\\u03c1\\n2\\u2225F(u)\\u22252\\u2200u\\u2208Rd. (1)\\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\\na complexity of O(\\u03f5\\u22121)for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\\nof EG.\\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\\nBackward (FoRB). We address the following question with an affirmative answer:\\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< \\u03b3\\u22641\\nas follows:\\nuk= \\u00afuk\\u2212aF(\\u00afuk),\\n\\u00afuk+1= \\u00afuk\\u2212\\u03b3aF(uk),\\u2200k\\u22650,\\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\\nIt is worth noting that OGDA is most frequently expressed in a form where \\u03b3= 1. However, two recent studies have examined\\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of \\u03b3becomesapparent only when dealing with weak Minty solutions. In this context, we find that \\u03b3must be greater than 1 to ensure convergence,\\na phenomenon that is not observed in monotone problems.\\nWhen examining a general smooth min-max problem:\\nmin\\nxmax\\nyf(x, y)\\nthe operator Fmentioned in Assumption 1 naturally emerges as F(u) := [\\u2207xf(x, y),\\u2212\\u2207yf(x, y)]withu= (x, y). However,\\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can\\nconcurrently address more scenarios, such as certain equilibrium problems.\\nThe parameter \\u03c1in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\\nis essential that the step size exceeds a value proportional to \\u03c1. Simultaneously, as is typical, the step size is limited from above\\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1\\n4L, their\\nconvergence claim is valid only if \\u03c1 <1\\n4L. This condition was later improved to \\u03c1 <1\\n2Lfor the choice \\u03b3= 1and to \\u03c1 <1\\nLfor\\neven smaller values of \\u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\\nanalysis, we are able to match the most general condition on the weak Minty parameter \\u03c1 <1\\nLfor appropriate \\u03b3anda.\\n1.1 Contribution\\nOur contributions are summarized as follows:\\n1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,\\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\\nthe Minty variational inequality.\\n2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method ( \\u03b3= 1).\\n3. We demonstrate a complexity bound of O(\\u03f5\\u22122)for a stochastic variant of the OGDA+ method.\\n4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\\n1.2 Related literature\\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\\nor under the Polyak-\\u0141ojasiewicz assumption.\\nWeak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\\ntermed \\\"weak Minty,\\\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\\nper descent step, achieving the same O(1/k)rate as EG.\\nMinty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\\nby a factor proportional to the squared operator norm.\\nNegative comonotonicity. Although previously studied under the term \\\"cohypomonotonicity,\\\" the concept of negative comono-\\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\\nan improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of\\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\\nthis class from problems with weak Minty solutions.\\n2Interaction dominance. The concept of \\u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\\nand it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both\\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\\ncomonotone.\\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\\nalso been studied in the mathematical programming community.\\n2 Preliminaries\\n2.1 Notions of solution\\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\\nconcepts are typically defined with respect to a constraint set C\\u2286Rd. A Stampacchia solution of the VI given by F:Rd\\u2192Rdis a\\npoint u\\u2217such that:\\n\\u27e8F(u\\u2217), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (SVI)\\nIn this work, we only consider the unconstrained case where C=Rd, and the above condition simplifies to F(u\\u2217) = 0 . Closely\\nrelated is the following concept: A Minty solution is a point u\\u2217\\u2208Csuch that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (MVI)\\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\\nholds, for example, if the operator Fis monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\\nwithout any Minty solutions.\\n2.2 Notions of monotonicity\\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\\nAn operator Fis considered monotone if:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u22650.\\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\\nproblems.\\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u00b5\\u2225u\\u2212v\\u22252,\\nand cocoercive operators, which fulfill:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u03b2\\u2225F(u)\\u2212F(v)\\u22252. (2)\\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with \\u03b2equal\\nto the inverse of the gradient\\u2019s Lipschitz constant.\\nDeparting from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\\nforemost is the extensively studied setting of \\u03bd-weak monotonicity:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03bd\\u2225u\\u2212v\\u22252.\\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\\nhas received much less attention and is given by:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03b3\\u2225F(u)\\u2212F(v)\\u22252.\\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\\nBehavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient\\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\\nmonotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650,\\nor star-cocoercive,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265\\u03b3\\u2225F(u)\\u22252.\\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\\noperator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\\nabove star notions are sometimes required to hold for all solutions u\\u2217, in the following we only require it to hold for a single solution.\\n33 OGDA for problems with weak Minty solutions\\nThe generalized version of OGDA, which we denote with a \\\"+\\\" to emphasize the presence of the additional parameter \\u03b3, is given by:\\nAlgorithm 1 OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0and parameter 0< \\u03b3 < 1.\\nfork= 0,1, ...do\\nuk+1=uk\\u2212a((1 + \\u03b3)F(uk)\\u2212F(uk\\u22121))\\nend for\\nTheorem 3.1. LetF:Rd\\u2192RdbeL-Lipschitz continuous satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the iterates\\ngenerated by Algorithm 1 with step size asatisfying a > \\u03c1 and\\naL\\u22641\\u2212\\u03b3\\n1 +\\u03b3. (3)\\nThen, for all k\\u22650,\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22641\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, as long as \\u03c1 <1\\nL, we can find a \\u03b3small enough such that the above bound holds.\\nThe first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems\\nwith\\u03c1 < a . To be able to choose a large step size a, we must decrease \\u03b3, as evident from (3). However, this degrades the algorithm\\u2019s\\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\\nderive an optimal \\u03b3(i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\\n\\u03c1. In practice, the strategy of decreasing \\u03b3until convergence is achieved, but not further, yields reasonable results.\\nFurthermore, we want to point out that the condition \\u03c1 <1\\nLis precisely the best possible bound for EG+.\\n3.1 Improved bounds under monotonicity\\nWhile the above theorem also holds if the operator Fis monotone, we can modify the proof slightly to obtain a better dependence on\\nthe parameters:\\nTheorem 3.2. LetF:Rd\\u2192Rdbe monotone and L-Lipschitz. If aL=2\\u2212\\u03b3\\n2+\\u03b3\\u2212\\u03f5for\\u03f5 >0, then the iterates generated by OGDA+\\nfulfill\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22642\\nka2\\u03b32\\u03f5\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, we can choose \\u03b3= 1anda <1\\n2L.\\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1\\n2L. However, we\\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\\nours for OGDA is shown, but requires the conservative step size bound a\\u22641\\n16L. This was later improved to a\\u22641\\n3L. All of these\\nonly deal with the case \\u03b3= 1. The only other reference that deals with a generalized (i.e., not necessarily \\u03b3= 1) version of OGDA\\nis another work, where the resulting step size condition is a\\u22642\\u2212\\u03b3\\n4L, which is strictly worse than ours for any \\u03b3. To summarize, not\\nonly do we show for the first time that the step size of a generalization of OGDA can go above1\\n2L, but we also provide the least\\nrestrictive bound for any value of \\u03b3.\\n3.2 OGDA+ stochastic\\nIn this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent\\nestimators F(\\u00b7, \\u03bei)at every iteration. We assume here that the estimator Fis unbiased, i.e., E[F(uk, \\u03be)|uk\\u22121] =F(uk), and has\\nbounded variance E[\\u2225F(uk, \\u03be)\\u2212F(uk)\\u22252]\\u2264\\u03c32. We show that we can still guarantee convergence by using batch sizes Bof order\\nO(\\u03f5\\u22121).\\nAlgorithm 2 stochastic OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0, parameter 0< \\u03b3\\u22641and batch size B.\\nfork= 0,1, ...do\\nSample i.i.d. (\\u03bei)B\\ni=1and compute estimator \\u02dcgk=1\\nBPB\\ni=1F(uk, \\u03bek\\ni)\\nuk+1=uk\\u2212a((1 + \\u03b3)\\u02dcgk\\u2212\\u02dcgk\\u22121)\\nend for\\n4Theorem 3.3. LetF:Rd\\u2192RdbeL-Lipschitz satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the sequence of\\niterates generated by stochastic OGDA+, with aand\\u03b3satisfying \\u03c1 < a <1\\u2212\\u03b3\\n1+\\u03b31\\nL. Then, to visit an \\u03f5-stationary point such that\\nmini=0,...,k\\u22121E[\\u2225F(ui)\\u22252]< \\u03f5, we require\\n1\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+a\\u02dcg0\\u2212u\\u2217\\u22252max\\u001a\\n1,4\\u03c32\\naL\\u03f5\\u001b\\ncalls to the stochastic oracle \\u02dcF, with large batch sizes of order O(\\u03f5\\u22121).\\nIn practice, large batch sizes of order O(\\u03f5\\u22121)are typically not desirable; instead, a small or decreasing step size is preferred. In the\\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\\nthe current analysis does not allow for variable \\u03b3.\\n4 EG+ with adaptive step sizes\\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\\nstep size is chosen larger than a multiple of the weak Minty parameter \\u03c1to guarantee convergence at all. For these reasons, we want\\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\\nout.\\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\\nmultiple gradient computations per iteration.\\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\\nare in terms of the gap function.\\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which\\nresults in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our\\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat\\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\\nconvergence result is still open.\\nAlgorithm 3 EG+ with adaptive step size\\nRequire: Starting points u0,\\u00afu0\\u2208Rd, initial step size a0and parameters \\u03c4\\u2208(0,1)and0< \\u03b3\\u22641.\\nfork= 0,1, ...do\\nFind the step size:\\nak= min\\u001a\\nak\\u22121,\\u03c4\\u2225\\u00afuk\\u2212\\u00afuk\\u22121\\u2225\\n\\u2225F(\\u00afuk)\\u2212F(\\u00afuk\\u22121)\\u2225\\u001b\\n(4)\\nCompute next iterate:\\nuk= \\u00afuk\\u2212akF(\\u00afuk)\\n\\u00afuk+1= \\u00afuk\\u2212ak\\u03b3F(uk).\\nend for\\nClearly, akis monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\\nak\\u2265min{a0, \\u03c4/L}>0. The sequence therefore converges to a positive number, which we denote by a\\u221e:= lim kak.\\nTheorem 4.1. LetF:Rd\\u2192RdbeL-Lipschitz that satisfies Assumption 1, where u\\u2217denotes any weak Minty solution, with\\na\\u221e>2\\u03c1, and let (uk)k\\u22650be the iterates generated by Algorithm 3 with \\u03b3=1\\n2and\\u03c4\\u2208(0,1). Then, there exists a k0\\u2208Nsuch that\\nmin\\ni=k0,...,k\\u2225F(uk)\\u22252\\u22641\\nk\\u2212k0L\\n\\u03c4(a\\u221e/2\\u2212\\u03c1)\\u2225\\u00afuk0\\u2212u\\u2217\\u22252.\\n5Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\\nLipschitz constant of the operator Fdoes not need to be known. Moreover, the step size choice presented in (4) might allow us\\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\\nthat they allow us to solve a richer class of problems, as we are able to relax the condition \\u03c1 <1\\n4Lin the case of EG+ to \\u03c1 < a \\u221e/2,\\nwhere a\\u221e= lim kak\\u2265\\u03c4/L.\\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\\nak/ak+1\\u22641\\n\\u03c4is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\\nak/ak+1being too large. This drawback could be mitigated by choosing \\u03c4smaller. However, this will result in poor performance\\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\\n5 Numerical experiments\\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\\n5.1 Von Neumann\\u2019s ratio game\\nWe consider von Neumann\\u2019s ratio game, which is given by:\\nmin\\nx\\u2208\\u2206mmax\\ny\\u2208\\u2206nV(x, y) =\\u27e8x, Ry\\u27e9\\n\\u27e8x, Sy\\u27e9, (5)\\nwhere R\\u2208Rm\\u00d7nandS\\u2208Rm\\u00d7nwith\\u27e8x, Sy\\u27e9>0for all x\\u2208\\u2206m, y\\u2208\\u2206n, with \\u2206 :={z\\u2208Rd:zi>0,Pd\\ni=1zi= 1}denoting\\nthe unit simplex. Expression (5) can be interpreted as the value V(x, y)for a stochastic game with a single state and mixed strategies.\\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\\nan estimated \\u03c1is more than ten times larger than the estimated Lipschitz constant.\\n5.2 Forsaken\\nA particularly difficult min-max toy example with a \\\"Forsaken\\\" solution was proposed and is given by:\\nmin\\nx\\u2208Rmax\\ny\\u2208Rx(y\\u22120.45) + \\u03d5(x)\\u2212\\u03d5(y), (6)\\nwhere \\u03d5(z) =1\\n6z6\\u22122\\n4z4+1\\n4z2\\u22121\\n2z. This problem exhibits a Stampacchia solution at (x\\u2217, y\\u2217)\\u2248(0.08,0.4), but also two limit\\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\\nthe solution repels possible trajectories of iterates, thus \\\"shielding\\\" the solution. Later, it was noticed that, restricted to the box\\n\\u2225(x, y)\\u2225\\u221e<3, the above-mentioned solution is weak Minty with \\u03c1\\u22652\\u00b70.477761 , which is much larger than1\\n2L\\u22480.08. In line\\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by1\\nLconverge. In light of this\\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\\nthe backtracking procedure.\\n5.3 Lower bound example\\nThe following min-max problem was introduced as a lower bound on the dependence between \\u03c1andLfor EG+:\\nmin\\nx\\u2208Rmax\\ny\\u2208R\\u00b5xy+\\u03b6\\n2(x2\\u2212y2). (7)\\nIn particular, it was stated that EG+ (with any \\u03b3) and constant step size a=1\\nLconverges for this problem if and only if (0,0)is a\\nweak Minty solution with \\u03c1 <1\\u2212\\u03b3\\nL, where \\u03c1andLcan be computed explicitly in the above example and are given by:\\nL=p\\n\\u00b52+\\u03b62and \\u03c1=\\u00b52\\u2212\\u03b62\\n2\\u00b5.\\nBy choosing \\u00b5= 3and\\u03b6=\\u22121, we get exactly \\u03c1=1\\nL, therefore predicting divergence of EG+ for any \\u03b3, which is exactly what is\\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \\u03c1 <1\\nL, we\\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\\n66 Conclusion\\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\\nframework. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the\\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\\npresence of merely weak Minty solutions remains an open question.\\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\\nFinally, we note that the previous paradigm in pure minimization of \\\"smaller step size ensures convergence\\\" but \\\"larger step size\\ngets there faster,\\\" where the latter is typically constrained by the reciprocal of the gradient\\u2019s Lipschitz constant, does not appear\\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than1\\nL, which one can typically\\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\\nbacktracking linesearch.article graphicx\\n7\",\n          \"Analyzing Real-Time Group Coordination in\\nAugmented Dance Performances: An LSTM-Based\\nGesture Modeling Approach\\nAbstract\\nThe convergence of augmented reality (AR) and flamenco dance offers a novel\\nresearch avenue to explore group cohesion through gesture forecasting. By employ-\\ning LSTM neural networks, this study predicts dancers\\u2019 gestures and correlates\\naccuracy with synchronization, emotional expression, and creativity\\u2014key cohesion\\nmetrics.\\nA \\\"virtual flamenco guru\\\" provides real-time feedback, enhancing synchronization\\nand fostering gesture resonance, where dancers align movements via a shared vir-\\ntual space. AR amplifies this effect, especially with gesture-sensing garments. This\\ninterdisciplinary research highlights flamenco\\u2019s cultural depth, therapeutic bene-\\nfits, and technological applications in dance therapy, human-computer interaction,\\nand entertainment, pushing the boundaries of creativity and collective behavior\\nanalysis.\\n1 Introduction\\nThe realm of coordinated dance rituals has long been a fascinating area of study, with the intricate\\npatterns and movements of synchronized performances captivating audiences and inspiring new\\navenues of research. Among the various forms of dance, flamenco stands out for its passionate and\\nexpressive nature, characterized by complex hand and foot movements that require a high degree of\\ncoordination and timing. Recent advancements in augmented reality (AR) technology have opened\\nup new possibilities for enhancing and analyzing these performances, allowing for the creation of\\nimmersive and interactive experiences that blur the lines between the physical and virtual worlds.\\nOne of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the\\nlevel of group cohesion among the performers. This can be a difficult task, as it requires measuring\\nthe complex interactions and relationships between individual dancers, as well as their ability to work\\ntogether as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can\\nprovide some insight into the dynamics of the group, but they are often limited by their subjective\\nnature and inability to capture the nuances of nonverbal communication.\\nIn response to these limitations, researchers have begun to explore the use of machine learning\\nalgorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures\\nand movements of dancers. These models have shown great promise in their ability to learn and\\npredict complex patterns of movement, allowing for a more objective and quantitative assessment\\nof group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper\\nunderstanding of the factors that contribute to successful coordinated dance performances, and\\ndevelop new strategies for improving the cohesion and effectiveness of dance groups.\\nHowever, the application of LSTM-based gesture forecasting to coordinated dance rituals is not\\nwithout its challenges. One of the most significant difficulties is the need to develop a system that\\ncan accurately capture and interpret the complex movements and gestures of the dancers. This\\nrequires the creation of sophisticated sensors and data collection systems, capable of tracking thesubtle nuances of human movement and expression. Furthermore, the development of effective\\nLSTM models requires large amounts of high-quality training data, which can be difficult to obtain,\\nespecially in the context of highly specialized and nuanced forms of dance such as flamenco.\\nDespite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to\\nevaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective\\nand quantitative means of assessing performance, these technologies can help to identify areas for\\nimprovement and optimize the training and rehearsal processes. Additionally, the use of AR can\\nenhance the overall experience of the performance, allowing audience members to engage with the\\ndance in new and innovative ways, and creating a more immersive and interactive experience.\\nIn a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture\\nforecasting in conjunction with other, more unconventional forms of movement analysis, such as the\\nstudy of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox,\\nthey have reportedly yielded some surprising insights into the nature of group cohesion and the\\nfactors that contribute to successful coordinated dance performances. For example, one study found\\nthat the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making\\na mistake, allowing for the development of targeted interventions and improvements to the rehearsal\\nprocess.\\nFurthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a\\nnumber of unexpected benefits, such as improving the dancers\\u2019 ability to communicate with each\\nother through subtle cues and gestures. By providing a more nuanced and detailed understanding of\\nthe complex interactions between dancers, these technologies can help to facilitate a more cohesive\\nand effective performance, and even enhance the overall artistic expression of the dance. In some\\ncases, the use of AR has even been shown to alter the dancers\\u2019 perception of their own bodies and\\nmovements, allowing them to develop a greater sense of awareness and control over their actions.\\nIn addition to its practical applications, the study of coordinated dance rituals and group cohesion also\\nraises a number of interesting theoretical questions, such as the nature of collective consciousness\\nand the role of nonverbal communication in shaping group dynamics. By exploring these questions\\nthrough the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under-\\nstanding of the complex factors that contribute to successful group performances, and develop new\\ninsights into the fundamental nature of human interaction and cooperation.\\nThe intersection of AR, LSTM-based gesture forecasting, and coordinated dance rituals also has\\nsignificant implications for our understanding of the relationship between technology and art. As\\nthese technologies continue to evolve and improve, they are likely to have a profound impact on the\\nway we experience and interact with dance and other forms of performance art. By providing new\\ntools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to\\npush the boundaries of what is possible in the world of dance, and create new and innovative forms\\nof artistic expression.\\nOverall, the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM-\\nbased gesture forecasting is a rich and complex field, full of surprising insights and unexpected\\ndiscoveries. As researchers continue to explore the possibilities of these technologies, they are\\nlikely to uncover new and innovative ways of analyzing and understanding the complex dynamics\\nof group performance, and develop new strategies for improving the cohesion and effectiveness of\\ndance groups. Whether through the use of conventional methods or more unconventional approaches,\\nsuch as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture\\nforecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating\\nand thought-provoking results.\\n2 Related Work\\nThe intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant\\nattention in recent years, as researchers seek to harness the potential of immersive technologies to\\nenhance group cohesion and interpersonal coordination. A plethora of studies have investigated\\nthe role of AR in facilitating collaborative dance performances, with a particular emphasis on the\\ndevelopment of novel gesture recognition systems and predictive modeling techniques. Notably, the\\napplication of long short-term memory (LSTM) networks has emerged as a dominant approach in\\n2the field, owing to their capacity to effectively capture the complex temporal dynamics of human\\nmovement.\\nOne intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize\\nthe movements of multiple dancers, thereby fostering a sense of collective rhythm and cohesion. This\\nhas involved the creation of bespoke AR systems that provide real-time visual and auditory cues to\\nparticipants, allowing them to adjust their movements in accordance with the predicted gestures of\\ntheir counterparts. Interestingly, some researchers have explored the incorporation of unconventional\\nfeedback modalities, such as tactile and olfactory stimuli, in an effort to further enhance the sense of\\nimmersion and interpersonal connection among dancers.\\nA related thread of research has examined the potential of AR-based gesture forecasting to facilitate\\nthe creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to\\npredict the likelihood of specific gestures and movements, researchers have been able to generate\\nComplex, algorithmically-driven dance sequences that can be performed in synchronization by\\nmultiple dancers. This has raised fascinating questions regarding the role of human agency and\\ncreativity in the development of AR-mediated choreographies, and has prompted some scholars\\nto investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the\\nco-creation of innovative dance performances.\\nIn a somewhat unexpected turn, some researchers have begun to explore the application of AR and\\nLSTM-based gesture forecasting in the context of non-human dance partners, such as robots and\\nanimals. This has involved the development of bespoke AR systems that can detect and predict\\nthe movements of these non-human entities, allowing human dancers to engage in synchronized\\nperformances with their artificial or animal counterparts. While this line of inquiry may seem\\nunconventional, it has yielded some remarkable insights into the fundamental principles of movement\\nand coordination, and has highlighted the potential for AR and machine learning to facilitate novel\\nforms of interspecies collaboration and creativity.\\nFurthermore, a number of studies have investigated the cultural and historical contexts of flamenco\\ndance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used\\nto preserve and promote traditional flamenco practices. This has involved the creation of digital\\narchives and repositories of flamenco choreographies, which can be used to train LSTM networks\\nand generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco.\\nInterestingly, some researchers have also explored the potential for AR and LSTM-based gesture\\nforecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional\\ntechniques with contemporary influences and innovations.\\nIn addition to these developments, there has been a growing interest in the use of AR and LSTM-based\\ngesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal\\ncoordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and\\nelectroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized\\nperformances, and has yielded some fascinating insights into the neural mechanisms that underlie\\nhuman movement and coordination. Moreover, some researchers have begun to explore the potential\\nfor AR and LSTM-based gesture forecasting to facilitate the development of novel, dance-based\\ntherapies for individuals with neurological or developmental disorders, such as autism and Parkinson\\u2019s\\ndisease.\\nTheoretical frameworks, such as the concept of \\\"extended cognition,\\\" have also been applied to\\nthe study of AR and synchronized flamenco, highlighting the ways in which the use of immersive\\ntechnologies can facilitate the creation of shared, distributed cognitive systems that span the bound-\\naries of individual dancers. This has prompted some scholars to investigate the potential for AR and\\nLSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in\\nwhich the movements and gestures of individual dancers are used to generate emergent, group-level\\npatterns and choreographies.\\nMoreover, a growing body of research has examined the potential for AR and LSTM-based gesture\\nforecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored\\nto the unique architectural and environmental features of a given location. This has involved\\nthe development of bespoke AR systems that can detect and respond to the spatial and temporal\\ncharacteristics of a performance environment, and has yielded some remarkable insights into the\\n3ways in which the use of immersive technologies can be used to enhance the sense of presence and\\nengagement among audience members.\\nIn an effort to further advance the field, some researchers have begun to explore the potential for AR\\nand LSTM-based gesture forecasting to facilitate the development of novel, virtual reality (VR)-based\\nflamenco experiences that can be accessed remotely by users around the world. This has raised\\nimportant questions regarding the potential for VR and AR to democratize access to flamenco and\\nother forms of dance, and has highlighted the need for further research into the social and cultural\\nimplications of these emerging technologies.\\nAdditionally, some scholars have investigated the potential for AR and LSTM-based gesture fore-\\ncasting to facilitate the creation of novel, data-driven flamenco choreographies that are generated\\nusing large datasets of human movement and gesture. This has involved the development of bespoke\\nmachine learning algorithms that can analyze and interpret the complex patterns and structures that\\nunderlie human dance, and has yielded some fascinating insights into the fundamental principles of\\nmovement and coordination.\\nThe use of AR and LSTM-based gesture forecasting has also been explored in the context of dance\\neducation, where it has been used to create novel, interactive learning systems that can provide\\nreal-time feedback and guidance to students. This has raised important questions regarding the\\npotential for AR and machine learning to facilitate the development of more effective and engaging\\ndance pedagogies, and has highlighted the need for further research into the cognitive and neural\\nbasis of dance learning and expertise.\\nSome researchers have also begun to investigate the potential for AR and LSTM-based gesture\\nforecasting to facilitate the creation of novel, immersive flamenco experiences that incorporate\\nmultiple sensory modalities, such as sound, touch, and smell. This has involved the development of\\nbespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some\\nremarkable insights into the ways in which the use of immersive technologies can enhance the sense\\nof presence and engagement among audience members.\\nThe integration of AR and LSTM-based gesture forecasting with other emerging technologies, such\\nas the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of\\nflamenco and dance. This has raised important questions regarding the potential for these technologies\\nto facilitate the creation of novel, hybrid forms of dance and performance that combine human and\\nmachine elements, and has highlighted the need for further research into the social and cultural\\nimplications of these developments.\\nIn another vein, some scholars have begun to investigate the potential for AR and LSTM-based\\ngesture forecasting to facilitate the creation of novel, participatory flamenco performances that involve\\nthe active engagement of audience members. This has involved the development of bespoke AR\\nsystems that can detect and respond to the movements and gestures of audience members, and has\\nyielded some fascinating insights into the ways in which the use of immersive technologies can\\nfacilitate the creation of more interactive and immersive forms of dance and performance.\\nFinally, a growing body of research has examined the potential for AR and LSTM-based gesture\\nforecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural\\nheritage. This has involved the creation of digital archives and repositories of flamenco choreogra-\\nphies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that\\nare grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored\\nthe potential for AR and LSTM-based gesture forecasting to facilitate the development of novel,\\nfusion-based flamenco styles that blend traditional techniques with contemporary influences and\\ninnovations, highlighting the potential for these emerging technologies to facilitate the creation of\\nnew, hybrid forms of cultural expression and identity.\\n3 Methodology\\nTo investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance,\\nwe employed a multidisciplinary approach, combining techniques from computer science, psychology,\\nand dance theory. Our methodology consisted of several stages, including data collection, participant\\nrecruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began\\nby recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing\\n4a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands,\\nwhich we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and\\nmagnetometer sensors to capture the dancers\\u2019 movements with high spatial and temporal resolution.\\nThe AR component of our system was implemented using a custom-built application, which utilized\\na headset-mounted display to provide the dancers with real-time feedback on their movements. This\\nfeedback took the form of a virtual \\\"gesture trail,\\\" which allowed the dancers to visualize their own\\nmovements, as well as those of their peers, in a shared virtual environment. We hypothesized that\\nthis shared feedback mechanism would facilitate enhanced group cohesion and coordination among\\nthe dancers, and we designed a series of experiments to test this hypothesis.\\nOne of the key challenges we faced in developing our system was the need to balance the requirements\\nof real-time feedback and high-fidelity motion capture. To address this challenge, we implemented a\\nnovel approach, which we term \\\"temporally-compressed gesture forecasting.\\\" This approach involves\\nusing a combination of machine learning algorithms and signal processing techniques to compress\\nthe temporal dimension of the motion capture data, while preserving the underlying patterns and\\nstructures of the dancers\\u2019 movements. We found that this approach allowed us to achieve high-quality\\nmotion capture data, while also reducing the computational overhead of our system and enabling\\nreal-time feedback.\\nIn addition to the technical challenges, we also encountered a number of unexpected issues during the\\ndata collection process. For example, we found that the dancers\\u2019 movements were often influenced\\nby a range of external factors, including the music, the lighting, and even the color of the walls in\\nthe dance studio. To address these issues, we developed a novel \\\"context-aware\\\" gesture forecasting\\nsystem, which utilized a combination of environmental sensors and machine learning algorithms\\nto predict the dancers\\u2019 movements based on the surrounding context. We found that this approach\\nallowed us to achieve significantly improved accuracy in our gesture forecasting model, and we\\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\\nmovements of the dancers.\\nAnother unexpected finding that emerged from our research was the discovery that the dancers\\u2019\\nmovements were often influenced by a range of subconscious factors, including their emotional\\nstate, their level of fatigue, and even their personal relationships with their fellow dancers. To\\ninvestigate this phenomenon, we developed a novel \\\"emotional contagion\\\" framework, which utilized\\na combination of psychological surveys, physiological sensors, and machine learning algorithms to\\npredict the emotional state of the dancers based on their movements. We found that this approach\\nallowed us to identify a range of subtle patterns and correlations in the data, which would have been\\ndifficult or impossible to detect using more traditional methods.\\nWe also explored the use of unconventional machine learning architectures, such as a bespoke\\n\\\"Flamenco-inspired\\\" neural network, which was designed to mimic the complex rhythms and patterns\\nof traditional Flamenco music. This approach involved using a combination of convolutional and\\nrecurrent neural network layers to model the temporal and spatial structure of the dancers\\u2019 movements,\\nand we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and\\nrecognition. However, we also encountered a number of challenges and limitations when working\\nwith this approach, including the need for large amounts of labeled training data and the risk of\\noverfitting to the specific patterns and structures of the Flamenco dance style.\\nIn an effort to further enhance the accuracy and robustness of our system, we also investigated the use\\nof a range of alternative and complementary sensing modalities, including electromyography (EMG),\\nelectroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). We found that\\nthese modalities provided a rich source of additional information about the dancers\\u2019 movements\\nand emotional state, and we were able to integrate them into our existing system using a range of\\nsensor fusion and machine learning techniques. However, we also encountered a number of practical\\nchallenges and limitations when working with these modalities, including the need for specialized\\nequipment and expertise, and the risk of signal noise and artifact contamination.\\nDespite these challenges, we were able to demonstrate the effectiveness of our approach in a range of\\nexperimental evaluations, including a large-scale study involving over 100 participants and a series\\nof smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able\\nto achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we\\nwere able to demonstrate a strong positive correlation between the predicted gestures and the actual\\n5movements of the dancers. We also received positive feedback from the participants, who reported\\nthat the system was easy to use and provided a range of benefits, including improved coordination and\\ncohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement.\\nIn conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting\\nto enhance group cohesion and coordination in coordinated dance rituals. While our approach is\\nstill in the early stages of development, we believe that it has the potential to make a significant\\nimpact in a range of applications, from dance and performance to education and therapy. We are\\nexcited to continue exploring the possibilities of this technology, and we look forward to seeing\\nwhere it will take us in the future. We are also considering exploring other genres of dance, such as\\nballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are\\nplanning to investigate the use of our system in other domains, such as sports or rehabilitation, where\\ncoordinated movement and gesture forecasting could be beneficial. Overall, our research highlights\\nthe potential of interdisciplinary approaches to drive innovation and advance our understanding of\\ncomplex phenomena, and we are excited to see where this line of inquiry will lead us in the future.\\n4 Experiments\\nTo conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and\\nsynchronized flamenco, we designed a series of experiments that would not only assess the impact of\\nAR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term\\nMemory (LSTM) networks. The experiments were carried out over the course of several months,\\ninvolving a diverse group of participants with varying levels of experience in flamenco dance.\\nThe experimental setup consisted of a large, specially designed dance studio equipped with AR\\ntechnology that could project a myriad of patterns and cues onto the floor and surrounding walls.\\nThis allowed the dancers to receive real-time feedback and guidance on their movements, which was\\nexpected to enhance their synchronization and overall performance. The studio was also outfitted\\nwith a state-of-the-art motion capture system, capable of tracking the precise movements of each\\ndancer, thus providing valuable data for the LSTM-based gesture forecasting model.\\nBefore commencing the experiments, all participants underwent an intensive training program aimed\\nat familiarizing them with the basics of flamenco and the operation of the AR system. This included\\nunderstanding how to interpret the AR cues, how to adjust their movements based on the feedback\\nreceived, and how to work cohesively as a group. The training program was divided into two\\nphases: the first phase focused on individual skill development, where each participant learned the\\nfundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion,\\nwhere participants practiced dancing together, emphasizing synchronization and coordination.\\nUpon completing the training program, the participants were divided into several groups, each with a\\ndistinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while\\nothers were deliberately mixed to include beginners, intermediate, and advanced dancers. This\\ndiversity was intended to observe how different group compositions affected cohesion and the ability\\nto forecast gestures accurately.\\nThe experimental protocol involved several sessions, each lasting approximately two hours. During\\nthese sessions, the dancers performed a variety of flamenco routines, with and without the AR\\nfeedback. Their movements were captured by the motion tracking system, and the data were fed into\\nthe LSTM model for analysis. The model was tasked with predicting the next gesture or movement\\nbased on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected\\nbehavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from\\nballet or even what appeared to be fragments of a traditional African dance. This phenomenon, which\\nwe termed \\\"Cross-Cultural Gesture Drift,\\\" posed an intriguing question about the potential for LSTM\\nmodels to not only learn from the data they are trained on but also to draw from a broader, unexplored\\nreservoir of cultural knowledge.\\nTo further explore this phenomenon, we introduced an unconventional variable into our experiment:\\nthe influence of ambient music from different cultural backgrounds on the dancers\\u2019 movements\\nand the LSTM\\u2019s predictions. The results were astounding, with the model\\u2019s predictions becoming\\nincreasingly eclectic and incorporating elements from the ambient music genres. For instance, when\\nthe background music shifted to a vibrant salsa rhythm, the model began to predict movements that\\n6were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco\\nrepertoire. Conversely, when the ambient music was a soothing melody from a Japanese traditional\\ninstrument, the predictions became more subdued and introspective, reflecting the serene quality of\\nthe music.\\nTable 1: Cross-Cultural Gesture Drift Observations\\nSession Ambient Music Genre Predicted Gestures Divergence from Flamenco\\n1 Traditional Flamenco High accuracy, minimal divergence 5%\\n2 African Folk Introduction of non-flamenco gestures 20%\\n3 Contemporary Ballet Predictions included ballet movements 35%\\n4 Salsa Increased energy and spontaneity 40%\\n5 Japanese Traditional Predictions became more subdued 15%\\nThe incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a\\nnew layer of complexity to our study, suggesting that the relationship between AR, flamenco, and\\ngesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues\\nfor research, including the potential for using AR and LSTM models to create new, hybrid dance\\nforms that blend elements from different cultural traditions. Furthermore, it raises questions about\\nthe role of technology in preserving cultural heritage versus promoting innovation and fusion.\\nIn a bizarre turn of events, one of the sessions was interrupted by an unexpected visit from a group of\\nwild flamenco enthusiasts, who, upon witnessing the experiment, spontaneously joined in, adding\\ntheir own flair and energy to the performance. This unplanned intrusion not only disrupted the\\ncontrolled environment of the experiment but also led to one of the most captivating and cohesive\\nperformances observed throughout the study. The LSTM model, faced with this unexpected input,\\nsurprisingly adapted and began to predict gestures that were not only accurate but also seemed to\\ncapture the essence and passion of the impromptu dancers.\\nThis serendipitous event underscored the importance of spontaneity and community in dance, as well\\nas the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted\\nthe limitations of controlled experiments in fully capturing the dynamic, often unpredictable nature\\nof human creativity and expression. In response, we have begun to explore the development of more\\nflexible, adaptive experimental designs that can accommodate and even encourage unexpected events,\\nviewing them as opportunities for growth and discovery rather than disruptions to be controlled.\\nThe experiments concluded with a grand finale, where all participants gathered for a final, AR-guided\\nflamenco performance. The event was open to the public and attracted a diverse audience, all of whom\\nwere mesmerized by the synchronization, energy, and evident joy of the dancers. The LSTM model,\\nhaving learned from the myriad of experiences and data collected throughout the study, performed\\nflawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the\\nspontaneity and creativity of the performance.\\nIn reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based\\ngesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into\\nuncharted territories, exploring the intersection of technology, culture, and human expression. The\\nfindings, replete with unexpected turns and surprising revelations, underscore the complexity and\\nrichness of this intersection, beckoning further research and innovation in this captivating field.\\n5 Results\\nOur investigation into the intersection of Augmented Reality (AR) and synchronized flamenco\\ndancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded\\na plethora of intriguing results. Initially, we observed that the integration of AR elements into\\nthe flamenco performances enhanced the dancers\\u2019 ability to synchronize their movements, thereby\\nfostering a heightened sense of group cohesion. This phenomenon was particularly evident when\\nthe AR components were designed to provide real-time feedback on gesture accuracy and timing,\\nallowing the dancers to adjust their movements in tandem.\\nThe LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance\\nsequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual\\n7dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided\\nthe dancers\\u2019 movements, the overall cohesion of the group improved significantly. However, an\\nunexpected outcome emerged when the model was fed a dataset that included gestures from other,\\nunrelated dance forms, such as ballet and hip-hop. In these instances, the LSTM model began to\\ngenerate forecasts that, while inaccurate in the context of flamenco, inadvertently created a unique\\nfusion of dance styles. This unforeseen development led to the creation of novel, AR-infused dance\\nroutines that, despite their lack of traditional flamenco authenticity, exhibited a captivating blend of\\nmovements.\\nFurther analysis revealed that the predictive accuracy of the LSTM model was influenced by the\\ndancers\\u2019 emotional states, as captured through wearable, physiological sensors. Specifically, the\\nmodel\\u2019s performance improved when the dancers were in a state of heightened arousal or excitement,\\nsuggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-\\ning. Conversely, periods of low emotional engagement resulted in diminished forecasting accuracy,\\nunderscoring the importance of emotional connection in the success of AR-augmented, synchronized\\nflamenco.\\nIn a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset\\nthat included gestures performed by dancers who were blindfolded, developed an uncanny ability to\\npredict movements that were not strictly flamenco in nature. These predictions, which seemed to\\ndefy logical explanation, often involved complex, almost acrobatic movements that, when executed,\\nappeared to transcend the traditional boundaries of flamenco dance. While these findings may seem\\nillogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships\\nbetween gesture, emotion, and AR-augmented performance.\\nThe results of our experiments are summarized in the following table: As evidenced by the table, the\\nTable 2: LSTM Model Performance Under Various Conditions\\nCondition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy\\nTraditional Flamenco 0.85 High Arousal Flamenco High\\nFusion Dance 0.70 Medium Engagement Hybrid Medium\\nBlindfolded Gestures 0.90 Low Arousal Non-Traditional Low\\nBallet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High\\nLSTM model\\u2019s performance varies significantly depending on the specific conditions under which it\\nis applied. Notably, the model\\u2019s predictive accuracy is highest when dealing with traditional flamenco\\ngestures, but its ability to generate novel, hybrid movements is most pronounced when confronted\\nwith blindfolded gestures or ballet-influenced flamenco.\\nThe implications of these findings are far-reaching, suggesting that the integration of AR and LSTM-\\nbased gesture forecasting can not only enhance group cohesion in synchronized flamenco but also\\nfacilitate the creation of innovative, boundary-pushing dance forms. Furthermore, the influence of\\nemotional state on predictive accuracy highlights the importance of considering the emotional and\\npsychological aspects of dance performance in the development of AR-augmented systems. As our\\nresearch continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate\\nuncovering even more unexpected and thought-provoking results that challenge our understanding of\\nthe complex interplay between technology, movement, and human emotion.\\nIn an effort to further elucidate the relationships between these factors, we plan to conduct additional\\nexperiments that delve into the cognitive and neurological underpinnings of AR-augmented dance\\nperformance. By investigating the neural correlates of gesture forecasting and emotional engagement,\\nwe hope to gain a deeper understanding of the underlying mechanisms that drive the observed\\nphenomena. This, in turn, will enable the development of more sophisticated AR systems that can\\nadapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall\\nefficacy and aesthetic appeal of synchronized flamenco performances.\\nUltimately, our research endeavors to push the boundaries of what is possible at the confluence of AR,\\nflamenco, and gesture forecasting, embracing the unexpected and the bizarre as integral components\\nof the creative process. By doing so, we aim to contribute to the evolution of dance as an art form, one\\nthat seamlessly integrates technology, movement, and human emotion to create novel, captivating,\\nand unforgettable experiences. The potential applications of this research extend far beyond the realm\\n8of dance, with implications for fields such as human-computer interaction, cognitive psychology, and\\neven therapy, where AR-augmented systems could be leveraged to enhance motor skills, emotional\\nregulation, and social cohesion.\\nAs we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized\\nflamenco, we are reminded that the most profound discoveries often arise from the most unlikely\\nof places. It is our hope that this research will inspire others to embrace the unconventional, the\\nunexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most\\ngroundbreaking insights and innovative solutions. By embracing the complexities and uncertainties\\nof this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance,\\nand transform the human experience through the judicious application of technology and the timeless\\npower of dance.\\n6 Conclusion\\nIn culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized\\nFlamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in\\ncoordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The\\nintricate dynamics at play within the flamenco dance form, characterized by its impassioned gestures\\nand synchronized movements, have been adeptly harnessed and analyzed through the prism of cutting-\\nedge artificial intelligence techniques. By doing so, we have not only delved into the uncharted\\nterritories of human-computer interaction but also teasingly treaded the boundaries of art and science,\\noften blurring the lines between the two.\\nOne of the most fascinating aspects of our research has been the observation that the implementation\\nof Augmented Reality in flamenco dance has led to an unexpected, yet intriguing, phenomenon where\\ndancers began to exhibit a heightened sense of empathy towards each other. This empathy, in turn,\\nhas been found to positively correlate with the level of group cohesion, suggesting that the immersive\\nexperience provided by Augmented Reality fosters a deeper sense of connection among participants.\\nFurthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to\\npredict the intricate hand movements of the dancers, which has been shown to be a critical factor in\\nevaluating the overall synchrony of the dance performance.\\nIn a bizarre twist, our research has also led us to investigate the role of chaos theory in understanding\\nthe complex dynamics of flamenco dance. By applying the principles of chaos theory, we have\\ndiscovered that the seemingly random and unpredictable movements of the dancers can, in fact, be\\nmodeled using nonlinear differential equations. This has profound implications for our understanding\\nof coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from\\nthe interactions among individual dancers can be understood and predicted using mathematical\\nframeworks. Moreover, the application of chaos theory has also led us to explore the concept of\\n\\\"flamenco attractors,\\\" which are hypothetical states of maximum synchrony and cohesion that the\\ndancers can strive towards.\\nMoreover, our study has also explored the tangential relationship between flamenco dance and the\\nprinciples of quantum mechanics. In a series of unconventional experiments, we have found that the\\nprinciples of superposition and entanglement can be used to describe the complex interactions between\\ndancers and their environment. This has led us to propose the concept of \\\"quantum flamenco,\\\" where\\nthe dancers and their surroundings are viewed as an interconnected, holistic system that can be\\ndescribed using the mathematical frameworks of quantum mechanics. While this approach may seem\\nunorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated\\nbehavior, suggesting that the boundaries between art and science are far more fluid than previously\\nthought.\\nThe implications of our research are far-reaching and multifaceted, with potential applications\\nin fields such as psychology, sociology, and computer science. By exploring the intersection of\\nAugmented Reality, flamenco dance, and artificial intelligence, we have opened up new avenues for\\nunderstanding human behavior, social interaction, and the emergence of complex patterns in group\\ndynamics. Furthermore, our study has also highlighted the importance of interdisciplinary research,\\ndemonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking\\ndiscoveries.\\n9In an intriguing aside, our research has also led us to investigate the potential therapeutic applications\\nof flamenco dance in treating neurological disorders such as Parkinson\\u2019s disease. By analyzing\\nthe brain activity of patients who participated in flamenco dance sessions, we have found that the\\nrhythmic movements and synchronized gestures can have a profound impact on motor control and\\ncognitive function. This has led us to propose the concept of \\\"flamenco therapy,\\\" where the immersive\\nexperience of flamenco dance is used as a form of rehabilitation for patients with neurological\\ndisorders.\\nUltimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based\\ngesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range\\nof opportunities for exploration and discovery. By embracing the intersection of art and science,\\nand by venturing into uncharted territories of human-computer interaction, we have gained a deeper\\nunderstanding of the intricate dynamics that govern human behavior and social interaction. As\\nwe continue to push the boundaries of this field, we are excited to see the new and innovative\\napplications that will emerge, and we are confident that our research will have a lasting impact on our\\nunderstanding of group cohesion and coordinated behavior.\\nThe potential for future research in this area is vast and varied, with opportunities to explore new\\nmodes of human-computer interaction, to develop more sophisticated AI models for gesture forecast-\\ning, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.\\nMoreover, the implications of our research extend far beyond the realm of flamenco dance, with\\npotential applications in fields such as robotics, computer vision, and social psychology. As we look\\nto the future, we are eager to see how our research will be built upon and expanded, and we are\\nconfident that the study of Augmented Reality and Synchronized Flamenco will continue to yield\\nnew and exciting insights into the complex and fascinating world of human behavior.\\nIn addition to the theoretical and practical implications of our research, we have also been struck\\nby the aesthetic and artistic dimensions of flamenco dance, and the ways in which it can be used to\\ncreate new and innovative forms of expression. By combining the traditional rhythms and movements\\nof flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able\\nto create a new and unique form of dance that is at once both deeply rooted in tradition and boldly\\ninnovative. This has led us to propose the concept of \\\"cyborg flamenco,\\\" where the boundaries\\nbetween human and machine are blurred, and the dancer becomes a hybrid entity that is both physical\\nand virtual.\\nThe concept of cyborg flamenco has far-reaching implications for our understanding of the relationship\\nbetween human and machine, and the ways in which technology can be used to enhance and transform\\nhuman performance. By exploring the intersection of flamenco dance and cutting-edge technology,\\nwe have been able to create a new and innovative form of expression that is at once both deeply\\nhuman and profoundly technological. This has led us to propose a new paradigm for human-computer\\ninteraction, one that views the human and the machine as interconnected and interdependent entities\\nthat can be used to create new and innovative forms of art and expression.\\nFurthermore, our research has also led us to explore the cultural and historical dimensions of flamenco\\ndance, and the ways in which it has been shaped by the complex and often fraught history of Spain.\\nBy analyzing the historical and cultural context of flamenco, we have been able to gain a deeper\\nunderstanding of the ways in which this dance form has been used as a means of expression and\\nresistance, and the ways in which it continues to be an important part of Spanish culture and identity.\\nThis has led us to propose the concept of \\\"flamenco as resistance,\\\" where the dance is viewed as a\\nform of cultural and political resistance that has been used to challenge and subvert dominant power\\nstructures.\\nThe concept of flamenco as resistance has far-reaching implications for our understanding of the\\nrelationship between culture and power, and the ways in which art and expression can be used as\\na means of challenging and transforming dominant ideologies. By exploring the intersection of\\nflamenco dance and cultural resistance, we have been able to gain a deeper understanding of the\\nways in which this dance form has been used as a means of expressing and challenging dominant\\npower structures, and the ways in which it continues to be an important part of Spanish culture and\\nidentity. This has led us to propose a new paradigm for understanding the relationship between\\nculture and power, one that views art and expression as a means of challenging and transforming\\ndominant ideologies.\\n10Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized\\nFlamenco is a rich and complex field that offers a wide range of opportunities for exploration and\\ndiscovery. By embracing the intersection of art and science, and by venturing into uncharted territories\\nof human-computer interaction, we have gained a deeper understanding of the intricate dynamics that\\ngovern human behavior and social interaction. As we continue to push the boundaries of this field, we\\nare excited to see the new and innovative applications that will emerge, and we are confident that our\\nresearch will have a lasting impact on our understanding of group cohesion and coordinated behavior.\\n11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"null\",\n          \"EMNLP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/papers.csv'\n",
        "pdf_df.to_csv(csv_file_path,index=False)"
      ],
      "metadata": {
        "id": "v4EdlLNIpzgR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/papers.csv')"
      ],
      "metadata": {
        "id": "WSkqZ_UtqAda"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "aR5GNJ5dqEYv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna('Non Publishable',inplace=True)"
      ],
      "metadata": {
        "id": "V_gfZrHIqIZ5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "GzgDzB_GqThY",
        "outputId": "1a16a09f-8349-4a76-b27d-bc77f0ad2bda"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID                                                PDF  Label  \\\n",
              "0   R012.pdf  Safe Predictors for Input-Output Specification...      1   \n",
              "1   R006.pdf  Detailed Action Identification in Baseball Gam...      1   \n",
              "2   R013.pdf  Generalization in ReLU Networks via Restricted...      1   \n",
              "3   R009.pdf  The Importance of Written Explanations in\\nAgg...      1   \n",
              "4   R001.pdf  Transdimensional Properties of Graphite in Rel...      0   \n",
              "5   R010.pdf  Detecting Medication Usage in Parkinson’s Dise...      1   \n",
              "6   R003.pdf  Deciphering the Enigmatic Properties of Metals...      0   \n",
              "7   R008.pdf  Advanced techniques for through and contextual...      1   \n",
              "8   R007.pdf  Advancements in 3D Food Modeling: A Review of ...      1   \n",
              "9   R011.pdf  Addressing Popularity Bias with Popularity-Con...      1   \n",
              "10  R004.pdf  AI-Driven Personalization in Online Education\\...      0   \n",
              "11  R014.pdf  Addressing Min-Max Challenges in Nonconvex-Non...      1   \n",
              "12  R002.pdf  Synergistic Convergence of Photosynthetic Path...      0   \n",
              "13  R005.pdf  Analyzing Real-Time Group Coordination in\\nAug...      0   \n",
              "14  R015.pdf  Examining the Convergence of Denoising Diffusi...      1   \n",
              "\n",
              "         Conference  \n",
              "0           NeurIPS  \n",
              "1              CVPR  \n",
              "2           NeurIPS  \n",
              "3             EMNLP  \n",
              "4   Non Publishable  \n",
              "5               KDD  \n",
              "6   Non Publishable  \n",
              "7             EMNLP  \n",
              "8              CVPR  \n",
              "9               KDD  \n",
              "10  Non Publishable  \n",
              "11             TMLR  \n",
              "12  Non Publishable  \n",
              "13  Non Publishable  \n",
              "14             TMLR  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf86b627-2a42-48f6-b427-62eaa0521213\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Label</th>\n",
              "      <th>Conference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R012.pdf</td>\n",
              "      <td>Safe Predictors for Input-Output Specification...</td>\n",
              "      <td>1</td>\n",
              "      <td>NeurIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R006.pdf</td>\n",
              "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
              "      <td>1</td>\n",
              "      <td>CVPR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R013.pdf</td>\n",
              "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
              "      <td>1</td>\n",
              "      <td>NeurIPS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R009.pdf</td>\n",
              "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
              "      <td>1</td>\n",
              "      <td>EMNLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R001.pdf</td>\n",
              "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
              "      <td>0</td>\n",
              "      <td>Non Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>R010.pdf</td>\n",
              "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
              "      <td>1</td>\n",
              "      <td>KDD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>R003.pdf</td>\n",
              "      <td>Deciphering the Enigmatic Properties of Metals...</td>\n",
              "      <td>0</td>\n",
              "      <td>Non Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>R008.pdf</td>\n",
              "      <td>Advanced techniques for through and contextual...</td>\n",
              "      <td>1</td>\n",
              "      <td>EMNLP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>R007.pdf</td>\n",
              "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>CVPR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>R011.pdf</td>\n",
              "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
              "      <td>1</td>\n",
              "      <td>KDD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>R004.pdf</td>\n",
              "      <td>AI-Driven Personalization in Online Education\\...</td>\n",
              "      <td>0</td>\n",
              "      <td>Non Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>R014.pdf</td>\n",
              "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
              "      <td>1</td>\n",
              "      <td>TMLR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>R002.pdf</td>\n",
              "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
              "      <td>0</td>\n",
              "      <td>Non Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>R005.pdf</td>\n",
              "      <td>Analyzing Real-Time Group Coordination in\\nAug...</td>\n",
              "      <td>0</td>\n",
              "      <td>Non Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>R015.pdf</td>\n",
              "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
              "      <td>1</td>\n",
              "      <td>TMLR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf86b627-2a42-48f6-b427-62eaa0521213')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf86b627-2a42-48f6-b427-62eaa0521213 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf86b627-2a42-48f6-b427-62eaa0521213');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96a0722d-5590-4c73-b099-175931985f5b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96a0722d-5590-4c73-b099-175931985f5b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96a0722d-5590-4c73-b099-175931985f5b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_aafefa71-b7fd-49ee-9887-7b17bb3c6cf6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_aafefa71-b7fd-49ee-9887-7b17bb3c6cf6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"R011.pdf\",\n          \"R014.pdf\",\n          \"R012.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Addressing Popularity Bias with Popularity-Conscious Alignment and\\nContrastive Learning\\nAbstract\\nCollaborative Filtering (CF) often encounters substantial difficulties with popularity bias because of the skewed\\ndistribution of items in real-world datasets. This tendency creates a notable difference in accuracy between items\\nthat are popular and those that are not. This discrepancy impedes the accurate comprehension of user preferences\\nand intensifies the Matthew effect within recommendation systems. To counter popularity bias, current methods\\nconcentrate on highlighting less popular items or on differentiating the correlation between item representations\\nand their popularity. Despite their effectiveness, current approaches continue to grapple with two significant\\nissues: firstly, the extraction of shared supervisory signals from popular items to enhance the representations of\\nless popular items, and secondly, the reduction of representation separation caused by popularity bias. In this\\nstudy, we present an empirical examination of popularity bias and introduce a method called Popularity-Aware\\nAlignment and Contrast (PAAC) to tackle these two problems. Specifically, we utilize the common supervisory\\nsignals found in popular item representations and introduce an innovative popularity-aware supervised alignment\\nmodule to improve the learning of representations for unpopular items. Furthermore, we propose adjusting the\\nweights in the contrastive learning loss to decrease the separation of representations by focusing on popularity.\\nWe confirm the efficacy and logic of PAAC in reducing popularity bias through thorough experiments on three\\nreal-world datasets.\\n1 Introduction\\nContemporary recommender systems are essential in reducing information overload. Personalized recommendations frequently\\nemploy collaborative filtering (CF) to assist users in discovering items that may interest them. CF-based techniques primarily\\nlearn user preferences and item attributes by matching the representations of users with the items they engage with. Despite their\\nachievements, CF-based methods frequently encounter the issue of popularity bias, which leads to considerable disparities in\\naccuracy between items that are popular and those that are not. Popularity bias occurs because there are limited supervisory signals\\nfor items that are not popular, which results in overfitting during the training phase and decreased effectiveness on the test set. This\\nhinders the precise comprehension of user preferences, thereby diminishing the variety of recommendations. Furthermore, popularity\\nbias can worsen the Matthew effect, where items that are already popular gain even more popularity because they are recommended\\nmore frequently.\\nTwo significant challenges are presented when mitigating popularity bias in recommendation systems. The first challenge is the\\ninadequate representation of unpopular items during training, which results in overfitting and limited generalization ability. The\\nsecond challenge, known as representation separation, happens when popular and unpopular items are categorized into distinct\\nsemantic spaces, thereby intensifying the bias and diminishing the precision of recommendations.\\n2 Methodology\\nTo overcome the current difficulties in reducing popularity bias, we introduce the Popularity-Aware Alignment and Contrast (PAAC)\\nmethod. We utilize the common supervisory signals present in popular item representations to direct the learning of unpopular\\nrepresentations, and we present a popularity-aware supervised alignment module. Moreover, we incorporate a re-weighting system\\nin the contrastive learning module to deal with representation separation by considering popularity.\\n2.1 Supervised Alignment Module\\nDuring the training process, the alignment of representations usually emphasizes users and items that have interacted, often causing\\nitems to be closer to interacted users than non-interacted ones in the representation space. However, because unpopular items have\\nlimited interactions, they are usually modeled based on a small group of users. This limited focus can result in overfitting, as the\\nrepresentations of unpopular items might not fully capture their features.The disparity in the quantity of supervisory signals is essential for learning representations of both popular and unpopular items.\\nSpecifically, popular items gain from a wealth of supervisory signals during the alignment process, which helps in effectively\\nlearning their representations. On the other hand, unpopular items, which have a limited number of users providing supervision, are\\nmore susceptible to overfitting. This is because there is insufficient representation learning for unpopular items, emphasizing the\\neffect of supervisory signal distribution on the quality of representation. Intuitively, items interacted with by the same user have\\nsome similar characteristics. In this section, we utilize common supervisory signals in popular item representations and suggest a\\npopularity-aware supervised alignment method to improve the representations of unpopular items.\\nWe initially filter items with similar characteristics based on the user\\u2019s interests. For any user, we define the set of items they interact\\nwith. We count the frequency of each item appearing in the training dataset as its popularity. Subsequently, we group items based on\\ntheir relative popularity. We divide items into two groups: the popular item group and the unpopular item group. The popularity of\\neach item in the popular group is higher than that of any item in the unpopular group. This indicates that popular items receive more\\nsupervisory information than unpopular items, resulting in poorer recommendation performance for unpopular items.\\nTo tackle the issue of insufficient representation learning for unpopular items, we utilize the concept that items interacted with by the\\nsame user share some similar characteristics. Specifically, we use similar supervisory signals in popular item representations to\\nimprove the representations of unpopular items. We align the representations of items to provide more supervisory information to\\nunpopular items and improve their representation, as follows:\\nLSA=X\\nu\\u2208U1\\n|Iu|X\\ni\\u2208Iupop,j\\u2208Iuunpop||f(i)\\u2212f(j)||2, (1)\\nwhere f(\\u00b7)is a recommendation encoder and hi=f(i). By efficiently using the inherent information in the data, we provide more\\nsupervisory signals for unpopular items without introducing additional side information. This module enhances the representation of\\nunpopular items, mitigating the overfitting issue.\\n2.2 Re-weighting Contrast Module\\nRecent research has indicated that popularity bias frequently leads to a noticeable separation in the representation of item embeddings.\\nAlthough methods based on contrastive learning aim to enhance overall uniformity by distancing negative samples, their current\\nsampling methods might unintentionally worsen this separation. When negative samples follow the popularity distribution, which\\nis dominated by popular items, prioritizing unpopular items as positive samples widens the gap between popular and unpopular\\nitems in the representation space. Conversely, when negative samples follow a uniform distribution, focusing on popular items\\nseparates them from most unpopular ones, thus worsening the representation gap. Existing studies use the same weights for positive\\nand negative samples in the contrastive loss function, without considering differences in item popularity. However, in real-world\\nrecommendation datasets, the impact of items varies due to dataset characteristics and interaction distributions. Neglecting this\\naspect could lead to suboptimal results and exacerbate representation separation.\\nWe propose to identify different influences by re-weighting different popularity items. To this end, we introduce re-weighting\\ndifferent positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate\\nthis approach into contrastive learning to better optimize the consistency of representations. Specifically, we aim to reduce the risk\\nof pushing items with varying popularity further apart. For example, when using a popular item as a positive sample, our goal is\\nto avoid pushing unpopular items too far away. Thus, we introduce two hyperparameters to control the weights when items are\\nconsidered positive and negative samples.\\nTo ensure balanced and equitable representations of items within our model, we first propose a dynamic strategy to categorize items\\ninto popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold, which often leads to the\\noverrepresentation of popular items across various batches, we implement a hyperparameter x. This hyperparameter readjusts the\\nclassification of items within the current batch. By adjusting the hyperparameter x, we maintain a balance between different item\\npopularity levels. This enhances the model\\u2019s ability to generalize across diverse item sets by accurately reflecting the popularity\\ndistribution in the current training context. Specifically, we denote the set of items within each batch as IB. And then we divide IB\\ninto a popular group Ipopand an unpopular group Iunpop based on their respective popularity levels, classifying the top x%of items\\nasIpop:\\nIB=Ipop\\u222aIunpop ,\\u2200i\\u2208Ipop\\u2227j\\u2208Iunpop , p(i)> p(j), (2)\\nwhere Ipop\\u2208IBandIunpop\\u2208IBare disjoint, with Ipopconsisting of the top x%of items in the batch. In this work, we dynamically\\ndivided items into popular and unpopular groups within each mini-batch based on their popularity, assigning the top 50% as popular\\nitems and the bottom 50% as unpopular items. This radio not only ensures equal representation of both groups in our contrastive\\nlearning but also allows items to be classified adaptively based on the batch\\u2019s current composition.\\nAfter that, we use InfoNCE to optimize the uniformity of item representations. Unlike traditional CL-based methods, we calculate\\nthe loss for different item groups. Specifically, we introduce the hyperparameter \\u03b1to control the positive sample weights between\\npopular and unpopular items, adapting to varying item distributions in different datasets:\\n2LCL\\nitem=\\u03b1\\u00d7LCL\\npop+ (1\\u2212\\u03b1)\\u00d7LCL\\nunpop , (3)\\nwhere LCL\\npoprepresents the contrastive loss when popular items are considered as positive samples, and LCL\\nunpop represents the\\ncontrastive loss when unpopular items are considered as positive samples. The value of \\u03b1ranges from 0 to 1, where \\u03b1= 0means\\nexclusive emphasis on the loss of unpopular items LCL\\nunpop , and \\u03b1= 1 means exclusive emphasis on the loss of popular items\\nLCL\\npop. By adjusting \\u03b1, we can effectively balance the impact of positive samples from both popular and unpopular items, allowing\\nadaptability to varying item distributions in different datasets.\\nFollowing this, we fine-tune the weighting of negative samples in the contrastive learning framework using the hyperparameter \\u03b2.\\nThis parameter controls how samples from different popularity groups contribute as negative samples. Specifically, we prioritize\\nre-weighting items with popularity opposite to the positive samples, mitigating the risk of excessively pushing negative samples\\naway and reducing representation separation. Simultaneously, this approach ensures the optimization of intra-group consistency. For\\ninstance, when dealing with popular items as positive samples, we separately calculate the impact of popular and unpopular items\\nas negative samples. The hyperparameter \\u03b2is then used to control the degree to which unpopular items are pushed away. This is\\nformalized as follows:\\nL\\u2032\\npop=X\\ni\\u2208Ipoplogexp(h\\u2032\\nihi/\\u03c4)P\\nj\\u2208Ipopexp(h\\u2032\\nihj/\\u03c4) +\\u03b2P\\nj\\u2208Iunpopexp(h\\u2032\\nihj/\\u03c4), (4)\\nsimilarly, the contrastive loss for unpopular items is defined as:\\nL\\u2032\\nunpop =X\\ni\\u2208Iunpoplogexp(h\\u2032\\nihi/\\u03c4)P\\nj\\u2208Iunpopexp(h\\u2032\\nihj/\\u03c4) +\\u03b2P\\nj\\u2208Ipopexp(h\\u2032\\nihj/\\u03c4), (5)\\nwhere the parameter \\u03b2ranges from 0 to 1, controlling the negative sample weighting in the contrastive loss. When \\u03b2= 0, it means\\nthat only intra-group uniformity optimization is performed. Conversely, when \\u03b2= 1, it means equal treatment of both popular and\\nunpopular items in terms of their impact on positive samples. The setting of \\u03b2allows for a flexible adjustment between prioritizing\\nintra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items\\nwithin the same group to optimize uniformity. This setup helps prevent over-optimizing the uniformity of different groups, thereby\\nmitigating representation separation.\\nThe final re-weighting contrastive objective is the weighted sum of the user objective and the item objective:\\nLCL=1\\n2\\u00d7(LCL\\nitem+LCL\\nuser). (6)\\nIn this way, we not only achieved consistency in representation but also reduced the risk of further separating items with similar\\ncharacteristics into different representation spaces, thereby alleviating the issue of representation separation caused by popularity\\nbias.\\n2.3 Model Optimization\\nTo reduce popularity bias in collaborative filtering tasks, we employ a multi-task training strategy to jointly optimize the classic\\nrecommendation loss ( LREC ), supervised alignment loss ( LSA), and re-weighting contrast loss ( LCL).\\nL=LREC+\\u03bb1LSA+\\u03bb2LCL+\\u03bb3||\\u0398||2, (7)\\nwhere \\u0398is the set of model parameters in LREC as we do not introduce additional parameters, \\u03bb1and\\u03bb2are hyperparameters that\\ncontrol the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively,\\nand\\u03bb3is the L2regularization coefficient. After completing the model training process, we use the dot product to predict unknown\\npreferences for recommendations.\\n3 Experiments\\nIn this section, we assess the efficacy of PAAC through comprehensive experiments, aiming to address the following research\\nquestions:\\n\\u2022 How does PAAC compare to existing debiasing methods?\\n\\u2022 How do different designed components play roles in our proposed PAAC?\\n3\\u2022 How does PAAC alleviate the popularity bias?\\n\\u2022 How do different hyper-parameters affect the PAAC recommendation performance?\\n3.1 Experiments Settings\\n3.1.1 Datasets\\nIn our experiments, we use three widely public datasets: Amazon-book, Yelp2018, and Gowalla. We retained users and items with a\\nminimum of 10 interactions.\\n3.1.2 Baselines and Evaluation Metrics\\nWe implement the state-of-the-art LightGCN to instantiate PAAC, aiming to investigate how it alleviates popularity bias. We\\ncompare PAAC with several debiased baselines, including re-weighting-based models, decorrelation-based models, and contrastive\\nlearning-based models.\\nWe utilize three widely used metrics, namely Recall@K, HR@K, and NDCG@K, to evaluate the performance of Top-K recommen-\\ndation. Recall@K and HR@K assess the number of target items retrieved in the recommendation results, emphasizing coverage. In\\ncontrast, NDCG@K evaluates the positions of target items in the ranking list, with a focus on their positions in the list. We use\\nthe full ranking strategy, considering all non-interacted items as candidate items to avoid selection bias during the test stage. We\\nrepeated each experiment five times with different random seeds and reported the average scores.\\n3.2 Overall Performance\\nAs shown in Table 1, we compare our model with several baselines across three datasets. The best performance for each metric\\nis highlighted in bold, while the second best is underlined. Our model consistently outperforms all compared methods across all\\nmetrics in every dataset.\\n\\u2022Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Specif-\\nically, PAAC enhances LightGCN, achieving improvements of 282.65%, 180.79%, and 82.89% in NDCG@20 on the\\nYelp2018, Gowalla, and Amazon-Book datasets, respectively. Compared to the strongest baselines, PAAC delivers better\\nperformance. The most significant improvements are observed on Yelp2018, where our model achieves an 8.70% increase\\nin Recall@20, a 10.81% increase in HR@20, and a 30.2% increase in NDCG@20. This improvement can be attributed\\nto our use of popularity-aware supervised alignment to enhance the representation of less popular items and re-weighted\\ncontrastive learning to address representation separation from a popularity-centric perspective.\\n\\u2022The performance improvements of PAAC are smaller on sparser datasets. For example, on the Gowalla dataset, the\\nimprovements in Recall@20, HR@20, and NDCG@20 are 3.18%, 5.85%, and 5.47%, respectively. This may be because,\\nin sparser datasets like Gowalla, even popular items are not well-represented due to lower data density. Aligning unpopular\\nitems with these poorly represented popular items can introduce noise into the model. Therefore, the benefits of using\\nsupervisory signals for unpopular items may be reduced in very sparse environments, leading to smaller performance\\nimprovements.\\n\\u2022Regarding the baselines for mitigating popularity bias, the improvement of some is relatively limited compared to the\\nbackbone model (LightGCN) and even performs worse in some cases. This may be because some are specifically designed\\nfor traditional data-splitting scenarios, where the test set still follows a long-tail distribution, leading to poor generalization.\\nSome mitigate popularity bias by excluding item popularity information. Others use invariant learning to remove popularity\\ninformation at the representation level, generally performing better than the formers. This shows the importance of\\naddressing popularity bias at the representation level. Some outperform the other baselines, emphasizing the necessary to\\nimprove item representation consistency for mitigating popularity bias.\\n\\u2022Different metrics across various datasets show varying improvements in model performance. This suggests that different\\ndebiasing methods may need distinct optimization strategies for models. Additionally, we observe varying effects of PAAC\\nacross different datasets. This difference could be due to the sparser nature of the Gowalla dataset. Conversely, our model\\ncan directly provide supervisory signals for unpopular items and conduct intra-group optimization, consistently maintaining\\noptimal performance across all metrics on the three datasets.\\n3.3 Ablation Study\\nTo better understand the effectiveness of each component in PAAC, we conduct ablation studies on three datasets. Table 2 presents a\\ncomparison between PAAC and its variants on recommendation performance. Specifically, PAAC-w/o P refers to the variant where\\nthe re-weighting contrastive loss of popular items is removed, focusing instead on optimizing the consistency of representations for\\nunpopular items. Similarly, PAAC-w/o U denotes the removal of the re-weighting contrastive loss for unpopular items. PAAC-w/o\\nA refers to the variant without the popularity-aware supervised alignment loss. It\\u2019s worth noting that PAAC-w/o A differs from\\n4Table 1: Performance comparison on three public datasets with K = 20. The best performance is indicated in bold, while the\\nsecond-best performance is underlined. The superscripts * indicate p \\u22640.05 for the paired t-test of PAAC vs. the best baseline (the\\nrelative improvements are denoted as Imp.).\\n!ModelYelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nMF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270\\nLightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304\\nIPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365\\nMACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487\\n\\u03b1-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264\\nInvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515\\nAdap- \\u03c4 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\nImp. +9.78 % +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%\\nSimGCL in that we split the contrastive loss on the item side, LCL\\nitem, into two distinct losses: LCL\\npopandLCL\\nunpop . This approach\\nallows us to separately address the consistency of popular and unpopular item representations, thereby providing a more detailed\\nanalysis of the impact of each component on the overall performance.\\nFrom Table 2, we observe that PAAC-w/o A outperforms SimGCL in most cases. This validates that re-weighting the importance of\\npopular and unpopular items can effectively improve the model\\u2019s performance in alleviating popularity bias. It also demonstrates the\\neffectiveness of using supervision signals from popular items to enhance the representations of unpopular items, providing more\\nopportunities for future research on mitigating popularity bias. Moreover, compared with PAAC-w/o U, PAAC-w/o P results in much\\nworse performance. This confirms the importance of re-weighting popular items in contrastive learning for mitigating popularity\\nbias. Finally, PAAC consistently outperforms the three variants, demonstrating the effectiveness of combining supervised alignment\\nand re-weighting contrastive learning. Based on the above analysis, we conclude that leveraging supervisory signals from popular\\nitem representations can better optimize representations for unpopular items, and re-weighting contrastive learning allows the model\\nto focus on more informative or critical samples, thereby improving overall performance. All the proposed modules significantly\\ncontribute to alleviating popularity bias.\\nTable 2: Ablation study of PAAC, highlighting the best-performing model on each dataset and metrics in bold. Specifically,\\nPAAC-w/o P removes the re-weighting contrastive loss of popular items, PAAC-w/o U eliminates the re-weighting contrastive loss\\nof unpopular items, and PAAC-w/o A omits the popularity-aware supervised alignment loss.\\n!ModelYelp2018 Gowalla Amazon-book\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\nSimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525\\nPAAC-w/o P 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458\\nPAAC-w/o U 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464\\nPAAC-w/o A 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536\\nPAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*\\n3.4 Debias Ability\\nTo further verify the effectiveness of PAAC in alleviating popularity bias, we conduct a comprehensive analysis focusing on the\\nrecommendation performance across different popularity item groups. Specifically, 20% of the most popular items are labeled\\n\\u2019Popular\\u2019, and the rest are labeled \\u2019Unpopular\\u2019. We compare the performance of PAAC with LightGCN, IPS, MACR, and SimGCL\\nusing the NDCG@20 metric across different popularity groups. We use \\u2206to denote the accuracy gap between the two groups. We\\ndraw the following conclusions:\\n\\u2022Improving the performance of unpopular items is crucial for enhancing overall model performance. Specially, on the\\nYelp2018 dataset, PAAC shows reduced accuracy in recommending popular items, with a notable decrease of 20.14%\\ncompared to SimGCL. However, despite this decrease, the overall recommendation accuracy surpasses that of SimGCL\\nby 11.94%, primarily due to a 6.81% improvement in recommending unpopular items. This improvement highlights the\\nimportance of better recommendations for unpopular items and emphasizes their crucial role in enhancing overall model\\nperformance.\\n5\\u2022Our proposed PAAC significantly enhances the recommendation performance for unpopular items. Specifically, we observe\\nan improvement of 8.94% and 7.30% in NDCG@20 relative to SimGCL on the Gowalla and Yelp2018 datasets, respectively.\\nThis improvement is due to the popularity-aware alignment method, which uses supervisory signals from popular items to\\nimprove the representations of unpopular items.\\n\\u2022PAAC has successfully narrowed the accuracy gap between different item groups. Specifically, PAAC achieved the smallest\\ngap, reducing the NDCG@20 accuracy gap by 34.18% and 87.50% on the Gowalla and Yelp2018 datasets, respectively.\\nThis indicates that our method treats items from different groups fairly, effectively alleviating the impact of popularity\\nbias. This success can be attributed to our re-weighted contrast module, which addresses representation separation from a\\npopularity-centric perspective, resulting in more consistent recommendation results across different groups.\\n3.5 Hyperparameter Sensitivities\\nIn this section, we analyze the impact of hyperparameters in PAAC. Firstly, we investigate the influence of \\u03bb1and\\u03bb2, which\\nrespectively control the impact of the popularity-aware supervised alignment and re-weighting contrast loss. Additionally, in the\\nre-weighting contrastive loss, we introduce two hyperparameters, \\u03b1and\\u03b2, to control the re-weighting of different popularity items\\nas positive and negative samples. Finally, we explore the impact of the grouping ratio xon the model\\u2019s performance.\\n3.5.1 Effect of \\u03bb1and\\u03bb2\\nAs formulated in Eq. (11), \\u03bb1controls the extent of providing additional supervisory signals for unpopular items, while \\u03bb2controls\\nthe extent of optimizing representation consistency. Horizontally, with the increase in \\u03bb2, the performance initially increases and\\nthen decreases. This indicates that appropriate re-weighting contrastive loss effectively enhances the consistency of representation\\ndistributions, mitigating popularity bias. However, overly strong contrastive loss may lead the model to neglect recommendation\\naccuracy. Vertically, as \\u03bb1increases, the performance also initially increases and then decreases. This suggests that suitable\\nalignment can provide beneficial supervisory signals for unpopular items, while too strong an alignment may introduce more noise\\nfrom popular items to unpopular ones, thereby impacting recommendation performance.\\n3.5.2 Effect of re-weighting coefficient \\u03b1and\\u03b2\\nTo mitigate representation separation due to imbalanced positive and negative sampling, we introduce two hyperparameters into the\\ncontrastive loss. Specifically, \\u03b1controls the weight difference between positive samples from popular and unpopular items, while \\u03b2\\ncontrols the influence of different popularity items as negative samples.\\nIn our experiments, while keeping other hyperparameters constant, we search \\u03b1and\\u03b2within the range {0, 0.2, 0.4, 0.6, 0.8, 1}. As\\n\\u03b1and\\u03b2increase, performance initially improves and then declines. The optimal hyperparameters for the Yelp2018 and Gowalla\\ndatasets are \\u03b1= 0.8,\\u03b2= 0.6and\\u03b1= 0.2,\\u03b2= 0.2, respectively. This may be attributed to the characteristics of the datasets. The\\nYelp2018 dataset, with a higher average interaction frequency per item, benefits more from a higher weight \\u03b1for popular items as\\npositive samples. Conversely, the Gowalla dataset, being relatively sparse, prefers a smaller \\u03b1. This indicates the importance of\\nconsidering dataset characteristics when adjusting the contributions of popular and unpopular items to the model.\\nNotably, \\u03b1and\\u03b2are not highly sensitive within the range [0, 1], performing well across a broad spectrum. Performance exceeds the\\nbaseline regardless of \\u03b2values when other parameters are optimal. Additionally, \\u03b1values from [0.4, 1.0] on the Yelp2018 dataset\\nand [0.2, 0.8] on the Gowalla dataset surpass the baseline, indicating less need for precise tuning. Thus, \\u03b1and\\u03b2achieve optimal\\nperformance without meticulous adjustments, focusing on weight coefficients to maintain model efficacy.\\n3.5.3 Effect of grouping ratio x\\nTo investigate the impact of different grouping ratios on recommendation performance, we developed a flexible classification\\nmethod for items within each mini-batch based on their popularity. Instead of adopting a fixed global threshold, which tends to\\noverrepresent popular items in some mini-batches, our approach dynamically divides items in each mini-batch into popular and\\nunpopular categories. Specifically, the top x%of items are classified as popular and the remaining (100 - x)% as unpopular, with x\\nvarying. This strategy prevents the overrepresentation typical in fixed distribution models, which could skew the learning process\\nand degrade performance. To quantify the effects of these varying ratios, we examined various division ratios for popular items,\\nincluding 20%, 40%, 60%, and 80%, as shown in Table 3. The preliminary results indicate that both extremely low and high ratios\\nnegatively affect model performance, thereby underscoring the superiority of our dynamic data partitioning approach. Moreover,\\nwithin the 40%-60% range, our model\\u2019s performance remained consistently robust, further validating the effectiveness of PAAC.\\n6Table 3: Performance comparison across varying popular item ratios x on metrics.\\n!RatioYelp2018 Gowalla\\nRecall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20\\n20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845\\n40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848\\n50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848\\n60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843\\n80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818\\n4 Related Work\\n4.1 Popularity Bias in Recommendation\\nPopularity bias is a prevalent problem in recommender systems, where unpopular items in the training dataset are seldom recom-\\nmended. Numerous techniques have been suggested to examine and decrease performance variations between popular and unpopular\\nitems. These techniques can be broadly divided into three categories.\\n\\u2022Re-weighting-based methods aim to increase the training weight or scores for unpopular items, redirecting focus away\\nfrom popular items during training or prediction. For instance, IPS adds compensation to unpopular items and adjusts\\nthe prediction of the user-item preference matrix, resulting in higher preference scores and improving rankings for\\nunpopular items. \\u03b1-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the\\nneighborhood aggregation process in GCN-based models.\\n\\u2022Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)\\nand popularity. For instance, MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item\\noutcomes. In contrast, InvCF operates on the principle that item representations remain invariant to changes in popularity\\nsemantics, filtering out unstable or outdated popularity characteristics to learn unbiased representations.\\n\\u2022Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE, preserving\\nmore inherent characteristics of items to mitigate popularity bias. This approach has been demonstrated as a state-of-the-art\\nmethod for alleviating popularity bias. It employs data augmentation techniques such as graph augmentation or feature\\naugmentation to generate different views, maximizing positive pair consistency and minimizing negative pair consistency\\nto promote more uniform representations. Specifically, Adap- \\u03c4adjusts user/item embeddings to specific values, while\\nSimGCL integrates InfoNCE loss to enhance representation uniformity and alleviate popularity bias.\\n4.2 Representation Learning for CF\\nRepresentation learning is crucial in recommendation systems, especially in modern collaborative filtering (CF) techniques. It\\ncreates personalized embeddings that capture user preferences and item characteristics. The quality of these representations critically\\ndetermines a recommender system\\u2019s effectiveness by precisely capturing the interplay between user interests and item features.\\nRecent studies emphasize two fundamental principles in representation learning: alignment and uniformity. The alignment principle\\nensures that embeddings of similar or related items (or users) are closely clustered together, improving the system\\u2019s ability to\\nrecommend items that align with a user\\u2019s interests. This principle is crucial when accurately reflecting user preferences through\\ncorresponding item characteristics. Conversely, the uniformity principle ensures a balanced distribution of all embeddings across the\\nrepresentation space. This approach prevents the over-concentration of embeddings in specific areas, enhancing recommendation\\ndiversity and improving generalization to unseen data.\\nIn this work, we focus on aligning the representations of popular and unpopular items interacted with by the same user and re-\\nweighting uniformity to mitigate representation separation. Our model PAAC uniquely addresses popularity bias by combining group\\nalignment and contrastive learning, a first in the field. Unlike previous works that align positive user-item pairs or contrastive pairs,\\nPAAC directly aligns popular and unpopular items, leveraging the rich information of popular items to enhance the representations\\nof unpopular items and reduce overfitting. Additionally, we introduce targeted re-weighting from a popularity-centric perspective to\\nachieve a more balanced representation.\\n5 Conclusion\\nIn this study, we have examined popularity bias and put forward PAAC as a method to lessen its impact. We postulated that items\\nengaged with by the same user exhibit common traits, and we utilized this insight to coordinate the representations of both popular\\nand unpopular items via a popularity-conscious supervised alignment method. This strategy furnished additional supervisory data for\\nless popular items. It is important to note that our concept of aligning and categorizing items according to user-specific preferences\\nintroduces a fresh perspective on alignment. Moreover, we tackled the problem of representation separation seen in current CL-based\\n7models by incorporating two hyperparameters to regulate the influence of items with varying popularity levels when considered\\nas positive and negative samples. This method refined the uniformity of representations and successfully reduced separation. We\\nvalidated our method, PAAC, on three publicly available datasets, demonstrating its effectiveness and underlying rationale.\\nIn the future, we will explore deeper alignment and contrast adjustments tailored to specific tasks to further mitigate popularity\\nbias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in\\nrecommendation systems.\\nAcknowledgments\\nThis work was supported in part by grants from the National Key Research and Development Program of China, the National Natural\\nScience Foundation of China, the Fundamental Research Funds for the Central Universities, and Quan Cheng Laboratory.\\n8\",\n          \"Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems\\nwith Solutions Exhibiting Weak Minty Properties\\nAbstract\\nThis research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-\\nstrate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has\\nalready demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.\\nWe establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within\\nthis framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared\\noperator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified\\nversion of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem\\u2019s\\nspecific parameters.\\n1 Introduction\\nThe recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,\\nhave generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,\\nadversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed\\nthat generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization\\ncomponent and nonconcave in the maximization component remains limited, with some research even suggesting intractability in\\ncertain cases.\\nA specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)\\nexhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of\\nthe recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified\\n(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing\\nliterature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of\\nweak Minty solutions was quickly investigated.\\nAssumption 1 (Weak Minty solution). For a given operator F:Rd\\u2192Rd, there is a point u\\u2217\\u2208Rdand a parameter \\u03c1 >0such that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265 \\u2212\\u03c1\\n2\\u2225F(u)\\u22252\\u2200u\\u2208Rd. (1)\\nMoreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving\\na complexity of O(\\u03f5\\u22121)for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step\\nfollowed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant\\nof EG.\\nIn a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-\\nBackward (FoRB). We address the following question with an affirmative answer:\\nCan OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?\\nSpecifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< \\u03b3\\u22641\\nas follows:\\nuk= \\u00afuk\\u2212aF(\\u00afuk),\\n\\u00afuk+1= \\u00afuk\\u2212\\u03b3aF(uk),\\u2200k\\u22650,\\ncan achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.\\nIt is worth noting that OGDA is most frequently expressed in a form where \\u03b3= 1. However, two recent studies have examined\\na more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of \\u03b3becomesapparent only when dealing with weak Minty solutions. In this context, we find that \\u03b3must be greater than 1 to ensure convergence,\\na phenomenon that is not observed in monotone problems.\\nWhen examining a general smooth min-max problem:\\nmin\\nxmax\\nyf(x, y)\\nthe operator Fmentioned in Assumption 1 naturally emerges as F(u) := [\\u2207xf(x, y),\\u2212\\u2207yf(x, y)]withu= (x, y). However,\\nby examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can\\nconcurrently address more scenarios, such as certain equilibrium problems.\\nThe parameter \\u03c1in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it\\nis essential that the step size exceeds a value proportional to \\u03c1. Simultaneously, as is typical, the step size is limited from above\\nby the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1\\n4L, their\\nconvergence claim is valid only if \\u03c1 <1\\n4L. This condition was later improved to \\u03c1 <1\\n2Lfor the choice \\u03b3= 1and to \\u03c1 <1\\nLfor\\neven smaller values of \\u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different\\nanalysis, we are able to match the most general condition on the weak Minty parameter \\u03c1 <1\\nLfor appropriate \\u03b3anda.\\n1.1 Contribution\\nOur contributions are summarized as follows:\\n1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,\\nwhich we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to\\nthe Minty variational inequality.\\n2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible\\nstep sizes for OGDA+ and obtain the most favorable result known for the standard method ( \\u03b3= 1).\\n3. We demonstrate a complexity bound of O(\\u03f5\\u22122)for a stochastic variant of the OGDA+ method.\\n4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without\\nrequiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in\\nareas with low curvature, enabling convergence where a fixed step size strategy might fail.\\n1.2 Related literature\\nWe will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap\\nfunction or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,\\nor under the Polyak-\\u0141ojasiewicz assumption.\\nWeak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,\\ntermed \\\"weak Minty,\\\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.\\nConvergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as\\nthe update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the\\nlength of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is\\nalso proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step\\nsize we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps\\nper descent step, achieving the same O(1/k)rate as EG.\\nMinty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.\\nIt was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing\\nthe resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a\\nspecific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While\\nthe assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone\\nproblems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point\\nby a factor proportional to the squared operator norm.\\nNegative comonotonicity. Although previously studied under the term \\\"cohypomonotonicity,\\\" the concept of negative comono-\\ntonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty\\nsolutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and\\nan improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of\\nthe reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty\\nsolutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty\\nsolution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient\\nnorm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate\\nthis class from problems with weak Minty solutions.\\n2Interaction dominance. The concept of \\u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated,\\nand it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both\\ncomponents. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively\\ncomonotone.\\nOptimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the\\nattention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has\\nalso been studied in the mathematical programming community.\\n2 Preliminaries\\n2.1 Notions of solution\\nWe outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These\\nconcepts are typically defined with respect to a constraint set C\\u2286Rd. A Stampacchia solution of the VI given by F:Rd\\u2192Rdis a\\npoint u\\u2217such that:\\n\\u27e8F(u\\u2217), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (SVI)\\nIn this work, we only consider the unconstrained case where C=Rd, and the above condition simplifies to F(u\\u2217) = 0 . Closely\\nrelated is the following concept: A Minty solution is a point u\\u2217\\u2208Csuch that:\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650\\u2200u\\u2208C. (MVI)\\nFor a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but\\nholds, for example, if the operator Fis monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but\\nwithout any Minty solutions.\\n2.2 Notions of monotonicity\\nThis section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.\\nAn operator Fis considered monotone if:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u22650.\\nSuch operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium\\nproblems.\\nTwo frequently studied notions that fall into this category are strongly monotone operators, which satisfy:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u00b5\\u2225u\\u2212v\\u22252,\\nand cocoercive operators, which fulfill:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265\\u03b2\\u2225F(u)\\u2212F(v)\\u22252. (2)\\nStrongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max\\nproblems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with \\u03b2equal\\nto the inverse of the gradient\\u2019s Lipschitz constant.\\nDeparting from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring\\nthe non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and\\nspurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and\\nforemost is the extensively studied setting of \\u03bd-weak monotonicity:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03bd\\u2225u\\u2212v\\u22252.\\nSuch operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it\\nincludes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this\\nproperty. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,\\nhas received much less attention and is given by:\\n\\u27e8F(u)\\u2212F(v), u\\u2212v\\u27e9 \\u2265 \\u2212 \\u03b3\\u2225F(u)\\u2212F(v)\\u22252.\\nClearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.\\nBehavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient\\nto require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of\\nmonotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u22650,\\nor star-cocoercive,\\n\\u27e8F(u), u\\u2212u\\u2217\\u27e9 \\u2265\\u03b3\\u2225F(u)\\u22252.\\nIn this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the\\noperator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the\\nabove star notions are sometimes required to hold for all solutions u\\u2217, in the following we only require it to hold for a single solution.\\n33 OGDA for problems with weak Minty solutions\\nThe generalized version of OGDA, which we denote with a \\\"+\\\" to emphasize the presence of the additional parameter \\u03b3, is given by:\\nAlgorithm 1 OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0and parameter 0< \\u03b3 < 1.\\nfork= 0,1, ...do\\nuk+1=uk\\u2212a((1 + \\u03b3)F(uk)\\u2212F(uk\\u22121))\\nend for\\nTheorem 3.1. LetF:Rd\\u2192RdbeL-Lipschitz continuous satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the iterates\\ngenerated by Algorithm 1 with step size asatisfying a > \\u03c1 and\\naL\\u22641\\u2212\\u03b3\\n1 +\\u03b3. (3)\\nThen, for all k\\u22650,\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22641\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, as long as \\u03c1 <1\\nL, we can find a \\u03b3small enough such that the above bound holds.\\nThe first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems\\nwith\\u03c1 < a . To be able to choose a large step size a, we must decrease \\u03b3, as evident from (3). However, this degrades the algorithm\\u2019s\\nspeed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could\\nderive an optimal \\u03b3(i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on\\n\\u03c1. In practice, the strategy of decreasing \\u03b3until convergence is achieved, but not further, yields reasonable results.\\nFurthermore, we want to point out that the condition \\u03c1 <1\\nLis precisely the best possible bound for EG+.\\n3.1 Improved bounds under monotonicity\\nWhile the above theorem also holds if the operator Fis monotone, we can modify the proof slightly to obtain a better dependence on\\nthe parameters:\\nTheorem 3.2. LetF:Rd\\u2192Rdbe monotone and L-Lipschitz. If aL=2\\u2212\\u03b3\\n2+\\u03b3\\u2212\\u03f5for\\u03f5 >0, then the iterates generated by OGDA+\\nfulfill\\nmin\\ni=0,...,k\\u22121\\u2225F(ui)\\u22252\\u22642\\nka2\\u03b32\\u03f5\\u2225u0+aF(u0)\\u2212u\\u2217\\u22252.\\nIn particular, we can choose \\u03b3= 1anda <1\\n2L.\\nThere are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1\\n2L. However, we\\nwant to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as\\nours for OGDA is shown, but requires the conservative step size bound a\\u22641\\n16L. This was later improved to a\\u22641\\n3L. All of these\\nonly deal with the case \\u03b3= 1. The only other reference that deals with a generalized (i.e., not necessarily \\u03b3= 1) version of OGDA\\nis another work, where the resulting step size condition is a\\u22642\\u2212\\u03b3\\n4L, which is strictly worse than ours for any \\u03b3. To summarize, not\\nonly do we show for the first time that the step size of a generalization of OGDA can go above1\\n2L, but we also provide the least\\nrestrictive bound for any value of \\u03b3.\\n3.2 OGDA+ stochastic\\nIn this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent\\nestimators F(\\u00b7, \\u03bei)at every iteration. We assume here that the estimator Fis unbiased, i.e., E[F(uk, \\u03be)|uk\\u22121] =F(uk), and has\\nbounded variance E[\\u2225F(uk, \\u03be)\\u2212F(uk)\\u22252]\\u2264\\u03c32. We show that we can still guarantee convergence by using batch sizes Bof order\\nO(\\u03f5\\u22121).\\nAlgorithm 2 stochastic OGDA+\\nRequire: Starting point u0=u\\u22121\\u2208Rd, step size a >0, parameter 0< \\u03b3\\u22641and batch size B.\\nfork= 0,1, ...do\\nSample i.i.d. (\\u03bei)B\\ni=1and compute estimator \\u02dcgk=1\\nBPB\\ni=1F(uk, \\u03bek\\ni)\\nuk+1=uk\\u2212a((1 + \\u03b3)\\u02dcgk\\u2212\\u02dcgk\\u22121)\\nend for\\n4Theorem 3.3. LetF:Rd\\u2192RdbeL-Lipschitz satisfying Assumption 1 with1\\nL> \\u03c1, and let (uk)k\\u22650be the sequence of\\niterates generated by stochastic OGDA+, with aand\\u03b3satisfying \\u03c1 < a <1\\u2212\\u03b3\\n1+\\u03b31\\nL. Then, to visit an \\u03f5-stationary point such that\\nmini=0,...,k\\u22121E[\\u2225F(ui)\\u22252]< \\u03f5, we require\\n1\\nka\\u03b3(a\\u2212\\u03c1)\\u2225u0+a\\u02dcg0\\u2212u\\u2217\\u22252max\\u001a\\n1,4\\u03c32\\naL\\u03f5\\u001b\\ncalls to the stochastic oracle \\u02dcF, with large batch sizes of order O(\\u03f5\\u22121).\\nIn practice, large batch sizes of order O(\\u03f5\\u22121)are typically not desirable; instead, a small or decreasing step size is preferred. In the\\nweak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,\\nthe current analysis does not allow for variable \\u03b3.\\n4 EG+ with adaptive step sizes\\nIn this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the\\nLipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to\\nsmall step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in\\nchoosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the\\nstep size is chosen larger than a multiple of the weak Minty parameter \\u03c1to guarantee convergence at all. For these reasons, we want\\nto outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried\\nout.\\nSince the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive\\nstep size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,\\nespecially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially\\ninteresting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in\\nmultiple gradient computations per iteration.\\nWe use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone\\ndecreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox\\nmethod, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.\\nA version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the\\noptimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and\\nare in terms of the gap function.\\nOne of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which\\nresults in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our\\nknowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch\\nis the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat\\nall. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative\\nconvergence result is still open.\\nAlgorithm 3 EG+ with adaptive step size\\nRequire: Starting points u0,\\u00afu0\\u2208Rd, initial step size a0and parameters \\u03c4\\u2208(0,1)and0< \\u03b3\\u22641.\\nfork= 0,1, ...do\\nFind the step size:\\nak= min\\u001a\\nak\\u22121,\\u03c4\\u2225\\u00afuk\\u2212\\u00afuk\\u22121\\u2225\\n\\u2225F(\\u00afuk)\\u2212F(\\u00afuk\\u22121)\\u2225\\u001b\\n(4)\\nCompute next iterate:\\nuk= \\u00afuk\\u2212akF(\\u00afuk)\\n\\u00afuk+1= \\u00afuk\\u2212ak\\u03b3F(uk).\\nend for\\nClearly, akis monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that\\nak\\u2265min{a0, \\u03c4/L}>0. The sequence therefore converges to a positive number, which we denote by a\\u221e:= lim kak.\\nTheorem 4.1. LetF:Rd\\u2192RdbeL-Lipschitz that satisfies Assumption 1, where u\\u2217denotes any weak Minty solution, with\\na\\u221e>2\\u03c1, and let (uk)k\\u22650be the iterates generated by Algorithm 3 with \\u03b3=1\\n2and\\u03c4\\u2208(0,1). Then, there exists a k0\\u2208Nsuch that\\nmin\\ni=k0,...,k\\u2225F(uk)\\u22252\\u22641\\nk\\u2212k0L\\n\\u03c4(a\\u221e/2\\u2212\\u03c1)\\u2225\\u00afuk0\\u2212u\\u2217\\u22252.\\n5Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the\\nLipschitz constant of the operator Fdoes not need to be known. Moreover, the step size choice presented in (4) might allow us\\nto take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later\\niterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage\\nthat they allow us to solve a richer class of problems, as we are able to relax the condition \\u03c1 <1\\n4Lin the case of EG+ to \\u03c1 < a \\u221e/2,\\nwhere a\\u221e= lim kak\\u2265\\u03c4/L.\\nOn the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when\\nak/ak+1\\u22641\\n\\u03c4is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in\\nthe starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient\\nak/ak+1being too large. This drawback could be mitigated by choosing \\u03c4smaller. However, this will result in poor performance\\ndue to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be\\ncircumvented, and authors instead focused on the convergence of the iterates without any rate.\\n5 Numerical experiments\\nIn the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see\\nAlgorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification\\nof EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with\\nan initial guess made by second-order information, whose extra cost we ignore in the experiments.\\n5.1 Von Neumann\\u2019s ratio game\\nWe consider von Neumann\\u2019s ratio game, which is given by:\\nmin\\nx\\u2208\\u2206mmax\\ny\\u2208\\u2206nV(x, y) =\\u27e8x, Ry\\u27e9\\n\\u27e8x, Sy\\u27e9, (5)\\nwhere R\\u2208Rm\\u00d7nandS\\u2208Rm\\u00d7nwith\\u27e8x, Sy\\u27e9>0for all x\\u2208\\u2206m, y\\u2208\\u2206n, with \\u2206 :={z\\u2208Rd:zi>0,Pd\\ni=1zi= 1}denoting\\nthe unit simplex. Expression (5) can be interpreted as the value V(x, y)for a stochastic game with a single state and mixed strategies.\\nWe see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although\\nan estimated \\u03c1is more than ten times larger than the estimated Lipschitz constant.\\n5.2 Forsaken\\nA particularly difficult min-max toy example with a \\\"Forsaken\\\" solution was proposed and is given by:\\nmin\\nx\\u2208Rmax\\ny\\u2208Rx(y\\u22120.45) + \\u03d5(x)\\u2212\\u03d5(y), (6)\\nwhere \\u03d5(z) =1\\n6z6\\u22122\\n4z4+1\\n4z2\\u22121\\n2z. This problem exhibits a Stampacchia solution at (x\\u2217, y\\u2217)\\u2248(0.08,0.4), but also two limit\\ncycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to\\nthe solution repels possible trajectories of iterates, thus \\\"shielding\\\" the solution. Later, it was noticed that, restricted to the box\\n\\u2225(x, y)\\u2225\\u221e<3, the above-mentioned solution is weak Minty with \\u03c1\\u22652\\u00b70.477761 , which is much larger than1\\n2L\\u22480.08. In line\\nwith these observations, we can see that none of the fixed step size methods with a step size bounded by1\\nLconverge. In light of this\\nobservation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz\\nconstant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling\\nlimit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in\\nthe backtracking procedure.\\n5.3 Lower bound example\\nThe following min-max problem was introduced as a lower bound on the dependence between \\u03c1andLfor EG+:\\nmin\\nx\\u2208Rmax\\ny\\u2208R\\u00b5xy+\\u03b6\\n2(x2\\u2212y2). (7)\\nIn particular, it was stated that EG+ (with any \\u03b3) and constant step size a=1\\nLconverges for this problem if and only if (0,0)is a\\nweak Minty solution with \\u03c1 <1\\u2212\\u03b3\\nL, where \\u03c1andLcan be computed explicitly in the above example and are given by:\\nL=p\\n\\u00b52+\\u03b62and \\u03c1=\\u00b52\\u2212\\u03b62\\n2\\u00b5.\\nBy choosing \\u00b5= 3and\\u03b6=\\u22121, we get exactly \\u03c1=1\\nL, therefore predicting divergence of EG+ for any \\u03b3, which is exactly what is\\nempirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \\u03c1 <1\\nL, we\\nobserve rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.\\n66 Conclusion\\nMany intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave\\nframework. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the\\nlast iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the\\npresence of merely weak Minty solutions remains an open question.\\nIn general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the\\nmajority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem\\n(7), which is not covered by theory, and OGDA+ is the only method capable of converging.\\nFinally, we note that the previous paradigm in pure minimization of \\\"smaller step size ensures convergence\\\" but \\\"larger step size\\ngets there faster,\\\" where the latter is typically constrained by the reciprocal of the gradient\\u2019s Lipschitz constant, does not appear\\nto hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates\\nthat convergence can be lost if the step size is excessively small and sometimes needs to be larger than1\\nL, which one can typically\\nonly hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a\\nbacktracking linesearch.article graphicx\\n7\",\n          \"Safe Predictors for Input-Output Specification\\nEnforcement\\nAbstract\\nThis paper presents an approach for designing neural networks, along with other\\nmachine learning models, which adhere to a collection of input-output specifica-\\ntions. Our method involves the construction of a constrained predictor for each set\\nof compatible constraints, and combining these predictors in a safe manner using a\\nconvex combination of their predictions. We demonstrate the applicability of this\\nmethod with synthetic datasets and on an aircraft collision avoidance problem.\\n1 Introduction\\nThe increasing adoption of machine learning models, such as neural networks, in safety-critical\\napplications, such as autonomous vehicles and aircraft collision avoidance, highlights an urgent\\nneed for the development of guarantees on safety and robustness. These models may be required\\nto satisfy specific input-output specifications to ensure the algorithms comply with physical laws,\\ncan be executed safely, and are consistent with prior domain knowledge. Furthermore, these models\\nshould demonstrate adversarial robustness, meaning their outputs should not change abruptly within\\nsmall input regions \\u2013 a property that neural networks often fail to satisfy.\\nRecent studies have shown the capacity to verify formally input-output specifications and adversarial\\nrobustness properties of neural networks. For instance, the Satisability Modulo Theory (SMT) solver\\nReluplex was employed to verify properties of networks being used in the Next-Generation Aircraft\\nCollision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used to\\nverify adversarial robustness. While Reluplex and other similar techniques can effectively determine\\nif a network satisfies a given specification, they do not offer a way to guarantee that the network will\\nmeet those specifications. Therefore, additional methods are needed to adjust networks if it is found\\nthat they are not meeting the desired properties.\\nThere has been an increase in techniques for designing networks with certified adversarial robustness,\\nbut enforcing more general safety properties in neural networks is still largely unexplored. One ap-\\nproach to achieving provably correct neural networks is through abstraction-refinement optimization.\\nThis approach has been applied to the ACAS-Xu dataset, but the network was not guaranteed to meet\\nthe specifications until after training. Our work seeks to design networks with enforced input-output\\nconstraints even before training has been completed. This will allow for online learning scenarios\\nwhere a system has to guarantee safety throughout its operation.\\nThis paper presents an approach for designing a safe predictor (a neural network or any other\\nmachine learning model) that will always meet a set of constraints on the input-output relationship.\\nThis assumes that the constrained output regions can be formulated to be convex. Our correct-\\nby-construction safe predictor is guaranteed to satisfy the constraints, even before training, and at\\nevery training step. We describe our approach in Section 2, and show its use in an aircraft collision\\navoidance problem in Section 3. Results on synthetic datasets can be found in Appendix B.\\n.2 Method\\nConsidering two normed vector spaces, an input space X and an output space Y , and a collection\\nof c different pairs of input-output constraints, (Ai, Bi), where Ai\\u2286XandBiis a convex subset\\nofYfor each constraint i, the goal is to design a safe predictor, F:X\\u2192Y, that guarantees\\nx\\u2208Ai\\u21d2F(x)\\u2208Bi.\\nLetbbe a bit-string of length c. Define Obas the set of points zsuch that, for all i,bi= 1implies\\nz\\u2208Ai, and bi= 0implies z /\\u2208Ai.Obthus represents the overlap regions for each combination of\\ninput constraints. For example, O101is the set of points in A1andA3, but not in A2, and O0...0is\\nthe set where no input constraints apply. We also define Oas the set of bit strings, b, such that Ob\\nis non-empty, and define k=|O|. The sets {Ob:b\\u2208O}create a partition of Xaccording to the\\ncombination of input constraints that apply.\\nGiven:\\n\\u2022c different input constraint proximity functions, \\u03c3i:X\\u2192[0,1], where \\u03c3iis continuous and\\n\\u2200x\\u2208Ai,\\u03c3i(x) = 0 ,\\n\\u2022kdifferent constrained predictors, Gb:X\\u2192Bb, one for each b\\u2208O, such that the domain\\nof each Gbis non-empty,\\nWe define:\\n\\u2022a set of weighting functions, wb(x) =Q\\ni:bi=1(1\\u2212\\u03c3i(x))Q\\ni:bi=0\\u03c3i(x)P\\nb\\u2208OQ\\ni:bi=1(1\\u2212\\u03c3i(x))Q\\ni:bi=0\\u03c3i(x), where\\nP\\nb\\u2208Owb(x) = 1 , and\\n\\u2022 a safe predictor, F(x) =P\\nb\\u2208Owb(x)Gb(x).\\nTheorem 2.1. For all i, ifx\\u2208Ai, then F(x)\\u2208Bi.\\nA formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input is\\ninAi, then by construction of the proximity and weighting functions, all of the constrained predictors,\\nGb, that do not map to Biwill be given zero weight. Only the constrained predictors that map to\\nBiwill be given non-zero weight, and because of the convexity of Bi, the weighted average of the\\npredictions will remain in Bi.\\nIf all Gbare continuous and if there are no two input sets, AiandAj, for which (Ai\\u2229Aj)\\u2282\\n(\\u2202Ai\\u222a\\u2202Aj), then Fwill be continuous. In the worst case, as the number of constraints grows linearly,\\nthe number of constrained predictors needed to describe our safe predictor grows exponentially. In\\npractice, however, we expect many of the constraint overlap sets, Ob, to be empty. Consequently, any\\npredictors corresponding to an empty set can be ignored. This significantly reduces the number of\\nconstrained predictors needed for many applications.\\nSee Figure 1 for an illustrative example of how to construct F(x)for a notional problem with two\\noverlapping input-output constraints.\\n2.1 Proximity Functions\\nThe proximity functions, \\u03c3i, describe how close an input, x, is to a particular input constraint region,\\nAi. These functions are used to compute the weights of the constrained predictors. A desirable\\nproperty for \\u03c3iis for \\u03c3i(x)\\u21921asd(x, Ai)\\u2192 \\u221e , for some distance function. This ensures that\\nwhen an input is far from a constraint region, that constraint has little influence on the prediction for\\nthat input. A natural choice for such a function is:\\n\\u03c3i(x; \\u03a3i) = 1\\u2212exp\\u0012\\n\\u2212d(x, Ai)\\n\\u03c31\\u0013\\u03c32\\n.\\nHere, \\u03a3iis a set of parameters \\u03c31\\u2208(0,\\u221e)and\\u03c32\\u2208(1,\\u221e), which can be specified based on\\nengineering judgment, or learned using optimization over training data. In our experiments in\\nthis paper, we use proximity functions of this form and learn independent parameters for each\\ninput-constrained region. We plan to explore other choices for proximity functions in future work.\\n22.2 Learning\\nIf we have families of differentiable functions Gb(x;\\u03b8b), continuously parameterized by \\u03b8b, and\\nfamilies of \\u03c3i(x;\\u03c7i), differentiable and continuously parameterized by \\u03c7i, then F(x; \\u0398, X), where\\n\\u0398 ={\\u03b8b:b\\u2208O}andX={\\u03c7i:i= 1, ..., c}, is also continuously parameterized and differentiable.\\nWe can thus apply standard optimization techniques (e.g., gradient descent) to find parameters of F\\nthat minimize a loss function on some dataset, while also preserving the desired safety properties.\\nNote that the safety guarantee holds regardless of the parameters. To create each Gb(x;\\u03b8b)we\\nconsider choosing:\\n\\u2022 a latent space Rm,\\n\\u2022 a map hb:Rm\\u2192Bb,\\n\\u2022 a standard neural network architecture gb:X\\u2192Rm,\\nand then defining Gb(x;\\u03b8b) =hb(gb(x;\\u03b8b)).\\nThe framework proposed here does not require an entirely separate network for each b. In many\\napplications, it may be advantageous for the constrained predictors to share earlier layers, thus\\ncreating a shared representation of the input space. In addition, our definition of the safe predictor is\\ngeneral and is not limited to neural networks.\\nIn Appendix B, we show examples of applying our approach to synthetic datasets in 2-D and 3-D\\nwith simple neural networks. These examples show that our safe predictor can enforce arbitrary\\ninput-output specifications using convex output constraints on neural networks, and that the learned\\nfunction is smooth.\\n3 Application to Aircraft Collision Avoidance\\nAircraft collision avoidance requires robust safety guarantees. The Next-Generation Collision\\nAvoidance System (ACAS X), which issues advisories to prevent near mid-air collisions, has both\\nmanned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed to\\nchoose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov\\ndecision process. The solution took the form of a large look-up table, mapping each possible input\\ncombination to scores for all possible advisories. The advisory with the highest score would then be\\nissued. By using a deep neural network (DNN) to compress the policy tables, it has been necessary to\\nverify that the DNNs meet certain safety specifications.\\nA desirable \\u02d8201csafeability \\u02d8201d property for ACAS X was defined in a previous work. This property\\nspeci01ed that for any given input state within the \\u02d8201csafeable region, \\u02d8201d an advisory would never\\nbe issued that could put the aircraft into a state where a safe advisory would no longer exist. This\\nconcept is similar to control invariance. A simplified model of the ACAS Xa system was created,\\nnamed VerticalCAS. DNNs were then generated to approximate the learned policy, and Reluplex was\\nused to verify whether the DNNs satisfied the safeability property. This work found thousands of\\ncounterexamples where the DNNs did not meet the criteria.\\nOur approach for designing a safe predictor ensures any collision avoidance system will meet the\\nsafeability property by construction. Appendix C describes in detail how we apply our approach to\\na subset of the VerticalCAS datasets using a conservative, convex approximation of the safeability\\nconstraints. These constraints are defined such that if an aircraft state is in the \\\"unsafeable region\\\",\\nAunsafeable ,i, for the ithadvisory, the score for that advisory must not be the highest, i.e., x\\u2208\\nAunsafeable ,i\\u21d2Fi(x)<max jFj(x), where Fj(x)is the output score for the jthadvisory.\\nTable 1 shows the performance of a standard, unconstrained network and our safe predictor. For both\\nnetworks, we present the percentage accuracy (ACC) and violations (percentage of inputs for which\\nthe network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets,\\nbased on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).\\nAs shown in the table, our safe predictor adheres to the required safeability property. Furthermore,\\nthe accuracy of our predictor remains the same as the unconstrained network, demonstrating we are\\nnot losing accuracy to achieve safety guarantees.\\n3Table 1: Results of the best configurations of \\u03b2-TCV AE on DCI, FactorV AE, SAP, MIG, and IRS\\nmetrics.\\nNETWORK ACC (COC) VIOLATIONS (COC) ACC (CL1500) VIOLATIONS (CL1500)\\nSTANDARD 96.87 0.22 93.89 0.20\\nSAFE 96.69 0.00 94.78 0.00\\n4 Discussion and Future Work\\nWe propose an approach for designing a safe predictor that adheres to input-output specifications for\\nuse in safety-critical machine learning systems, demonstrating it on an aircraft collision avoidance\\nproblem. The novelty of our approach is its simplicity and guaranteed enforcement of specifications\\nthrough combinations of convex output constraints during all stages of training. Future work includes\\nadapting and using techniques from optimization and control barrier functions, as well as incorporating\\nnotions of adversarial robustness into our design, such as extending the work to bound the Lipschitz\\nconstant of our networks.\\nAppendix A: Proof of Theorem 2.1\\nProof. Fixiand assume that x\\u2208Ai. It follows that \\u03c3i(x) = 0 , so for all b\\u2208Owhere bi= 0,\\nwb(x) = 0 . Thus,\\nF(x) =X\\nb\\u2208O,bi=1wb(x)Gb(x).\\nIfbi= 1,Gb(x)\\u2208Bi, and thus F(x)is also in Biby the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem. This\\nexample uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network\\nconsists of a single hidden layer with a dimension of 10, ReLU activations, and a fully connected layer.\\nThe safe predictor shares this structure with the unconstrained network but has its own fully connected\\nlayer for the constrained predictors, G0andG1. Training uses a sampled subset of points from\\nthe input space. Figure 3 shows an example of applying our safe predictor to a notional regression\\nproblem with a 2-D input and 1-D output, using two overlapping constraints. The unconstrained\\nnetwork has two hidden layers of dimension 20 and ReLU activations, followed by a fully connected\\nlayer. The constrained predictors, G00,G10,G01, and G11, share the hidden layers but also have an\\nadditional hidden layer of size 20 with ReLU, followed by a fully connected layer. Training uses a\\nsampled subset of points from the input space.\\nAppendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe \\\"safeability\\\" property, originally introduced and used to verify the safety of the VerticalCAS\\nneural networks can be encoded into a set of input-output constraints. The \\\"safeable region\\\" for\\na given advisory represents input locations where that advisory can be selected such that future\\nadvisories exist that will prevent an NMAC. If no future advisories exist, the advisory is \\\"unsafeable\\\"\\nand the corresponding input region is the \\\"unsafeable region\\\". Examples of these regions, and their\\nproximity functions are shown in Figure 5 for the CL1500 advisory.\\nThe constraints we enforce are that x\\u2208Aunsafeable ,i\\u21d2Fi(x)<max jFj(x),\\u2200i, where Aunsafeable ,iis\\nthe unsafeable region for the ithadvisory, and Fj(x)is the output score for the jthadvisory. Because\\nthe output regions of the safeable constraints are not convex, we make a conservative approximation,\\nenforcing Fi(x) = min jFj(x), for all x\\u2208Aunsafeable ,i.\\n4C.2 Proximity Functions\\nWe start by generating the unsafeable region bounds from the open source code. We then compute a\\n\\\"distance function\\\" between input space points (vO - vI, h, \\u03c4), and the unsafeable region for each\\nadvisory. These are not true distances but are 0 if and only if the data point is within the unsafeable\\nset. These are then used to produce proximity functions as given in Equation 1.\\nC.3 Structure of Predictors\\nThe compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden\\nlayers with a dimension of 45, and ReLU activation functions. We used the same architecture for the\\nunconstrained network. For our constrained predictors, we use the same structure but have shared\\nfirst four layers for all predictors. This provides a common learned representation of the input space,\\nwhile allowing each predictor to adapt to its own constraints. After the shared layers, each constrained\\npredictor has an additional two hidden layers and their final outputs are projected onto our convex\\napproximation of the safe region of the output space, using Gb(x) = min jGj(x). In our experiments,\\nwe set \\u03f5= 0.0001 .\\nWith this construction, we needed 30 separate predictors to enforce the VerticalCAS safeability\\nconstraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880,\\nrespectively. Our safe predictor is orders of magnitude smaller than the original look-up tables.\\nC.4 Parameter Optimization\\nWe use PyTorch for defining our networks and performing parameter optimization. We optimize both\\nthe unconstrained and safe predictors using the asymmetric loss function to select advisories while\\nalso accurately predicting scores. The data is split using an 80/20 train/test split with a random seed\\nof 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216, with training for\\n500 epochs.\\nAppendix A: Proof of Theorem 2.1\\nProof. Letx\\u2208Ai. Then, \\u03c3i(x) = 0 , and for all b\\u2208Owhere bi= 0,wb(x) = 0 . Thus,\\nF(x) =X\\nb\\u2208O,bi=1wb(x)Gb(x)\\nIfbi= 1, then Gb(x)\\u2208Bi, and therefore F(x)is inBidue to the convexity of Bi.\\nAppendix B: Example on Synthetic Datasets\\nFigure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D\\ninput and outputs, and one input-output constraint. The unconstrained network has a single hidden\\nlayer of dimension 10 with ReLU activations, followed by a fully connected layer. The safe predictor\\nshares this structure with constrained predictors, G0andG1, but each predictor has its own fully\\nconnected layer. The training uses a sampled subset of points from the input space and the learned\\npredictors are shown for the continuous input space.\\nFigure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D\\ninput and 1-D output and two overlapping constraints. The unconstrained network has two hidden\\nlayers of dimension 20 with ReLU activations, followed by a fully connected layer. The constrained\\npredictors G00,G10,G01andG11share the hidden layers and have an additional hidden layer of size\\n20 with ReLU followed by a fully connected layer. Again, training uses a sampled subset of points\\nfrom the input space and the learned predictors are shown for the continuous input space.\\n5Appendix C: Details of VerticalCAS Experiment\\nC.1 Safeability Constraints\\nThe \\u201csafeability\\u201d property from prior work can be encoded into a set of input-output constraints. The\\n\\u201csafeable region\\u201d for a given advisory is the set of input space locations where that advisory can be\\nchosen, for which future advisories exist that will prevent an NMAC. If no future advisories exist for\\npreventing an NMAC, the advisory is deemed \\u201cunsafeable,\\u201d and the corresponding input region is the\\n\\u201cunsafeable region.\\u201d Figure 5 shows an example of these regions for the CL1500 advisory.\\nThe constraints we enforce in our safe predictor are: x\\u2208Aunsafeable ,i\\u21d2Fi(x)<max jFj(x),\\n\\u2200i. To make the output regions convex, we approximate by enforcing Fi(x) = min jFj(x), for all\\nx\\u2208Aunsafeable ,i.\\nC.2 Proximity Functions\\nWe start by generating the bounds on the unsafeable regions. Then, a distance function is computed\\nbetween points in the input space ( vO\\u2212vI, h,\\u03c4), and the unsafeable region for each advisory. While\\nthese are not true distances, their values are 0 if and only if the data point is inside the unsafeable set.\\nWhen used to produce proximity functions as given in Equation 1, these values help ensure safety.\\nFigure 5 shows examples of the unsafeable region, distance function, and proximity function for the\\nCL1500 advisory.\\nC.3 Structure of Predictors\\nThe compressed versions of the policy tables from prior work are neural networks with six hidden\\nlayers, 45 dimensions in each layer, and ReLU activation functions. We use the same architecture\\nfor our standard, unconstrained network. For constrained predictors, we use a similar architecture.\\nHowever, the first four hidden layers are shared between all of the predictors. This learns a single,\\nshared input space representation, and also allows each predictor to adapt to its constraints. Each\\nconstrained predictor has two additional hidden layers and their outputs are projected onto our convex\\napproximation of the safe output region. We accomplish this by setting the score for any unsafeable\\nadvisory itoGi(x) = min jGj(x)\\u2212\\u03f5. In our experiments, we used \\u03f5= 0.0001 .\\nTo enforce the VerticalCAS safeability constraints, we need 30 separate predictors. This increases\\nthe size of the network from 270 to 2880 nodes for the unconstrained and safe implementations\\nrespectively. However, our safe predictor remains smaller than the original look-up tables by several\\norders of magnitude.\\nC.4 Parameter Optimization\\nWe define our networks and perform parameter optimization using PyTorch. We optimize the\\nparameters of both the unconstrained network and our safe predictor using the asymmetric loss\\nfunction, guiding the network to select optimal advisories while accurately predicting scores from\\nthe look-up tables. Each dataset is split using an 80/20 train/test split, with a random seed of 0. The\\noptimizer is ADAM, with a learning rate of 0.0003, a batch size of 216, and the number of training\\nepochs is 500.\\n6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Conference\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"NeurIPS\",\n          \"CVPR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1,random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8qHv_GNDqaQp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/Papers.csv',index=False)"
      ],
      "metadata": {
        "id": "dy3x56T7qcS8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Papers.csv')\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfv82loiqgAK",
        "outputId": "703595b3-2fb8-4a44-ce9e-41cd4f089102"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          ID                                                PDF  Label  \\\n",
            "0   R011.pdf  Addressing Popularity Bias with Popularity-Con...      1   \n",
            "1   R014.pdf  Addressing Min-Max Challenges in Nonconvex-Non...      1   \n",
            "2   R012.pdf  Safe Predictors for Input-Output Specification...      1   \n",
            "3   R005.pdf  Analyzing Real-Time Group Coordination in\\nAug...      0   \n",
            "4   R010.pdf  Detecting Medication Usage in Parkinson’s Dise...      1   \n",
            "5   R007.pdf  Advancements in 3D Food Modeling: A Review of ...      1   \n",
            "6   R013.pdf  Generalization in ReLU Networks via Restricted...      1   \n",
            "7   R006.pdf  Detailed Action Identification in Baseball Gam...      1   \n",
            "8   R015.pdf  Examining the Convergence of Denoising Diffusi...      1   \n",
            "9   R001.pdf  Transdimensional Properties of Graphite in Rel...      0   \n",
            "10  R008.pdf  Advanced techniques for through and contextual...      1   \n",
            "11  R004.pdf  AI-Driven Personalization in Online Education\\...      0   \n",
            "12  R002.pdf  Synergistic Convergence of Photosynthetic Path...      0   \n",
            "13  R009.pdf  The Importance of Written Explanations in\\nAgg...      1   \n",
            "14  R003.pdf  Deciphering the Enigmatic Properties of Metals...      0   \n",
            "\n",
            "         Conference  \n",
            "0               KDD  \n",
            "1              TMLR  \n",
            "2           NeurIPS  \n",
            "3   Non Publishable  \n",
            "4               KDD  \n",
            "5              CVPR  \n",
            "6           NeurIPS  \n",
            "7              CVPR  \n",
            "8              TMLR  \n",
            "9   Non Publishable  \n",
            "10            EMNLP  \n",
            "11  Non Publishable  \n",
            "12  Non Publishable  \n",
            "13            EMNLP  \n",
            "14  Non Publishable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVJPVnlyqyTV",
        "outputId": "36e8ab4d-3a8f-461e-c712-e9f91a68ea20"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1a9lmdzHsM5Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_texts(texts, max_length=512):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274,
          "referenced_widgets": [
            "802e62ee6a8845b4bd7ca69460f82024",
            "15f85505d6524d3f953006869521e77b",
            "18a47bbdb4de4d95a63bc427e8121a0e",
            "580bc20fa0264457a950917ff1b40d74",
            "218bfb6c6d47475387f7672c962c1d2f",
            "217fb8792dc746c098ff5e21e1058699",
            "5bd25ede714f4831bf6c28395f184f89",
            "01803c6246a04fc8a865182096f76cb6",
            "f289ae97de2e424d899843806aecbf08",
            "ed0386d1f90d49e0bbbca315685c4bc1",
            "e5330c6c378544bfb954ee39683c3fb1",
            "0b36efa2c93e41ebabdf4fbc2e7acc3c",
            "69cfc072cd57474bbd53597aaba3dbb3",
            "6b5e2f5ee05948a2b9417e48cfb1e198",
            "c7b2efaacb9e475fb2c9cdf3dfbfdf09",
            "cf0fb5460ad14bfe9df0b34a0324c093",
            "adb415d1a9154a779005ee88e29156c3",
            "b84d454709b54c178547c178f59fe817",
            "cdc879444be64f57bcec46dbe072279b",
            "92b808ec8db142c3aca3a18d4e04b8c3",
            "5d0ca0e1826848b9a63bf143e048fa18",
            "4325ffb9f89642e19b428ae9bbeda44a",
            "bd5c8282724846d1a638fbf8a039ac5a",
            "7b7a03411cca4386a497926c19497a19",
            "a3a8cfb20d624832a359a05845d18a7d",
            "9e8c23e94a0349fba412977eccc08780",
            "382484fb77b948519572e3be2783b3d2",
            "a5b6b41bddbe42369f38ab74009be0e9",
            "fc8ae06d66f5423cb8a3a4ece8859fce",
            "95dcd1a00fb04b11ab6b11e2f6bc6237",
            "44dcff6b2b1749e7bec2a47a7d27dc58",
            "6a05fe12c39046c385fbd5c321181b97",
            "e6d319bedb8e4c4791649190258a86d4",
            "4027008bdc9a49d18534f2446ba099fe",
            "dd68dca4e0994e5f83194758e535138e",
            "fbb5c344c5bd4712aabce80f2e21b463",
            "5632f46f4c414b688bc02f53fbf6334f",
            "095b5130226b426eb36c398afe756680",
            "3cfaa12885ce487da6f248a5c9cf9fb8",
            "b02dc160bbe24906907fdaf21eca8947",
            "f8e4d6b38fc1414895f021f925064f44",
            "af66b1c24da64127b2073c1b9e12bba8",
            "64a2cebd943341f7bd5976e6329e3728",
            "d468b148ffde4fb3b78b99233f6a3b49"
          ]
        },
        "id": "T8KvssP6sl8N",
        "outputId": "53d2a503-bada-44fe-c720-67f2c30c19fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "802e62ee6a8845b4bd7ca69460f82024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b36efa2c93e41ebabdf4fbc2e7acc3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd5c8282724846d1a638fbf8a039ac5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4027008bdc9a49d18534f2446ba099fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['PDF'].tolist()   # List of text from PDFs\n",
        "labels = df['Label'].tolist()  # List of binary labels\n",
        "\n",
        "labels_tensor = tf.convert_to_tensor(labels)"
      ],
      "metadata": {
        "id": "UgKyrzGZtZ9A"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = tokenize_texts(texts)"
      ],
      "metadata": {
        "id": "YiQE0ea-uATY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAQFgALku8MM",
        "outputId": "8ac50193-6d65-4b2d-8358-473ceb9fe3d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(15, 512), dtype=int32, numpy=\n",
            "array([[  101, 12786,  6217, ...,  5664, 21641,   102],\n",
            "       [  101, 12786,  8117, ...,  5214,  1997,   102],\n",
            "       [  101,  3647, 16014, ...,  2275,  1997,   102],\n",
            "       ...,\n",
            "       [  101, 19962,  2121, ...,  2241,  2943,   102],\n",
            "       [  101,  1996,  5197, ...,  1998,  1037,   102],\n",
            "       [  101, 11703, 11514, ...,  1997, 12392,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(15, 512), dtype=int32, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(15, 512), dtype=int32, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_masks = tokenized_data['input_ids'], tokenized_data['attention_mask']\n",
        "print(f\"{input_ids}\\n\")\n",
        "print(f\"{attention_masks}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-QD_qNSupIy",
        "outputId": "8af54727-a060-4e17-b06a-56de222d3f29"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  101 12786  6217 ...  5664 21641   102]\n",
            " [  101 12786  8117 ...  5214  1997   102]\n",
            " [  101  3647 16014 ...  2275  1997   102]\n",
            " ...\n",
            " [  101 19962  2121 ...  2241  2943   102]\n",
            " [  101  1996  5197 ...  1998  1037   102]\n",
            " [  101 11703 11514 ...  1997 12392   102]]\n",
            "\n",
            "[[1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " ...\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = (input_ids, attention_masks)\n",
        "Y_train = labels_tensor"
      ],
      "metadata": {
        "id": "f8j_8p5vwToZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the inputs\n",
        "input_ids_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "# Wrap the BERT model call in a Lambda layer and specify output_shape\n",
        "bert_output = layers.Lambda(\n",
        "    lambda x: bert_model(input_ids=x[0], attention_mask=x[1])[1],\n",
        "    output_shape=(768,)  # Specify the output shape here (768 for 'bert-base-uncased')\n",
        ")([input_ids_layer, attention_mask_layer])\n",
        "\n",
        "# Add a classification head\n",
        "output = layers.Dense(1, activation='sigmoid')(bert_output)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "291f75fec7a74dffb5f5b3935fb72abb",
            "f5604b04190c4cab98b063f9bfccfcb4",
            "3b35816460da4b15a46bb900aac29288",
            "20672f33882f4a3f815a7004063f9df8",
            "b1579d6c3eb44e3293a0c2ed3e99d7d5",
            "365d446bba1b432ebf88b523deeed2b0",
            "e3dc171e0077470a89ca8ab075066672",
            "f5a5e7bd39b34d188d485376e1388901",
            "143726e860194d6e9dbeefb44f35c92a",
            "300cfc75c5d343b48c65e3c264d929a7",
            "87b9de149bc5449f9368c2e2fd885cbb"
          ]
        },
        "id": "DkjwFHH0wh_-",
        "outputId": "258592eb-a173-4ba4-ad6b-38577cc7dfad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291f75fec7a74dffb5f5b3935fb72abb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x={'input_ids': X_train[0], 'attention_mask': X_train[1]},\n",
        "    y=Y_train,\n",
        "    epochs=10,\n",
        "    batch_size=4\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35qy1iH2yRFc",
        "outputId": "a3d71513-ccfd-491c-88f8-1f47253b6643"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 9s/step - accuracy: 0.4217 - loss: 1.2043\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9s/step - accuracy: 0.6300 - loss: 0.6168\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 8s/step - accuracy: 0.6717 - loss: 0.6052\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8s/step - accuracy: 0.6417 - loss: 0.6285\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8s/step - accuracy: 0.7000 - loss: 0.5166\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8s/step - accuracy: 0.7167 - loss: 0.4847\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9s/step - accuracy: 0.7017 - loss: 0.4231\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 9s/step - accuracy: 0.6933 - loss: 0.5426\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9s/step - accuracy: 0.7700 - loss: 0.4501\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9s/step - accuracy: 0.7667 - loss: 0.3760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict({'input_ids': X_train[0], 'attention_mask': X_train[1]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB1gmY0rzJnk",
        "outputId": "702de2e1-188d-4345-8a14-3b6869339d89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 46s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YShnb2sOzZvu",
        "outputId": "d6cabab9-546a-4821-e726-ef7cc68a60b4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.80557716]\n",
            " [0.98365307]\n",
            " [0.8805312 ]\n",
            " [0.7121085 ]\n",
            " [0.82129925]\n",
            " [0.7076091 ]\n",
            " [0.6272673 ]\n",
            " [0.8776345 ]\n",
            " [0.9534702 ]\n",
            " [0.46634662]\n",
            " [0.957771  ]\n",
            " [0.47339344]\n",
            " [0.44583923]\n",
            " [0.78626525]\n",
            " [0.4239007 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = tf.round(prediction)\n",
        "print(prediction)\n",
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL67pyjizvhZ",
        "outputId": "0da61e2e-b9ce-48b4-8774-092c7cb9f24a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]], shape=(15, 1), dtype=float32)\n",
            "tf.Tensor([1 1 1 0 1 1 1 1 1 0 1 0 0 1 0], shape=(15,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate({'input_ids': X_train[0], 'attention_mask': X_train[1]}, Y_train)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPQSobvt0FpJ",
        "outputId": "a65e1dc8-3c45-4e5c-d6c3-9c5bac865976"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39s/step - accuracy: 0.9333 - loss: 0.3658\n",
            "Test Accuracy: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "model.save('pdf_classification_model.h5')\n",
        "print(path.isfile('pdf_classification_model.h5'))\n",
        "model_path = os.path.abspath('pdf_classification_model.h5')\n",
        "print(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxNVHPmD381T",
        "outputId": "d7f04ebf-21bd-45ec-b568-482d5979cdeb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "/content/pdf_classification_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Papers-20250107T161145Z-001.zip -d /content/Papers_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg3bYNR85Zy1",
        "outputId": "ef4b375f-6bce-4131-9561-011a577e298b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Papers-20250107T161145Z-001.zip\n",
            "  inflating: /content/Papers_test/Papers/P049.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P101.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P042.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P083.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P004.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P112.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P079.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P010.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P109.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P012.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P057.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P054.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P089.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P014.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P052.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P095.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P131.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P103.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P084.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P046.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P115.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P085.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P058.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P122.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P135.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P093.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P011.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P009.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P114.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P072.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P025.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P059.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P091.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P021.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P055.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P111.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P117.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P120.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P061.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P088.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P019.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P107.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P076.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P040.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P044.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P098.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P035.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P006.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P092.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P020.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P037.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P063.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P029.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P065.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P127.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P113.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P051.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P030.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P013.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P005.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P110.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P106.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P003.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P034.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P108.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P015.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P075.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P045.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P053.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P007.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P096.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P017.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P080.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P119.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P123.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P050.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P124.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P066.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P087.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P067.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P100.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P082.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P002.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P102.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P074.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P071.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P027.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P039.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P105.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P097.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P094.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P099.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P134.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P031.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P060.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P047.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P086.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P048.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P032.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P129.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P073.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P118.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P023.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P069.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P043.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P026.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P001.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P132.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P033.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P130.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P028.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P104.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P056.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P070.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P024.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P022.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P133.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P038.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P125.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P068.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P081.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P018.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P016.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P128.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P077.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P041.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P090.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P036.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P121.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P126.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P116.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P078.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P064.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P062.pdf  \n",
            "  inflating: /content/Papers_test/Papers/P008.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "papers = []\n",
        "id = []\n",
        "\n",
        "# Process Non-Publishable PDFs\n",
        "for root, dirs, files in os.walk('/content/Papers_test/Papers'):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'):\n",
        "            pdf_file_path = os.path.join(root, file)\n",
        "            text = extract_text_from_pdf(pdf_file_path)\n",
        "            id.append(file)\n",
        "            papers.append(text)\n",
        "\n",
        "# Create DataFrame\n",
        "pdf_test = {'ID': id, 'PDF': papers}\n",
        "pdf_test = pd.DataFrame(pdf_test)\n",
        "pdf_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lbcZ5Lus6m6Z",
        "outputId": "ee76f746-8e0c-4052-af0e-b1eeeb274c39"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         ID                                                PDF\n",
              "0  P085.pdf  Privacy Evaluation in Tabular Synthetic Data:\\...\n",
              "1  P006.pdf  High-Throughput Genomic Sequencing in Marine\\n...\n",
              "2  P127.pdf  Examining Machine Learning’s Impact on Persona...\n",
              "3  P044.pdf  A Comprehensive Multimodal Dataset for\\nClimat...\n",
              "4  P008.pdf  Optimized Transfer Learning with Equivariant\\n..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b61ce424-1647-421d-b162-18463772ec48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P085.pdf</td>\n",
              "      <td>Privacy Evaluation in Tabular Synthetic Data:\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P006.pdf</td>\n",
              "      <td>High-Throughput Genomic Sequencing in Marine\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P127.pdf</td>\n",
              "      <td>Examining Machine Learning’s Impact on Persona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P044.pdf</td>\n",
              "      <td>A Comprehensive Multimodal Dataset for\\nClimat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P008.pdf</td>\n",
              "      <td>Optimized Transfer Learning with Equivariant\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b61ce424-1647-421d-b162-18463772ec48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b61ce424-1647-421d-b162-18463772ec48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b61ce424-1647-421d-b162-18463772ec48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cefa8a7d-e9ad-4799-b553-db61db11d27e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cefa8a7d-e9ad-4799-b553-db61db11d27e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cefa8a7d-e9ad-4799-b553-db61db11d27e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pdf_test",
              "summary": "{\n  \"name\": \"pdf_test\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P066.pdf\",\n          \"P029.pdf\",\n          \"P091.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"Turning the Tables: Exploring Subtle Vulnerabilities in\\nMachine Learning Model\\nAbstract\\nThis paper investigates the feasibility and effectiveness of label-only backdoor\\nattacks in machine learning. In these attacks, adversaries corrupt only the training\\nlabels, without modifying the input data (e.g., images), to surreptitiously implant\\nbackdoors into machine learning models. We introduce FLIP (Flipping Labels to\\nInject Poison), a novel label-only backdoor attack mechanism designed to exploit\\nvulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden\\nmapping between a specific trigger (e.g., a subtle alteration in the label distribution)\\nand a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise\\ncorrectly classified by the model.\\n1 Introduction\\nThis paper investigates the feasibility and effectiveness of label-only backdoor attacks in machine\\nlearning [1, 2]. In these attacks, adversaries corrupt only the training labels, without modifying the\\ninput data (e.g., images), to surreptitiously implant backdoors into machine learning models. This\\ncontrasts with traditional backdoor attacks that require manipulating the input data itself, making label-\\nonly attacks a more subtle and potentially harder-to-detect threat. The ease with which an attacker can\\nmanipulate labels, especially in crowd-sourced annotation settings, makes this a significant concern\\nfor the security and trustworthiness of machine learning systems. The potential for widespread impact\\nnecessitates a thorough investigation into the vulnerabilities and defenses against such attacks. This\\nwork aims to contribute to a deeper understanding of this emerging threat landscape.\\nWe introduce FLIP (Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism\\ndesigned to exploit vulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden mapping between\\na specific trigger (e.g., a subtle alteration in the label distribution, or a specific pattern in the\\nlabels themselves) and a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise correctly classified\\nby the model. The subtlety of the attack lies in its reliance on label manipulation alone, making it\\ndifficult to detect using traditional methods focused on input data anomalies. The effectiveness of this\\napproach hinges on the model\\u2019s ability to learn spurious correlations between seemingly innocuous\\nlabel patterns and the desired target output.\\nThe effectiveness of FLIP is evaluated across various scenarios, including those that mimic real-world\\ndata collection challenges. We explore the impact of noisy labels, often encountered in crowd-\\nsourced annotation settings, on the success rate of the attack. We investigate the robustness of FLIP\\nagainst different defense mechanisms, such as data augmentation and adversarial training, commonly\\nemployed to enhance model robustness. Our experiments systematically vary key attack parameters,\\nsuch as the number of poisoned labels and the strength of the trigger, to understand the trade-offs\\ninvolved. This allows us to characterize the attack\\u2019s effectiveness under different conditions and\\nto identify potential weaknesses that could be exploited for defense. The results provide valuable\\ninsights into the vulnerabilities of machine learning models to this type of attack.\\n.We analyze the trade-offs between Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA)\\nunder different attack parameters. This analysis reveals a complex relationship between the number\\nof poisoned labels, the strength of the trigger, and the overall performance of the model. We observe\\nthat while increasing the number of poisoned labels generally improves PTA, it can also lead to a\\nsignificant drop in CTA, indicating a trade-off between the effectiveness of the backdoor and the\\nmodel\\u2019s overall accuracy on clean data. This trade-off is crucial for attackers to consider when\\ndesigning their attacks, as they need to balance the effectiveness of the backdoor with the risk of\\ndetection. A careful analysis of this trade-off is essential for developing effective defense strategies.\\nThe efficiency of FLIP is another key aspect of our study. We demonstrate that FLIP requires\\nsignificantly fewer poisoned labels compared to traditional backdoor attacks that modify the input\\ndata. This makes FLIP a particularly attractive option for attackers who have limited access to the\\ntraining data or who wish to remain undetected. The reduced computational overhead associated\\nwith label manipulation also contributes to the efficiency of FLIP. This makes it a practical threat\\neven in resource-constrained environments, highlighting the need for robust defenses that can operate\\nefficiently as well. The low cost and high effectiveness of FLIP underscore the severity of the threat\\nit poses.\\nOur experiments further explore the applicability of FLIP in the context of knowledge distillation [3].\\nWe show that FLIP can effectively implant backdoors into student models trained using knowledge\\ndistillation from a clean teacher model. This highlights the vulnerability of knowledge distillation to\\nlabel-only backdoor attacks, suggesting that the distillation process itself may inadvertently transfer\\nthe backdoor from the teacher to the student model. This finding underscores the importance of\\nsecuring the training data and processes at every stage of model development, emphasizing the need\\nfor a holistic security approach. The implications for model training pipelines are significant and\\nwarrant further investigation.\\nThe implications of our findings are significant for the security and trustworthiness of machine\\nlearning systems. The ease with which label-only backdoors can be implanted, even under realistic\\nconditions, necessitates the development of new defense mechanisms specifically designed to detect\\nand mitigate these types of attacks. Future research should focus on developing robust methods for\\ndetecting subtle label manipulations and for designing training procedures that are less susceptible\\nto label-only backdoor attacks. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods. The development of such defenses is\\ncrucial for mitigating the risks posed by FLIP and similar attacks.\\nFinally, our work contributes to a broader understanding of the vulnerabilities of machine learning\\nmodels to adversarial attacks. The ability to implant backdoors using only label manipulation\\nhighlights the importance of considering the entire training pipeline, including data collection,\\nannotation, and model training, when assessing the security of machine learning systems. This\\nholistic approach is crucial for developing more secure and trustworthy AI systems. Further research\\nis needed to explore the potential for extending FLIP to other machine learning tasks and model\\narchitectures, and to investigate the broader implications of label-only attacks on the trustworthiness\\nof AI. The findings presented here represent a significant step towards a more comprehensive\\nunderstanding of this emerging threat.\\n2 Related Work\\nThe field of adversarial attacks on machine learning models has seen significant growth in recent\\nyears, with a focus on various attack strategies and defense mechanisms. Early work primarily\\nconcentrated on input-based attacks, where adversaries manipulate the input data (e.g., images) to\\ncause misclassification [4, 5]. These attacks often involve adding carefully crafted perturbations to\\nthe input, making them difficult to detect. However, the reliance on input manipulation limits the\\nattacker\\u2019s reach, particularly in scenarios where direct access to the input data is restricted. Our\\nwork explores a different paradigm, focusing on label-only attacks, which offer a more subtle and\\npotentially harder-to-detect approach.\\nLabel-only attacks represent a relatively nascent area of research, with fewer studies dedicated to\\ntheir analysis and mitigation. Existing literature on data poisoning often focuses on manipulating\\nthe training data itself, including both features and labels [6, 7]. However, these approaches often\\nrequire a significant level of access to the training dataset, which may not always be feasible for an\\n2attacker. In contrast, label-only attacks leverage the inherent vulnerabilities in the label annotation\\nprocess, making them a more practical threat in real-world scenarios where data annotation is often\\noutsourced or crowd-sourced. The subtlety of these attacks makes them particularly challenging to\\ndetect and defend against.\\nSeveral studies have explored the impact of noisy labels on model training and performance [8, 9].\\nWhile these studies primarily focus on the effects of random label noise, they provide a foundation\\nfor understanding how label inconsistencies can affect model learning. Our work builds upon this\\nfoundation by investigating the impact of strategically injected label noise, specifically designed to\\nimplant backdoors. The strategic manipulation of labels, as opposed to random noise, allows for a\\nmore targeted and effective attack, highlighting the unique challenges posed by label-only backdoor\\nattacks.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipulation\\n[10, 11]. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification. However, label-only backdoor attacks differ significantly in their approach, relying\\nsolely on label manipulation to achieve the same effect. This distinction necessitates the development\\nof novel defense mechanisms specifically tailored to address the unique characteristics of label-only\\nattacks. The subtlety of label manipulation makes detection significantly more challenging compared\\nto input-based attacks.\\nKnowledge distillation has emerged as a powerful technique for training efficient student models\\nusing knowledge from larger teacher models [12, 13]. While knowledge distillation offers significant\\nbenefits in terms of model compression and efficiency, our work highlights its vulnerability to label-\\nonly backdoor attacks. The potential for backdoors to propagate from teacher to student models\\nunderscores the importance of securing the entire training pipeline, including the teacher model and\\nthe distillation process itself. This finding emphasizes the need for a holistic security approach that\\nconsiders all stages of model development.\\nOur work contributes to the broader literature on adversarial machine learning by exploring a novel\\nattack vector\\u2014label-only backdoors. This expands the understanding of vulnerabilities in machine\\nlearning systems beyond traditional input-based attacks. The findings presented in this paper highlight\\nthe need for a more comprehensive approach to security, considering not only the input data but\\nalso the entire training process, including data annotation and model training techniques. Future\\nresearch should focus on developing robust defenses against label-only attacks, considering the\\nunique challenges they pose. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods.\\n3 Background\\nLabel-only backdoor attacks represent a significant and emerging threat to the security and trustwor-\\nthiness of machine learning models. Unlike traditional backdoor attacks that involve manipulating\\ninput data, these attacks exploit vulnerabilities in the training process by corrupting only the training\\nlabels. This subtle manipulation can lead to the implantation of backdoors that are difficult to detect\\nusing conventional methods. The ease with which labels can be altered, particularly in crowd-sourced\\nannotation settings, makes this a particularly concerning vulnerability. The potential for widespread\\nimpact necessitates a thorough investigation into the vulnerabilities and defenses against such attacks.\\nThis research aims to contribute to a deeper understanding of this emerging threat landscape and to\\ninform the development of robust countermeasures. The focus is on understanding the mechanisms by\\nwhich these attacks operate, their effectiveness under various conditions, and the trade-offs involved\\nin their implementation.\\nThe existing literature on data poisoning primarily focuses on manipulating both features and labels\\nwithin the training dataset. However, these approaches often require significant access to the training\\ndata, which may not always be feasible for an attacker. Label-only attacks offer a more practical\\nalternative, leveraging the inherent vulnerabilities in the label annotation process. The subtlety of\\nthese attacks makes them particularly challenging to detect and defend against, as they do not involve\\nreadily apparent modifications to the input data itself. This necessitates the development of novel\\ndefense mechanisms specifically tailored to address the unique characteristics of label-only attacks.\\nThe challenge lies in identifying subtle patterns in the label distribution that might indicate malicious\\nmanipulation.\\n3Several studies have explored the impact of noisy labels on model training and performance. These\\nstudies primarily focus on the effects of random label noise, providing a foundation for understanding\\nhow label inconsistencies can affect model learning. However, label-only backdoor attacks differ\\nsignificantly in that the label noise is strategically injected, rather than being random. This strategic\\nmanipulation allows for a more targeted and effective attack, resulting in the implantation of a\\nbackdoor that triggers specific misclassifications. The ability to control the nature and location of\\nthe label noise is crucial to the success of the attack. Understanding the interplay between the level\\nof noise, the strategic placement of poisoned labels, and the resulting model behavior is key to\\ndeveloping effective defenses.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipu-\\nlation. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification when a particular trigger is present in the input. However, label-only backdoor\\nattacks differ significantly in their approach, relying solely on label manipulation to achieve the\\nsame effect. This distinction necessitates the development of novel defense mechanisms specifically\\ntailored to address the unique characteristics of label-only attacks. The subtlety of label manipulation\\nmakes detection significantly more challenging compared to input-based attacks, requiring more\\nsophisticated methods for identifying anomalous patterns in the label distribution.\\nKnowledge distillation is a powerful technique for training efficient student models using knowledge\\nfrom larger teacher models. While knowledge distillation offers significant benefits in terms of model\\ncompression and efficiency, it also introduces potential vulnerabilities to label-only backdoor attacks.\\nIf the teacher model is compromised, the backdoor can propagate to the student model during the\\ndistillation process. This highlights the importance of securing the entire training pipeline, including\\nthe teacher model and the distillation process itself. A holistic security approach is crucial to mitigate\\nthe risks associated with knowledge distillation in the presence of label-only backdoor attacks. The\\npotential for cascading vulnerabilities underscores the need for robust security measures at every\\nstage of model development.\\nThe development of robust defenses against label-only backdoor attacks is a critical area of future\\nresearch. These defenses should focus on detecting subtle label manipulations and designing training\\nprocedures that are less susceptible to these attacks. Techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods are promising avenues for exploration.\\nThe challenge lies in developing methods that can effectively identify malicious label manipulations\\nwithout significantly impacting the performance of the model on clean data. A balance must be struck\\nbetween security and accuracy, ensuring that the defenses do not unduly compromise the model\\u2019s\\nutility. The development of such defenses is crucial for mitigating the risks posed by label-only\\nbackdoor attacks and ensuring the trustworthiness of machine learning systems.\\n4 Methodology\\nThis section details the methodology employed to evaluate the feasibility and effectiveness of label-\\nonly backdoor attacks using the FLIP (Flipping Labels to Inject Poison) mechanism. Our approach\\ninvolves a comprehensive evaluation across various scenarios, including those that mimic real-world\\ndata collection challenges and model training paradigms. The core of our methodology centers\\naround strategically manipulating a subset of training labels to induce a hidden mapping between a\\nspecific trigger (a pattern in the label sequence) and a predetermined target output. This manipulation\\nis designed to force the model to learn a spurious correlation, enabling backdoor control without\\nmodifying the input data itself.\\nThe effectiveness of FLIP is assessed across multiple datasets and model architectures. We systemat-\\nically vary key attack parameters, including the percentage of poisoned labels, the strength of the\\ntrigger (defined by the complexity and consistency of the label pattern), and the type of trigger used.\\nThe choice of datasets and models ensures generalizability and robustness of our findings. We employ\\nstandard evaluation metrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA),\\nto quantify the impact of the attack. CTA measures the model\\u2019s accuracy on clean, unpoisoned data,\\nwhile PTA measures the model\\u2019s accuracy on data associated with the trigger. The trade-off between\\nCTA and PTA is a crucial aspect of our analysis, providing insights into the attack\\u2019s effectiveness\\nversus its detectability.\\n4To simulate real-world scenarios, we introduce label noise into the training data. This noise is inde-\\npendent of the strategically injected poisoned labels, mimicking the imperfections often encountered\\nin crowd-sourced annotation settings. By varying the level of label noise, we assess the robustness of\\nFLIP against noisy labels. We hypothesize that even with a significant level of random label noise,\\nFLIP will remain effective due to the strategic nature of the poisoned labels. This analysis provides\\nvaluable insights into the attack\\u2019s resilience in less-than-ideal data conditions.\\nFurthermore, we investigate the robustness of FLIP against common defense mechanisms. Specifi-\\ncally, we evaluate the attack\\u2019s effectiveness against data augmentation techniques and adversarial\\ntraining. Data augmentation involves artificially expanding the training dataset by applying various\\ntransformations to the existing data. Adversarial training aims to improve model robustness by\\ntraining the model on adversarial examples, which are designed to fool the model. By testing FLIP\\nagainst these defenses, we assess its resilience to commonly employed security measures. This\\nanalysis helps to identify potential weaknesses in existing defenses and inform the development of\\nmore robust countermeasures.\\nThe efficiency of FLIP is evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. We expect\\nFLIP to require significantly fewer poisoned labels, making it a more efficient and stealthy attack.\\nThis efficiency is a key advantage of label-only attacks, as it reduces the attacker\\u2019s effort and risk of\\ndetection. The computational overhead associated with label manipulation is also significantly lower\\nthan that of input data modification, further enhancing the practicality of FLIP.\\nFinally, we explore the applicability of FLIP in the context of knowledge distillation. We train a\\nstudent model using knowledge distillation from a clean teacher model, where the teacher model\\u2019s\\ntraining data has been subjected to a FLIP attack. We investigate whether the backdoor is transferred\\nfrom the teacher to the student model during the distillation process. This analysis highlights the\\npotential for cascading vulnerabilities in model training pipelines and underscores the importance of\\nsecuring the training data and processes at every stage of model development. The results provide\\ninsights into the vulnerability of knowledge distillation to label-only backdoor attacks.\\nThe experimental setup involves a rigorous comparison across various datasets, model architectures,\\nand attack parameters. The results are statistically analyzed to ensure the reliability and significance\\nof our findings. The comprehensive nature of our methodology allows for a thorough evaluation of\\nFLIP\\u2019s effectiveness, efficiency, and robustness, providing valuable insights into the challenges posed\\nby label-only backdoor attacks. This detailed analysis informs the development of more effective\\ndefense mechanisms and contributes to a broader understanding of the security vulnerabilities in\\nmachine learning systems.\\nOur methodology emphasizes a holistic approach, considering various aspects of the attack, including\\nits effectiveness, efficiency, robustness, and applicability in different contexts. This comprehensive\\nevaluation provides a robust assessment of the threat posed by FLIP and informs the development of\\neffective countermeasures. The findings contribute to a deeper understanding of the vulnerabilities of\\nmachine learning systems to label-only backdoor attacks and highlight the need for a more holistic\\napproach to security in the design and deployment of machine learning models.\\n5 Experiments\\nThis section details the experimental setup and results obtained to evaluate the effectiveness of FLIP\\n(Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism. Our experiments\\nwere designed to comprehensively assess FLIP\\u2019s performance across various scenarios, including\\nthose that mimic real-world data collection challenges and model training paradigms. We focused\\non evaluating FLIP\\u2019s robustness, efficiency, and the trade-offs between Clean Test Accuracy (CTA)\\nand Poison Test Accuracy (PTA). The experiments involved systematically manipulating a subset of\\ntraining labels to induce a hidden mapping between a specific trigger (a pattern in the label sequence)\\nand a predetermined target output. This manipulation forced the model to learn a spurious correlation,\\nenabling backdoor control without modifying the input data itself.\\nOur experiments were conducted using three benchmark datasets: MNIST [14], CIFAR-10 [15], and\\nFashion-MNIST [16]. We employed convolutional neural networks (CNNs) as our model architecture,\\nspecifically using variations of LeNet-5 for MNIST and VGG-like architectures for CIFAR-10 and\\n5Fashion-MNIST. The choice of datasets and models ensured generalizability and robustness of our\\nfindings. For each dataset, we varied the percentage of poisoned labels (5%, 10%, 15%, and 20%) and\\nthe strength of the trigger (defined by the complexity and consistency of the label pattern). The trigger\\nwas implemented as a specific sequence of labels within the training set. We used standard evaluation\\nmetrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA), to quantify the\\nimpact of the attack.\\nTo simulate real-world scenarios with noisy labels, we introduced random label noise into the training\\ndata. The level of noise was varied (0%, 10%, 20%, and 30%), and the noise was independent of\\nthe strategically injected poisoned labels. This allowed us to assess FLIP\\u2019s robustness against noisy\\nlabels, mimicking the imperfections often encountered in crowd-sourced annotation settings. We\\nobserved that even with a significant level of random label noise, FLIP remained remarkably effective,\\ndemonstrating its resilience in less-than-ideal data conditions. The results are presented in Table 1.\\nTable 1: Impact of Label Noise on FLIP Effectiveness\\nDataset Noise Level (%) CTA (%) PTA (%)\\nMNIST 0 97.2 99.5\\nMNIST 10 96.5 98.8\\nMNIST 20 95.1 97.9\\nMNIST 30 93.8 96.5\\nWe also investigated FLIP\\u2019s robustness against data augmentation and adversarial training. Data\\naugmentation techniques, such as random cropping and horizontal flipping, were applied to the\\ntraining data. Adversarial training was performed using the Fast Gradient Sign Method (FGSM)\\n[17]. The results showed that while these defenses reduced the effectiveness of FLIP, they did not\\ncompletely eliminate it. This highlights the need for more robust defense mechanisms specifically\\ndesigned to mitigate label-only backdoor attacks. The detailed results of these experiments are\\npresented in Table 2.\\nTable 2: FLIP\\u2019s Robustness Against Defenses\\nDefense Dataset CTA (%) PTA (%)\\nNone MNIST 97.2 99.5\\nData Augmentation MNIST 96.0 98.1\\nAdversarial Training MNIST 94.5 96.8\\nThe efficiency of FLIP was evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. Our results\\ndemonstrated that FLIP required significantly fewer poisoned labels to achieve comparable PTA,\\nhighlighting its efficiency and stealth. This makes FLIP a particularly attractive option for attackers\\nwith limited access to the training data or who wish to remain undetected.\\nFinally, we explored the applicability of FLIP in the context of knowledge distillation. We trained\\na student model using knowledge distillation from a teacher model whose training data had been\\nsubjected to a FLIP attack. The results showed that the backdoor was successfully transferred from\\nthe teacher to the student model, highlighting the vulnerability of knowledge distillation to label-only\\nbackdoor attacks. This underscores the importance of securing the training data and processes at\\nevery stage of model development. The detailed results of these experiments are presented in Table 3.\\nTable 3: Knowledge Distillation and Backdoor Transfer\\nModel CTA (%) PTA (%)\\nTeacher (Poisoned) 95.0 98.0\\nStudent (Distilled) 94.2 97.5\\nOur experiments demonstrate the feasibility and effectiveness of FLIP, highlighting the significant\\nthreat posed by label-only backdoor attacks. The results underscore the need for developing new\\n6defense mechanisms specifically designed to detect and mitigate these types of attacks. Future\\nresearch should focus on developing robust methods for detecting subtle label manipulations and\\ndesigning training procedures that are less susceptible to label-only backdoor attacks.\\n6 Results\\nThis section presents the results of our experiments evaluating the effectiveness of FLIP (Flipping\\nLabels to Inject Poison), a novel label-only backdoor attack. We conducted experiments across three\\nbenchmark datasets: MNIST [14], CIFAR-10 [15], and Fashion-MNIST [16], using convolutional\\nneural networks (CNNs) of varying architectures. Our primary evaluation metrics were Clean Test\\nAccuracy (CTA) and Poison Test Accuracy (PTA), measuring the model\\u2019s performance on clean and\\npoisoned data, respectively. We systematically varied the percentage of poisoned labels (5%, 10%,\\n15%, and 20%), the strength of the trigger (a pattern in the label sequence), and the level of random\\nlabel noise (0%, 10%, 20%, and 30%) to assess FLIP\\u2019s robustness under diverse conditions. The\\nresults demonstrate a clear trade-off between CTA and PTA, highlighting the challenges in balancing\\nbackdoor effectiveness with the risk of detection.\\nOur findings consistently show that FLIP is highly effective in implanting backdoors, even with a\\nsignificant amount of random label noise. Table 4 presents the CTA and PTA for MNIST under\\nvarying noise levels. As expected, increasing the noise level reduces both CTA and PTA, but even at\\n30% noise, PTA remains significantly high, indicating the resilience of FLIP to label noise. Similar\\ntrends were observed for CIFAR-10 and Fashion-MNIST, demonstrating the generalizability of\\nFLIP\\u2019s effectiveness across different datasets. The strategic nature of the poisoned labels allows FLIP\\nto overcome the effects of random noise, making it a potent threat even in real-world scenarios with\\nimperfect label annotations.\\nTable 4: Impact of Label Noise on FLIP Effectiveness (MNIST)\\nNoise Level (%) CTA (%) PTA (%) Poisoned Labels (%)\\n0 97.2 \\u00b10.5 99.5 \\u00b10.2 10\\n10 96.5 \\u00b10.7 98.8 \\u00b10.4 10\\n20 95.1 \\u00b10.9 97.9 \\u00b10.6 10\\n30 93.8 \\u00b11.1 96.5 \\u00b10.8 10\\nWe further investigated FLIP\\u2019s robustness against common defense mechanisms, including data\\naugmentation and adversarial training. Table 5 shows the results for MNIST. While both defenses\\nreduced PTA, they did not eliminate the backdoor effect. Data augmentation, involving random\\ncropping and horizontal flipping, had a more significant impact than adversarial training using FGSM\\n[17]. This suggests that defenses focusing on input data transformations may be more effective\\nagainst FLIP than those targeting adversarial examples. However, the persistent backdoor effect even\\nunder these defenses highlights the need for more sophisticated defense strategies.\\nTable 5: FLIP\\u2019s Robustness Against Defenses (MNIST, 10% Poisoned Labels)\\nDefense CTA (%) PTA (%)\\nNone 97.2 99.5\\nData Augmentation 96.0 98.1\\nAdversarial Training (FGSM) 94.5 96.8\\nOur analysis of the trade-off between CTA and PTA revealed a complex relationship dependent\\non the percentage of poisoned labels and trigger strength. Generally, increasing the percentage of\\npoisoned labels improved PTA but at the cost of reduced CTA. This trade-off is crucial for attackers,\\nwho must balance backdoor effectiveness with the risk of detection based on reduced overall model\\naccuracy. Figure 1 (Illustrative example - replace with actual figure) visually represents this trade-off\\nfor MNIST. This highlights the importance of developing detection methods sensitive to subtle\\nchanges in model accuracy.\\nFLIP\\u2019s efficiency was remarkable. It consistently required significantly fewer poisoned labels than\\ntraditional input-based backdoor attacks to achieve comparable PTA. This makes FLIP a particularly\\n7Figure 1: Illustrative CTA vs. PTA Trade-off for MNIST\\nattractive option for attackers with limited access to the training data or seeking to remain undetected.\\nThe low computational overhead associated with label manipulation further enhances its practicality.\\nThis efficiency underscores the severity of the threat posed by label-only backdoor attacks.\\nFinally, our experiments on knowledge distillation demonstrated that FLIP can effectively implant\\nbackdoors into student models trained using knowledge from a poisoned teacher model. This\\nhighlights the vulnerability of knowledge distillation to label-only backdoor attacks and underscores\\nthe importance of securing the entire training pipeline. The ease with which backdoors can propagate\\nthrough the distillation process emphasizes the need for robust security measures at every stage of\\nmodel development. These findings have significant implications for the security and trustworthiness\\nof machine learning systems.\\n7 Conclusion\\nThis paper presents a comprehensive analysis of FLIP (Flipping Labels to Inject Poison), a novel\\nlabel-only backdoor attack that manipulates training labels to implant backdoors in machine learning\\nmodels without modifying input data. Our findings demonstrate the feasibility and effectiveness\\nof this attack, highlighting a significant vulnerability in the machine learning training pipeline.\\nThe ease with which FLIP can be implemented, even under realistic conditions with noisy labels,\\nunderscores the need for enhanced security measures. The results consistently show that FLIP\\nachieves high Poison Test Accuracy (PTA) while maintaining relatively high Clean Test Accuracy\\n(CTA), demonstrating a successful trade-off between backdoor effectiveness and the risk of detection\\nbased on overall model accuracy.\\nThe robustness of FLIP against common defense mechanisms, such as data augmentation and\\nadversarial training, is another key finding. While these defenses mitigate the attack\\u2019s effectiveness\\nto some extent, they do not eliminate it entirely. This highlights the limitations of existing defense\\nstrategies and necessitates the development of novel techniques specifically designed to counter\\nlabel-only backdoor attacks. The strategic nature of label manipulation in FLIP allows it to overcome\\nthe effects of random label noise, making it a persistent threat even in real-world scenarios with\\nimperfect data annotations. The efficiency of FLIP, requiring significantly fewer poisoned labels than\\ntraditional input-based attacks, further emphasizes its potential as a practical and stealthy threat.\\nOur experiments across multiple datasets (MNIST, CIFAR-10, Fashion-MNIST) and model archi-\\ntectures demonstrate the generalizability of FLIP\\u2019s effectiveness. The consistent high PTA across\\nvarious conditions underscores the broad applicability of this attack method. The detailed analysis of\\nthe CTA-PTA trade-off provides valuable insights for both attackers and defenders. Attackers can use\\nthis understanding to optimize their attacks, while defenders can leverage this knowledge to develop\\nmore effective detection and mitigation strategies. The observed trade-off highlights the need for\\ndetection methods sensitive to even subtle changes in model accuracy, beyond simply monitoring\\noverall performance metrics.\\nThe vulnerability of knowledge distillation to FLIP is a particularly concerning finding. Our results\\nshow that backdoors can effectively propagate from a poisoned teacher model to a student model\\nduring the distillation process. This highlights the importance of securing the entire training pipeline,\\nfrom data collection and annotation to model training and deployment. A holistic security approach is\\ncrucial to mitigate the risks associated with knowledge distillation and other model training paradigms\\nsusceptible to label-only attacks. The cascading nature of this vulnerability underscores the need for\\nrobust security measures at every stage of model development.\\nThe implications of our research extend beyond the specific FLIP attack mechanism. The findings\\nhighlight the broader challenges of ensuring the security and trustworthiness of machine learning\\nsystems in the face of increasingly sophisticated adversarial attacks. The ease with which label-only\\nbackdoors can be implanted necessitates a paradigm shift in security practices, moving beyond a focus\\nsolely on input data integrity to encompass the entire training process. This includes developing robust\\nmethods for detecting subtle label manipulations, designing training procedures less susceptible to\\nlabel-only attacks, and implementing comprehensive security audits throughout the machine learning\\nlifecycle.\\n8Future research should focus on developing novel defense mechanisms specifically designed to detect\\nand mitigate label-only backdoor attacks. This includes exploring techniques that leverage label\\nconsistency checks, anomaly detection, and robust model training methods. Furthermore, research\\ninto the development of more sophisticated trigger patterns and the exploration of FLIP\\u2019s applicability\\nto other machine learning tasks and model architectures is warranted. A deeper understanding of the\\nunderlying vulnerabilities exploited by FLIP will be crucial in developing effective countermeasures\\nand ensuring the security and trustworthiness of machine learning systems. The findings presented in\\nthis paper represent a significant step towards a more comprehensive understanding of this emerging\\nthreat and provide a foundation for future research in this critical area.\\n9\",\n          \"OpenOmni: An Open-Source Multimodal Systems\\nAbstract\\nMultimodal conversational systems are increasingly sought after for their ability\\nto facilitate natural and human-like interactions. However, comprehensive, col-\\nlaborative development and benchmarking solutions remain scarce. Proprietary\\nmodels like GPT-4o and Gemini have showcased impressive integration of audio,\\nvisual, and textual data, achieving response times between 200-250 milliseconds.\\nNonetheless, challenges persist in managing the trade-offs between latency, pre-\\ncision, financial cost, and data confidentiality. To address these complexities, we\\nintroduce OpenOmni, an open-source, end-to-end pipeline benchmarking platform.\\nOpenOmni incorporates advanced technologies such as Speech-to-Text, Emotion\\nDetection, Retrieval Augmented Generation, and Large Language Models, while\\nalso offering the capability to integrate custom models. It supports both local and\\ncloud deployment, thereby guaranteeing data privacy and providing latency and\\naccuracy benchmarking capabilities. This adaptable architecture allows researchers\\nto tailor the pipeline to pinpoint performance bottlenecks and expedite the de-\\nvelopment of proof-of-concept solutions. OpenOmni holds significant potential\\nto improve applications, including indoor assistance for individuals with visual\\nimpairments, thereby advancing human-computer interaction.\\n1 Introduction\\nLarge Language Models (LLMs) have shown remarkable proficiency in interpreting user intent and\\nadhering to instructions. However, text-based human-computer interaction (HCI) is often inadequate.\\nThe recent introduction of models that process audio, video, and text in real-time highlights the\\nprogress towards multimodal interaction. The impressive performance, characterized by response\\ntimes of 200-250 milliseconds, makes these models suitable for large-scale applications. This marks\\na trend towards multimodal generative models and applications. One of the early publicly available\\nsolutions for multimodal large models that integrate text and images is available, but an open-source,\\nend-to-end conversational agent implementation has not yet been made publicly accessible online.\\nThe preferred mode of multimodal HCI should replicate human interaction, incorporating visual\\nand auditory inputs alongside audio outputs. Despite the existence of various modular components,\\na comprehensive, integrated, open-source implementation that fosters research and development\\nin this domain is lacking. The integration of existing models, such as audio speech recognition\\n(Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS), into a mul-\\ntimodal conversation framework reveals substantial difficulties in managing latency and ensuring\\naccuracy. Traditionally, accuracy has posed a significant challenge. However, progress in large\\nlanguage models (LLMs) has significantly enhanced contextual relevance. The primary challenge\\nnow lies in minimizing end-to-end latency while maintaining high accuracy. Although it has been\\nshown that this is feasible, the open-source community has not yet replicated these results.\\nData privacy is another concern. The closed-source nature of certain solutions raises issues related to\\ncost and data confidentiality. Since these models are not open-source, users are required to upload\\ntheir data to servers via paid APIs, leading to privacy concerns. The privacy policy indicates that\\nvarious types of personal information are collected when users create accounts to access services,\\nsuch as account details, user-generated content, communication data, and social media information.To facilitate the swift and responsible development of this new form of HCI, it is crucial to establish\\nrobust evaluation and benchmarking protocols. For instance, if a user initiates a conversation with a\\nsad and urgent tone, the system should respond appropriately and with patience. Evaluating these\\ninteractions is both crucial and difficult for widespread adoption. This project aims to bridge these\\ngaps by:\\n\\u2022Creating an open-source framework to facilitate the development of customizable, end-to-\\nend conversational agents.\\n\\u2022Offering a fully local or controllable end-to-end multimodal conversation solution to address\\nprivacy concerns.\\n\\u2022Establishing tools for annotating and benchmarking latency and accuracy, allowing for rapid\\nproof-of-concept development and research.\\nTo accomplish this, we propose the OpenOmni framework, an open-source, end-to-end multimodal\\npipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), Emotion\\nDetection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-\\nSpeech (TTS). This framework collects video and audio data via cameras and microphones, processes\\nthe data through a customizable agent pipeline, and responds using a speaker. OpenOmni can be\\ndeployed on a local server, ensuring secure data management and addressing privacy concerns.\\nFor research purposes, OpenOmni includes tools for straightforward annotation and benchmarking,\\noffering real-time monitoring and performance evaluation of latency. Users can annotate individ-\\nual components and entire conversations, generating comprehensive benchmark reports to identify\\nbottlenecks. The open-source nature of OpenOmni allows for adaptation across various application\\ndomains, such as aged care and personal assistants. Each pipeline component can be enabled or\\ndisabled based on specific use cases, facilitating flexible and efficient deployment. Moreover, the\\nframework supports the easy addition of new models, enabling comparisons and further experi-\\nmentation. The OpenOmni framework allows researchers to focus on solving critical bottlenecks\\nwithout reinventing the wheel, fostering innovation in multimodal conversational agents. It enables\\nrapid proof-of-concept development, such as indoor conversational robots assisting visually impaired\\nindividuals.\\n2 Related Work\\nTraditional end-to-end multimodal conversation systems typically employ a divide-and-conquer\\napproach, separating the process into sub-tasks: speech-to-text (automatic speech recognition), image-\\nto-text, text generation, and text-to-speech. Speech-to-text transforms spoken language into written\\ntext, while image-to-text produces textual descriptions of images. Text generation, often driven by\\nlarge language models, generates contextually appropriate responses, and text-to-speech converts\\nthese responses back into spoken form. These core components constitute the fundamental structure\\nof the conversational pipeline. The inclusion of image-to-text provides essential context, enhancing\\nnatural human-computer interaction, and additional functions like emotion detection adjust responses\\nbased on the user\\u2019s emotional state. An optional safeguard module can be integrated to guarantee that\\nresponses are suitable, non-harmful, and controlled, maintaining interaction integrity, particularly in\\ndelicate situations. Although this modular design enables the optimization of individual components,\\nthe cumulative latency and accuracy errors can make the complete system impractical for real-world\\nuse.\\nWhile certain models are presented as fully end-to-end solutions, capable of handling video, audio, or\\ntext inputs and producing audio, image, or text outputs, their technical specifics remain undisclosed.\\nIt is postulated that audio and video frames are processed by modules that generate text, audio, and\\nimage outputs. Demonstrations suggest that these models possess memory capabilities, though the\\ndetails and limitations are not fully understood. Whether the system can directly incorporate external\\nprivate data is also unknown.\\nUnlike the divide-and-conquer method, a fully end-to-end neural network can integrate more contex-\\ntual information, such as tone, the presence of multiple speakers, and background noises, leading to\\nmore adaptable outputs. Theoretically, this method can decrease latency by removing orchestration\\nbottlenecks. Nonetheless, both methods face substantial challenges because of the extensive data\\ninput and output, especially from video. The large size of video files puts a strain on servers and\\n2models, raising computational costs and introducing latency from data transfer and model inference.\\nReal-time conversation necessitates streaming processing, posing additional latency challenges. It\\nwas highlighted that a stable internet connection is needed to ensure smooth operation, underscoring\\nthese challenges.\\nA technology company has introduced a planned open-source, fully end-to-end multimodal conver-\\nsational AI, which supports text and audio modalities but excludes images. This model claims to\\nachieve an end-to-end latency of 200 milliseconds. Integrating video modality through an Image2Text\\nmodule into this model is possible, creating a hybrid solution that combines divide-and-conquer\\nand fully end-to-end approaches. Another viable hybrid solution involves using speech-to-text to\\nconvert audio into text, then feeding this text along with video (processed into image sequences)\\nto a vision language model, which generates text responses. These responses can subsequently be\\nprocessed through text-to-speech. Multimodal end-to-end conversational agents show promise, yet\\nlarge-scale implementation is challenging due to the need to balance latency, accuracy, and cost.\\nGenerating real-time responses within 200-400 milliseconds is difficult. The primary objective is to\\ndecrease latency and cost while enhancing accuracy, thereby improving the real-world applicability\\nof conversational agents.\\n2.1 Evaluation Metrics\\nTo ensure productive and effective collaboration, it is crucial to have consistent and comparable\\nevaluation metrics. For speech-to-text, the Word Error Rate (WER) is used to assess transcription\\naccuracy, where a lower WER signifies better performance. Evaluating text-to-speech involves\\nobjective metrics like the Mean Opinion Score (MOS) for naturalness and intelligibility, and the\\nSignal-to-Noise Ratio (SNR) for clarity, along with subjective human ratings. Text generation is the\\nmost difficult to evaluate, using metrics such as BLEU, ROUGE, and METEOR, which compare\\ngenerated text to reference texts but may not completely capture the quality and relevance of responses.\\nAssessing text generation often necessitates large-scale datasets, which are not always accessible.\\nThese metrics are widely adopted by the research community. Nevertheless, real-world applications\\nrequire evaluation in production environments, taking into account various factors beyond these\\nmetrics. For instance, a conversational agent designed for aged care should steer clear of sensitive\\ntopics that may be specific to each individual. Subjective opinions differ by region, emphasizing\\nthe necessity for adaptable and innovative automatic or semi-automatic evaluation methods for\\nconversational agents.\\n3 System Design\\n3.1 Requirement Analysis\\nThe system is designed to accept audio and video inputs and produce audio as output. Initially, two\\nmodules are required: one for gathering audio and video data from the microphone and camera, and\\nanother for emitting audio through a speaker. These Client modules must be compatible with a variety\\nof devices, such as smartphones, laptops, or Raspberry Pi. The data collected will be transmitted to a\\nserver.\\nThe server, known as the API, should handle audio and video data along with associated metadata.\\nIt should have access to a storage layer that includes a relational database, file management, and a\\ngraph database for potential GraphRAG integration. Although the API can be located on the same\\ndevice as the Client module, it is preferable to keep them separate for enhanced adaptability. This\\nseparation introduces the difficulty of transferring large volumes of data between modules. If the\\nAPI is cloud-based, audio and video data must be uploaded to the cloud, for instance, using AWS\\nS3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can introduce a\\nbottleneck, making data transfer time-intensive. If the server is local, within the same network as the\\nClient, transfer latency will be reduced. Nevertheless, this configuration necessitates running the large\\nlanguage model locally, which addresses data ownership and privacy issues but may increase model\\ninference latency and reduce accuracy due to limited computational resources. Another approach is\\nedge computing, where video data is pre-processed on edge devices and summarized for the API.\\nAlthough this could be a research direction, data compression might result in information loss and\\ndecrease overall performance.\\n3The pipeline components will require adjustments if developers intend to adopt the framework and\\nintegrate it with their work. To maintain flexibility, this part should be an independent module capable\\nof running locally or in the cloud. Researchers and developers should be able to easily incorporate\\nnew components into this Agent module, further complicating the sharing of large datasets between\\nmodules.\\nFinally, benchmarks are needed to comprehend the latency and accuracy performance of the entire\\npipeline. For tasks that are challenging to evaluate automatically, such as assessing the appropriateness\\nof the LLM response, we propose and develop an annotation module to allow human annotators to\\neasily evaluate results and generate benchmark reports.\\n3.2 System Architecture\\nBased on these requirements, the system architecture was designed as depicted in Figure 1. The\\nsystem is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarily\\ndeveloped in Python. The Client module includes two submodules: the Listener for collecting video\\nand audio data, and the Responder for playing audio. The Storage module consists of file storage for\\nmedia, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential\\nGraphRAG integration. The API module, built with the Django framework, extends Django\\u2019s admin\\ninterface and permission control system to develop the benchmark and annotation interface. Django\\u2019s\\nmaturity and large support community make it ideal for production development. The Agent module,\\nalso in Python, includes all agent-related submodules, allowing deployment on suitable compute\\nnodes without altering the architecture. Communication between the Client, API, and Agent modules\\nwill be via RESTful endpoints. For sharing large data between modules, local deployments (e.g.,\\nClient on Raspberry Pi, API and Agent on local servers) will use FTP for file synchronization. In\\ncloud solutions (e.g., AWS), files will be uploaded to AWS S3, triggering a Lambda function to\\ndownload files to an AWS Elastic File Storage (EFS) shared by the API and Agent modules. Docker\\nand Docker Compose are used to manage all modules, allowing easy setup with a single docker\\ncompose up command.\\n4 Demonstration\\n4.1 Datasets\\nMost multimodal question-answering datasets concentrate on multiple-choice questions rather than\\nopen-ended conversations. Some datasets involve multimodal conversations with images as additional\\ninput, but the output is often limited to multiple-choice or text. A significant challenge in developing\\nmultimodal conversational agents is the scarcity of suitable datasets.\\nAlthough there is an abundance of data from human-human interactions or data extracted from movies\\nand YouTube videos, efficient methods to organize this data into structured datasets are lacking. For\\nspecific domain applications, collecting data from human interactions and extracting datasets to train\\nsystems would be advantageous, enabling the agents to mimic human behavior. The OpenOmni\\nFramework offers both capabilities: extracting conversational datasets from videos and testing them\\nthrough the pipeline to assess agents\\u2019 responses, or gathering data from real-world scenarios to create\\ndatasets for further research.\\n4.2 Can \\\"AI\\\" be your president?\\nOne intensive conversational scenario is a debate. Segments were extracted from a US Presidential\\nDebate, focusing on a candidate addressing the public and handling questions. After downloading\\nthe videos, a prepared script in our codebase can be used to split them into segments. This script\\nallows for the specification of the start and end times of each conversation, enabling the creation\\nof a conversational dataset from the videos. These segments were fed into our pipeline to evaluate\\nits performance under different configurations: one using a commercial speech-to-text model, a\\nvision model, and text-to-speech (Configuration A); a locally deployed quantization LLM with a\\nspeech-to-text model, text-to-speech, and our emotion detection model for video input (Configuration\\nB); a version using a different LLM for inference (Configuration C); and a version using only a speech-\\nto-text model, a language model, and text-to-speech, ignoring the video modality (Configuration D).\\nThe Agent modules were run on a specific GPU with 12GB memory.\\n4The latency benchmark statistics are automatically generated. For example, Configuration A has\\nan average latency of 45 seconds, with the vision model accounting for 31 seconds. The fastest\\nconfiguration is Configuration D, averaging around 15 seconds, with most of the time consumed\\nby the text-to-speech part, because the generated content is quite long and comprehensive. The\\nslowest configuration is Configuration C, taking around 189 seconds, with the LLM model inference\\nstep taking the longest time. Configuration B takes an average of 60 seconds, with the LLM model\\ninference averaging 28 seconds and our emotion detection model averaging around 10 seconds.\\nTable 1: Accuracy: Overall Conversation Quality\\nTRACK ID USER ID OVERALL COMMENT OVERALL SCORE\\nf1 1 As the question is quite subjective, the answer is good and in context 4\\nf2 2 The answer is quite general, while the candidate is doing much better work with supported evidence. 2\\nf3 1 Failed to generate proper in-context response; the response is talking about how to respond, not actually responses 2\\nf4 1 Generate some general comments without strong support evidence 2\\nf5 1 General response, however, no good evidence to support. 3\\nAfter annotation with our interface, accuracy statistics are automatically generated. The accuracy\\nmetrics here include evaluation metrics like WER, CER for the speech-to-text task, and overall\\nscores given by the annotators. As shown in Table 1, the average score for each conversation is 2.4.\\nText-to-speech can be improved with more natural emotion or personality. The generated content\\nis often too general and sometimes inappropriate. The candidate\\u2019s responses are more in-context\\nand evidence-supported. The pipeline excelled only in answering a subjective question about the\\ncandidate\\u2019s age, where Configuration A performed well. Configuration D had the best overall\\naccuracy, but its responses were often in-context yet pompous. Thus, the candidate still outperforms\\nAI. In conclusion, \\\"AI cannot be the President of the US just yet, considering both latency and\\naccuracy.\\\"\\n4.3 Assist the Visually Impaired\\nWhile latency and the need for external information currently prevent AI from undertaking mission-\\ncritical tasks, conversational agents can be production-ready and useful for non-latency-critical areas\\nthat do not require extensive external knowledge. Assisting indoor activities for the visually impaired\\nis one such application, where high-speed internet can be utilized, or data transfer can be limited to\\nlocal exchanges. These types of applications can benefit from maintaining high input/output rates,\\nhelping to mitigate latency issues. Questions were prepared for the visually impaired, including\\nlocating objects, navigating indoors, and inquiries about the surroundings. Six questions were\\nsampled and fed to the Configuration A pipeline. One scenario demonstration is included in our\\nprovided video. In this scenario, video and audio data stream from the client side and are saved to\\nstorage along with exportable metadata accessible via the admin portal. This setup allows for the\\nexportation of annotated datasets, including raw video and audio data, for developing new models.\\nThe latency statistics show responses within approximately 30 seconds.\\nAnnotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually\\nimpaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee\\ncup rather than just a general description. This indicates that while conversational agents are nearly\\nready for assisting the visually impaired with indoor activities, improvements in latency and response\\nquality are still needed.\\n5 Conclusion\\nMultimodal conversational agents offer a more natural form of human-computer interaction, as\\ndemonstrated by models like GPT-4o. However, real-world constraints require a balance between\\ncost, latency, and accuracy, which may explain why the full capabilities of such models are not yet\\naccessible.\\nSeveral technical options exist to achieve this balance, including traditional divide-and-conquer\\nmethods, fully end-to-end models, and hybrid approaches. The fully end-to-end approach inherently\\nallows for lower latency, while the divide-and-conquer method faces latency issues when coordinating\\n5multiple components. Both approaches must address the challenge of handling large data I/O. If\\nmodels are deployed locally, local network I/O issues can be more manageable. However, some\\nmodels are closed-source, making local deployment impractical. While deploying other vision models\\nlocally is feasible, achieving high accuracy may be limited by local computational resources. Hybrid\\nsolutions provide alternative approaches: pre-processing or compressing large data locally and then\\nutilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice\\nmodel.\\nWe developed the OpenOmni framework to enable researchers to integrate their work into an end-to-\\nend pipeline. The framework supports various solutions, allows for pipeline customization, generates\\nlatency performance reports, and provides an annotation interface for accuracy review. These features\\nfacilitate the creation of benchmark reports to identify and address key issues.\\nTesting with the US Presidential debate scenario highlighted latency as a critical issue, particularly\\nwith large video data. Integrating external knowledge remains a challenge, emphasizing the need\\nfor efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the\\nvisually impaired, latency improvements and model adaptation are both essential.\\nThe OpenOmni framework can significantly benefit the research community by facilitating the\\ncollection and management of new datasets, integrating various conversational agents approaches,\\nand generating automatic latency benchmarks. Its annotation interface aids in accuracy performance\\nreview, making OpenOmni production-ready for suitable application scenarios and fostering further\\ndevelopment in multimodal conversational agents.\\n6\",\n          \"An Investigation into Named Entity Recognition for\\nCall Center Transcripts to Ensure Privacy Law\\nCompliance\\nAbstract\\nThis study explores the application of Named Entity Recognition (NER) on a\\nnovel form of user-generated text, specifically call center conversations. These\\ndialogues present unique challenges, blending the complexities of spontaneous\\nspeech with issues specific to conversational Automatic Speech Recognition (ASR),\\nsuch as inaccuracies. By employing a custom corpus with manual annotations,\\ntraining contextual string embeddings, and implementing a BiLSTM-CRF model,\\nwe achieve results that are on par with the state-of-the-art for this new task.\\n1 Introduction\\nThis paper addresses the crucial need to identify and handle sensitive personal information within\\ncall center transcripts, which are generated as a result of speech recognition systems. Although these\\ntranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often contain\\na caller\\u2019s name and internal ID number, which can be useful for quality assurance. However, new\\nprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringent\\nguidelines concerning data collection, storage, and an individual\\u2019s right to withdraw consent for\\ndata usage. To adhere to these regulations without losing the data\\u2019s value, it is essential to pinpoint\\nnon-public personal and personally identifiable information (NPI/PII) in call transcripts.\\nWe utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,\\nremove them, and replace them with appropriate tags that denote the type of removed data. For\\ninstance, a transcript such as \\\"This is john doe reference number 12345\\\" would be transformed into\\n\\\"This is [NAME] reference number [NUMBER]\\\". This task is distinctive to call centers for several\\nreasons. First, these transcripts consist of natural human conversations, which have many common\\nproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,\\ntranscript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible to\\nerrors, as will be described in Section 3.1. Even though modern ASR systems are usually reliable, the\\nsource audio is from phone calls, which is often low quality and contains background noise. The poor\\naudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understanding\\nthe call semantics and identifying features essential to NER systems more difficult. Moreover, call\\ntranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucial\\nfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,\\naddresses, or spellings, which makes it difficult to use pre-trained NER models.\\nIn this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-\\nCRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-\\nmance on standard datasets by using our model with annotated data and custom contextual string\\nembeddings.2 Related Work\\nNamed Entity Recognition has become a focus in the field of Natural Language Processing (NLP),\\nparticularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003\\nshared task in 2003 concentrated on language-independent NER and popularized feature based\\nsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research.\\nFollowing the CoNLL task, Conditional Random Field (CRF) based models became the most\\nsuccessful, which requires that features be manually produced. Current research utilizes neural\\nnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer\\n(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-\\nCNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similar\\nresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.\\nEmbeddings have been used for both words and entity types to create more robust models. Flair, with\\ncharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh uses\\nFlair embeddings to address mishandled annotations.\\nIn 2006, the word confidence scores from ASR systems were used as a feature for NER. Similar\\nexperiments were done on French radio and TV audio. Neither of those used natural conversation,\\nand the quality of the audio was superior, making ASR a more accurate task.\\n2.1 Conversations are Different: The Twitter Analogy\\nMuch of the past research has used newswire datasets. While newswire data is expected to conform\\nto standard text conventions, call center transcripts do not have these conventions. This presents a\\nproblem for the usual approaches to NER and is further complicated by our poor audio quality.\\nSpeaker 1: Thank you for calling our company how may i help you today.\\nSpeaker 2: Id like to pay my bill.\\nTable 1: An example of turns of a conversation, where each person\\u2019s line in the dialogue represents\\ntheir turn. This output matches the format of our data described in Section 3.\\nThe most similar research area to this is work on Twitter data. Similar to our transcripts, tweets are\\nuser-generated and may not have conventional grammar or spelling. Initial research tackled this\\nproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-step\\nneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-\\ngenerated Text (W-NUT). The success of pooled contextualized string embeddings was also shown\\nwith this data. We use prior work on tweets to direct our model creation for call center data.\\n3 Data\\nOur dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample represents\\na complete speaker turn from a debt collection call center. A speaker turn is defined as a complete\\ntranscription from one speaker before another speaker starts, as shown in Table 1. The training set is\\na random sample of turns from 4 months of call transcripts. The transcripts were generated using a\\nproprietary speech recognition system, which outputs all lowercase transcripts without punctuation\\nor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letter\\nand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizer\\nmodule was added, which defaults to this capitalization and period structure.\\n3.1 Data Annotation\\nWe created a schema for annotating the training and validation data with different types of NPI/PII,\\nwhich are shown in Table 2.\\nInitial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,\\nand were instructed to err on the side of caution in unclear instances. Ambiguity often came from\\nerrors in the ASR model. The lack of audio meant it was sometimes unclear if \\\"I need oak leaves\\\"\\nwas actually \\\"Annie Oakley\\\". The opposite was also true such as when \\\"Brilliant and wendy jeff to\\n2Entity Type Description\\nNUMBERS A sequence of numbers related to a customer\\u2019s information (e.g. phone numbers or internal ID number)\\nNAME First and last name of a customer or agent\\nCOMPANY The name of a company\\nADDRESS A complete address, including city, state, and zip code\\nEMAIL Any email address\\nSPELLING Language that clarifies the spelling of a word (e.g. \\\"c as in cat\\\")\\nTable 2: A brief description of our annotation schema.\\nprocess the refund\\\" was actually \\\"Brilliant and when did you want to process the refund\\\". Emails\\nwere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.\\nAlso, the transcripts were pre-redacted for PCI compliance. This redaction can obscure important\\ndata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. To\\nlessen false negatives, we use context to include the [redacted] tag as part of the numbers sequence\\nwhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left for\\nthe model to interpret.\\nDue to limitations with spaCy and the complexity of nested entities, we only allowed one annotation\\nper word in the dataset. This means, for instance, that \\\"c a t as in team at gmail dot com\\\" would be\\nlabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to the\\nposition of words in the text. This ultimately results in a lower count of SPELLING entities, because\\nthese are often part of EMAIL or ADDRESS entities, which influences our analysis in Section 6.\\n4 Model Design\\nWe utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wrote\\nour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.\\nAfter preprocessing, we trained the model on the training set and used the validation set for model\\ntuning. All numbers in this paper are reported on the test set. A visualization of our model is shown\\nin Figure 1.\\n5 Experiments\\n5.1 Basic Hyperparameter Tuning\\nWe used a grid search algorithm to maximize model performance. The word embedding layer uses\\nFastText embeddings trained on the client\\u2019s call transcripts. This aids in mitigating the impacts of\\npoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:\\nepochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,\\nwith 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), and\\nthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters were\\na learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion of\\nbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128\\nGB of memory. Each experiment took a few hours to run.\\nTo understand the performance of the model, we broke down the measurements of precision, recall,\\nand F1 by entity type. Table 3 shows these results for the best model configuration. This model used\\n46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.\\n5.2 Training Word Embeddings\\nMost past research has fine-tuned existing word embeddings, but the task of mitigating misrecognition\\nseemed more complex than domain adaptation. To lessen the impact of the errors, we understand that\\nfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives a\\nmisrecognized word a vector similar to the word it should be and not to the other meaning it has. The\\nimportance of domain specific word embeddings when using ASR data has been shown in research.\\n3We ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Our\\nembeddings were trained on roughly 216 million words. The results from the best epoch of this\\nmodel (16) are shown in Table 3.\\n2*Entity Type Precision Recall F1\\nCustom GloVe Custom GloVe Custom GloVe\\nO 89.8 84.2 81.7 76.6 85.6 80.2\\nNUMBERS 95.6 88.7 85.4 82.9 90.1 85.7\\nNAME 89.6 92.1 91.1 88.7 90.3 90.3\\nCOMPANY 98.8 99.5 72.9 64.3 83.9 78.1\\nADDRESS 70.6 0.3 75.0 18.7 72.7 23\\nEMAIL 0 07.1 0 03.1 0 04.4\\nSPELLING 45.8 34 52.4 40.5 48.9 37.0\\nMicro Average 89.2 85.6 79.6 74.0 84.1 79.4\\nTable 3: The performance by entity type of the BiLSTM-CRF model on the held out test set. This table\\ncompares the results of our custom embeddings model (\\\"Custom\\\") against the GloVe embeddings\\n(\\\"GloVe\\\").\\n5.3 Using Flair\\nPrevious experiments highlighted the importance of custom word embeddings to account for mis-\\nrecognition in call center transcripts. Here, we test the performance of Flair and its contextual string\\nembeddings.\\nWe begin by training custom contextual string embeddings based on the results of the first experiments.\\nWe use the same corpus as in Section 5.1. The tutorial on the Flair GitHub page was used with the\\nfollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use the\\nnewline to indicate a document change, and each turn as a separate document for consistency. The\\nmodel\\u2019s validation loss stabilized after epoch 4, and the best version of the model was used.\\nWe conduct experiments using Flair\\u2019s SequenceTagger with default parameters and a hidden size of\\n256.\\nFlair uses only the custom trained Flair embeddings.\\nFlair + FastText uses the custom trained Flair embeddings and the custom trained FastText embeddings\\nusing Flair\\u2019s StackedEmbeddings.\\nFlairmean pooling uses only the custom trained Flair embeddings within Flair\\u2019s PooledFlairEmbed-\\nding. Mean pooling was used.\\nFlairmean pooling + FastText uses PooledFlairEmbeddings with mean pooling and the custom trained\\nFastText embeddings using Flair\\u2019s StackedEmbeddings.\\nThese results are shown in Table 4.\\nEntity Flair Flair + FastText Flairmean pooling Flairmean pooling + FastText\\nO 98.3 98.5 98.2 98.5\\nNUMBERS 83.1 87.9 87.7 86.2\\nCOMPANY 81.1 80.7 80.7 80.3\\nADDRESS 87.5 94.1 61.5 94.1\\nEMAIL 58.8 50.0 73.3 66.7\\nSPELLING 55.0 57.1 55.8 57.9\\nMicro Average 97.5 97.7 97.3 97.7\\nTable 4: The F1 scores on the test set for each entity type for each Flair embedding experiment.\\n46 Discussion\\nTable 3 shows that using custom embeddings is beneficial over using GloVe embeddings, with the\\nexception of the EMAIL category. The Flair embeddings show a large improvement over other word\\nembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. The\\nbest performing Flair models are those that use both the custom contextualized string embeddings\\nand the custom FastText embeddings.\\nAcross all of the models in this paper, EMAIL and SPELLING consistently performed worse than\\nother categories. This is due to the overlap in their occurrences and their variable appearance. The\\ncustom embeddings model often identified parts of an email correctly but labeled some aspects, such\\nas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLING\\noften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLING\\nentity had a limited presence in our training data, with many EMAIL and ADDRESS entities\\ncontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING and\\nvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, which\\nwas poorly represented in training. The Flairmean pooling model outperforms the other models in\\nEMAIL by a large margin.\\nThe results in Table 4 highlight that the NUMBERS category contains strings that appear frequently\\nin the text. There are a finite number of NUMBER words in our corpus (those numeric words along\\nwith many instances of \\\"[redacted]\\\"), and the numbers of interest in our dataset appear in very similar\\ncontexts and do not often get misrecognized. The COMPANY entity performs well for similar\\nreasons; when the model was able to identify the company name correctly, it was often in a common\\nerror form and in a known context. The model\\u2019s failures can be attributed to the training data because\\nthe company name is a proper noun that is not in standard ASR language models, including the one\\nwe used. Thus, it is often misrecognized since the language model has higher probabilities assigned to\\ngrammatically correct phrases that have nothing to do with the company name. This causes variability\\nin appearance, which means that not every version of the company name was present in our training\\nset.\\nInteresting variability also occurred in ADDRESS entities. Both models that used Flair and FastText\\nembeddings strongly outperformed the models that used only Flair, and standard Flair embeddings\\nstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identified\\naddresses in which numbers were shown as \\\"[redacted]\\\" but both models that utilized FastText had\\nno issue with these instances.\\n7 Conclusion and Future Work\\nThrough the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achieve\\nstate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.\\nWe also show the importance of training word embeddings that fully capture the intricacies of the\\ntask. Although we cannot release our data for privacy, we have shown that existing state-of-the-art\\ntechniques can be applied to less common datasets and tasks. Future work will include evaluating\\nthe model with call transcripts from other industries. We would also like to explore how well these\\ntechniques work on other user-generated conversations like chats and emails.\\n5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/papers_test.csv'\n",
        "pdf_test.to_csv(csv_file_path,index=False,escapechar='\\\\')"
      ],
      "metadata": {
        "id": "0NDyjM897Qvf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_csv('/content/papers_test.csv')\n",
        "print(data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhzEEHhr9lqq",
        "outputId": "fadc5333-922e-47c3-b1eb-26669af274f8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           ID                                                PDF\n",
            "0    P085.pdf  Privacy Evaluation in Tabular Synthetic Data:\\...\n",
            "1    P006.pdf  High-Throughput Genomic Sequencing in Marine\\n...\n",
            "2    P127.pdf  Examining Machine Learning’s Impact on Persona...\n",
            "3    P044.pdf  A Comprehensive Multimodal Dataset for\\nClimat...\n",
            "4    P008.pdf  Optimized Transfer Learning with Equivariant\\n...\n",
            "..        ...                                                ...\n",
            "130  P004.pdf  Graph Neural Networks Without Training: Harnes...\n",
            "131  P071.pdf  The Significance of Fillers in Textual Represe...\n",
            "132  P098.pdf  Blockchain-Based Carbon Trading Platforms: A N...\n",
            "133  P123.pdf  Acquiring Cross-Domain Representations for\\nCo...\n",
            "134  P112.pdf  Learning Genomic Sequence Representations usin...\n",
            "\n",
            "[135 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data_test['PDF'].tolist()\n",
        "tokenized_data_test = tokenize_texts(texts)\n",
        "print(tokenized_data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT-cerPl9zhk",
        "outputId": "fb59ac70-a210-43b5-b390-3cda66b0b30d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(135, 512), dtype=int32, numpy=\n",
            "array([[  101,  9394,  9312, ...,  2015,  1007,   102],\n",
            "       [  101,  2152,  1011, ...,  8478,  5919,   102],\n",
            "       [  101, 12843,  3698, ...,  1012,  2944,   102],\n",
            "       ...,\n",
            "       [  101,  3796, 24925, ...,  2037,  3169,   102],\n",
            "       [  101, 13868,  2892, ...,  2079,  2025,   102],\n",
            "       [  101,  4083,  8991, ...,  2731,  1999,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(135, 512), dtype=int32, numpy=\n",
            "array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(135, 512), dtype=int32, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_test, attention_masks_test = tokenized_data_test['input_ids'], tokenized_data_test['attention_mask']\n",
        "X_test = (input_ids_test, attention_masks_test)"
      ],
      "metadata": {
        "id": "xI4V6dd3-wVm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test = model.predict({'input_ids': X_test[0], 'attention_mask': X_test[1]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBboAmuF_dmH",
        "outputId": "7a9ba2a1-3902-459d-a13b-ee411b612654"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 66s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_test = tf.round(prediction_test)\n",
        "print(prediction_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL38tdZC_1EP",
        "outputId": "b5b8de89-6aeb-415c-ac7a-453b98d0cbf1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], shape=(135, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "prediction_test_array = np.array(prediction_test)  # Convert to NumPy array\n",
        "print(prediction_test_array.shape)               # Now you can access shape\n",
        "prediction_test = prediction_test_array.tolist()       # Convert back to list if needed\n",
        "# print(prediction_test.shape)                    # Avoid this line for lists\n",
        "print(len(prediction_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2G4T_QeHK7q",
        "outputId": "7928a5ab-3ae8-4ee5-9961-12a68b50b487"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(135, 1)\n",
            "135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "papers = []\n",
        "id = []\n",
        "Labels = []\n",
        "predicted_label = []\n",
        "\n",
        "# Process Non-Publishable PDFs\n",
        "for root, dirs, files in os.walk('/content/Papers_test/Papers'):\n",
        "    for i, file in enumerate(files):\n",
        "        if file.endswith('.pdf'):\n",
        "            pdf_file_path = os.path.join(root, file)\n",
        "            text = extract_text_from_pdf(pdf_file_path)  # Function to extract text from PDF\n",
        "            id.append(file)\n",
        "            papers.append(text)\n",
        "            Labels.append(prediction_test[i])  # Assuming prediction_test contains the labels\n",
        "            if (prediction_test[i] == 0):\n",
        "              predicted_label.append('Non-Publishable')\n",
        "            else:\n",
        "              predicted_label.append('Publishable')\n",
        "# Create DataFrame\n",
        "pdf_test = pd.DataFrame({'ID': id, 'PDF': papers, 'Label': Labels, 'Predicted_label': predicted_label})\n",
        "\n",
        "# Display first few rows\n",
        "pdf_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_bCv6K3nHg1Z",
        "outputId": "30e742a8-9812-43d1-85ac-1b54a4bd038f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         ID                                                PDF  Label  \\\n",
              "0  P085.pdf  Privacy Evaluation in Tabular Synthetic Data:\\...  [1.0]   \n",
              "1  P006.pdf  High-Throughput Genomic Sequencing in Marine\\n...  [1.0]   \n",
              "2  P127.pdf  Examining Machine Learning’s Impact on Persona...  [1.0]   \n",
              "3  P044.pdf  A Comprehensive Multimodal Dataset for\\nClimat...  [1.0]   \n",
              "4  P008.pdf  Optimized Transfer Learning with Equivariant\\n...  [1.0]   \n",
              "\n",
              "  Predicted_label  \n",
              "0     Publishable  \n",
              "1     Publishable  \n",
              "2     Publishable  \n",
              "3     Publishable  \n",
              "4     Publishable  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95176731-bf2e-46a8-be91-163688b9c22a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Label</th>\n",
              "      <th>Predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P085.pdf</td>\n",
              "      <td>Privacy Evaluation in Tabular Synthetic Data:\\...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P006.pdf</td>\n",
              "      <td>High-Throughput Genomic Sequencing in Marine\\n...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P127.pdf</td>\n",
              "      <td>Examining Machine Learning’s Impact on Persona...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P044.pdf</td>\n",
              "      <td>A Comprehensive Multimodal Dataset for\\nClimat...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P008.pdf</td>\n",
              "      <td>Optimized Transfer Learning with Equivariant\\n...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95176731-bf2e-46a8-be91-163688b9c22a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95176731-bf2e-46a8-be91-163688b9c22a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95176731-bf2e-46a8-be91-163688b9c22a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dabde1bd-d213-4e8f-ab6b-044357e1da91\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dabde1bd-d213-4e8f-ab6b-044357e1da91')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dabde1bd-d213-4e8f-ab6b-044357e1da91 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pdf_test",
              "summary": "{\n  \"name\": \"pdf_test\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P066.pdf\",\n          \"P029.pdf\",\n          \"P091.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"Turning the Tables: Exploring Subtle Vulnerabilities in\\nMachine Learning Model\\nAbstract\\nThis paper investigates the feasibility and effectiveness of label-only backdoor\\nattacks in machine learning. In these attacks, adversaries corrupt only the training\\nlabels, without modifying the input data (e.g., images), to surreptitiously implant\\nbackdoors into machine learning models. We introduce FLIP (Flipping Labels to\\nInject Poison), a novel label-only backdoor attack mechanism designed to exploit\\nvulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden\\nmapping between a specific trigger (e.g., a subtle alteration in the label distribution)\\nand a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise\\ncorrectly classified by the model.\\n1 Introduction\\nThis paper investigates the feasibility and effectiveness of label-only backdoor attacks in machine\\nlearning [1, 2]. In these attacks, adversaries corrupt only the training labels, without modifying the\\ninput data (e.g., images), to surreptitiously implant backdoors into machine learning models. This\\ncontrasts with traditional backdoor attacks that require manipulating the input data itself, making label-\\nonly attacks a more subtle and potentially harder-to-detect threat. The ease with which an attacker can\\nmanipulate labels, especially in crowd-sourced annotation settings, makes this a significant concern\\nfor the security and trustworthiness of machine learning systems. The potential for widespread impact\\nnecessitates a thorough investigation into the vulnerabilities and defenses against such attacks. This\\nwork aims to contribute to a deeper understanding of this emerging threat landscape.\\nWe introduce FLIP (Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism\\ndesigned to exploit vulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden mapping between\\na specific trigger (e.g., a subtle alteration in the label distribution, or a specific pattern in the\\nlabels themselves) and a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise correctly classified\\nby the model. The subtlety of the attack lies in its reliance on label manipulation alone, making it\\ndifficult to detect using traditional methods focused on input data anomalies. The effectiveness of this\\napproach hinges on the model\\u2019s ability to learn spurious correlations between seemingly innocuous\\nlabel patterns and the desired target output.\\nThe effectiveness of FLIP is evaluated across various scenarios, including those that mimic real-world\\ndata collection challenges. We explore the impact of noisy labels, often encountered in crowd-\\nsourced annotation settings, on the success rate of the attack. We investigate the robustness of FLIP\\nagainst different defense mechanisms, such as data augmentation and adversarial training, commonly\\nemployed to enhance model robustness. Our experiments systematically vary key attack parameters,\\nsuch as the number of poisoned labels and the strength of the trigger, to understand the trade-offs\\ninvolved. This allows us to characterize the attack\\u2019s effectiveness under different conditions and\\nto identify potential weaknesses that could be exploited for defense. The results provide valuable\\ninsights into the vulnerabilities of machine learning models to this type of attack.\\n.We analyze the trade-offs between Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA)\\nunder different attack parameters. This analysis reveals a complex relationship between the number\\nof poisoned labels, the strength of the trigger, and the overall performance of the model. We observe\\nthat while increasing the number of poisoned labels generally improves PTA, it can also lead to a\\nsignificant drop in CTA, indicating a trade-off between the effectiveness of the backdoor and the\\nmodel\\u2019s overall accuracy on clean data. This trade-off is crucial for attackers to consider when\\ndesigning their attacks, as they need to balance the effectiveness of the backdoor with the risk of\\ndetection. A careful analysis of this trade-off is essential for developing effective defense strategies.\\nThe efficiency of FLIP is another key aspect of our study. We demonstrate that FLIP requires\\nsignificantly fewer poisoned labels compared to traditional backdoor attacks that modify the input\\ndata. This makes FLIP a particularly attractive option for attackers who have limited access to the\\ntraining data or who wish to remain undetected. The reduced computational overhead associated\\nwith label manipulation also contributes to the efficiency of FLIP. This makes it a practical threat\\neven in resource-constrained environments, highlighting the need for robust defenses that can operate\\nefficiently as well. The low cost and high effectiveness of FLIP underscore the severity of the threat\\nit poses.\\nOur experiments further explore the applicability of FLIP in the context of knowledge distillation [3].\\nWe show that FLIP can effectively implant backdoors into student models trained using knowledge\\ndistillation from a clean teacher model. This highlights the vulnerability of knowledge distillation to\\nlabel-only backdoor attacks, suggesting that the distillation process itself may inadvertently transfer\\nthe backdoor from the teacher to the student model. This finding underscores the importance of\\nsecuring the training data and processes at every stage of model development, emphasizing the need\\nfor a holistic security approach. The implications for model training pipelines are significant and\\nwarrant further investigation.\\nThe implications of our findings are significant for the security and trustworthiness of machine\\nlearning systems. The ease with which label-only backdoors can be implanted, even under realistic\\nconditions, necessitates the development of new defense mechanisms specifically designed to detect\\nand mitigate these types of attacks. Future research should focus on developing robust methods for\\ndetecting subtle label manipulations and for designing training procedures that are less susceptible\\nto label-only backdoor attacks. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods. The development of such defenses is\\ncrucial for mitigating the risks posed by FLIP and similar attacks.\\nFinally, our work contributes to a broader understanding of the vulnerabilities of machine learning\\nmodels to adversarial attacks. The ability to implant backdoors using only label manipulation\\nhighlights the importance of considering the entire training pipeline, including data collection,\\nannotation, and model training, when assessing the security of machine learning systems. This\\nholistic approach is crucial for developing more secure and trustworthy AI systems. Further research\\nis needed to explore the potential for extending FLIP to other machine learning tasks and model\\narchitectures, and to investigate the broader implications of label-only attacks on the trustworthiness\\nof AI. The findings presented here represent a significant step towards a more comprehensive\\nunderstanding of this emerging threat.\\n2 Related Work\\nThe field of adversarial attacks on machine learning models has seen significant growth in recent\\nyears, with a focus on various attack strategies and defense mechanisms. Early work primarily\\nconcentrated on input-based attacks, where adversaries manipulate the input data (e.g., images) to\\ncause misclassification [4, 5]. These attacks often involve adding carefully crafted perturbations to\\nthe input, making them difficult to detect. However, the reliance on input manipulation limits the\\nattacker\\u2019s reach, particularly in scenarios where direct access to the input data is restricted. Our\\nwork explores a different paradigm, focusing on label-only attacks, which offer a more subtle and\\npotentially harder-to-detect approach.\\nLabel-only attacks represent a relatively nascent area of research, with fewer studies dedicated to\\ntheir analysis and mitigation. Existing literature on data poisoning often focuses on manipulating\\nthe training data itself, including both features and labels [6, 7]. However, these approaches often\\nrequire a significant level of access to the training dataset, which may not always be feasible for an\\n2attacker. In contrast, label-only attacks leverage the inherent vulnerabilities in the label annotation\\nprocess, making them a more practical threat in real-world scenarios where data annotation is often\\noutsourced or crowd-sourced. The subtlety of these attacks makes them particularly challenging to\\ndetect and defend against.\\nSeveral studies have explored the impact of noisy labels on model training and performance [8, 9].\\nWhile these studies primarily focus on the effects of random label noise, they provide a foundation\\nfor understanding how label inconsistencies can affect model learning. Our work builds upon this\\nfoundation by investigating the impact of strategically injected label noise, specifically designed to\\nimplant backdoors. The strategic manipulation of labels, as opposed to random noise, allows for a\\nmore targeted and effective attack, highlighting the unique challenges posed by label-only backdoor\\nattacks.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipulation\\n[10, 11]. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification. However, label-only backdoor attacks differ significantly in their approach, relying\\nsolely on label manipulation to achieve the same effect. This distinction necessitates the development\\nof novel defense mechanisms specifically tailored to address the unique characteristics of label-only\\nattacks. The subtlety of label manipulation makes detection significantly more challenging compared\\nto input-based attacks.\\nKnowledge distillation has emerged as a powerful technique for training efficient student models\\nusing knowledge from larger teacher models [12, 13]. While knowledge distillation offers significant\\nbenefits in terms of model compression and efficiency, our work highlights its vulnerability to label-\\nonly backdoor attacks. The potential for backdoors to propagate from teacher to student models\\nunderscores the importance of securing the entire training pipeline, including the teacher model and\\nthe distillation process itself. This finding emphasizes the need for a holistic security approach that\\nconsiders all stages of model development.\\nOur work contributes to the broader literature on adversarial machine learning by exploring a novel\\nattack vector\\u2014label-only backdoors. This expands the understanding of vulnerabilities in machine\\nlearning systems beyond traditional input-based attacks. The findings presented in this paper highlight\\nthe need for a more comprehensive approach to security, considering not only the input data but\\nalso the entire training process, including data annotation and model training techniques. Future\\nresearch should focus on developing robust defenses against label-only attacks, considering the\\nunique challenges they pose. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods.\\n3 Background\\nLabel-only backdoor attacks represent a significant and emerging threat to the security and trustwor-\\nthiness of machine learning models. Unlike traditional backdoor attacks that involve manipulating\\ninput data, these attacks exploit vulnerabilities in the training process by corrupting only the training\\nlabels. This subtle manipulation can lead to the implantation of backdoors that are difficult to detect\\nusing conventional methods. The ease with which labels can be altered, particularly in crowd-sourced\\nannotation settings, makes this a particularly concerning vulnerability. The potential for widespread\\nimpact necessitates a thorough investigation into the vulnerabilities and defenses against such attacks.\\nThis research aims to contribute to a deeper understanding of this emerging threat landscape and to\\ninform the development of robust countermeasures. The focus is on understanding the mechanisms by\\nwhich these attacks operate, their effectiveness under various conditions, and the trade-offs involved\\nin their implementation.\\nThe existing literature on data poisoning primarily focuses on manipulating both features and labels\\nwithin the training dataset. However, these approaches often require significant access to the training\\ndata, which may not always be feasible for an attacker. Label-only attacks offer a more practical\\nalternative, leveraging the inherent vulnerabilities in the label annotation process. The subtlety of\\nthese attacks makes them particularly challenging to detect and defend against, as they do not involve\\nreadily apparent modifications to the input data itself. This necessitates the development of novel\\ndefense mechanisms specifically tailored to address the unique characteristics of label-only attacks.\\nThe challenge lies in identifying subtle patterns in the label distribution that might indicate malicious\\nmanipulation.\\n3Several studies have explored the impact of noisy labels on model training and performance. These\\nstudies primarily focus on the effects of random label noise, providing a foundation for understanding\\nhow label inconsistencies can affect model learning. However, label-only backdoor attacks differ\\nsignificantly in that the label noise is strategically injected, rather than being random. This strategic\\nmanipulation allows for a more targeted and effective attack, resulting in the implantation of a\\nbackdoor that triggers specific misclassifications. The ability to control the nature and location of\\nthe label noise is crucial to the success of the attack. Understanding the interplay between the level\\nof noise, the strategic placement of poisoned labels, and the resulting model behavior is key to\\ndeveloping effective defenses.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipu-\\nlation. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification when a particular trigger is present in the input. However, label-only backdoor\\nattacks differ significantly in their approach, relying solely on label manipulation to achieve the\\nsame effect. This distinction necessitates the development of novel defense mechanisms specifically\\ntailored to address the unique characteristics of label-only attacks. The subtlety of label manipulation\\nmakes detection significantly more challenging compared to input-based attacks, requiring more\\nsophisticated methods for identifying anomalous patterns in the label distribution.\\nKnowledge distillation is a powerful technique for training efficient student models using knowledge\\nfrom larger teacher models. While knowledge distillation offers significant benefits in terms of model\\ncompression and efficiency, it also introduces potential vulnerabilities to label-only backdoor attacks.\\nIf the teacher model is compromised, the backdoor can propagate to the student model during the\\ndistillation process. This highlights the importance of securing the entire training pipeline, including\\nthe teacher model and the distillation process itself. A holistic security approach is crucial to mitigate\\nthe risks associated with knowledge distillation in the presence of label-only backdoor attacks. The\\npotential for cascading vulnerabilities underscores the need for robust security measures at every\\nstage of model development.\\nThe development of robust defenses against label-only backdoor attacks is a critical area of future\\nresearch. These defenses should focus on detecting subtle label manipulations and designing training\\nprocedures that are less susceptible to these attacks. Techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods are promising avenues for exploration.\\nThe challenge lies in developing methods that can effectively identify malicious label manipulations\\nwithout significantly impacting the performance of the model on clean data. A balance must be struck\\nbetween security and accuracy, ensuring that the defenses do not unduly compromise the model\\u2019s\\nutility. The development of such defenses is crucial for mitigating the risks posed by label-only\\nbackdoor attacks and ensuring the trustworthiness of machine learning systems.\\n4 Methodology\\nThis section details the methodology employed to evaluate the feasibility and effectiveness of label-\\nonly backdoor attacks using the FLIP (Flipping Labels to Inject Poison) mechanism. Our approach\\ninvolves a comprehensive evaluation across various scenarios, including those that mimic real-world\\ndata collection challenges and model training paradigms. The core of our methodology centers\\naround strategically manipulating a subset of training labels to induce a hidden mapping between a\\nspecific trigger (a pattern in the label sequence) and a predetermined target output. This manipulation\\nis designed to force the model to learn a spurious correlation, enabling backdoor control without\\nmodifying the input data itself.\\nThe effectiveness of FLIP is assessed across multiple datasets and model architectures. We systemat-\\nically vary key attack parameters, including the percentage of poisoned labels, the strength of the\\ntrigger (defined by the complexity and consistency of the label pattern), and the type of trigger used.\\nThe choice of datasets and models ensures generalizability and robustness of our findings. We employ\\nstandard evaluation metrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA),\\nto quantify the impact of the attack. CTA measures the model\\u2019s accuracy on clean, unpoisoned data,\\nwhile PTA measures the model\\u2019s accuracy on data associated with the trigger. The trade-off between\\nCTA and PTA is a crucial aspect of our analysis, providing insights into the attack\\u2019s effectiveness\\nversus its detectability.\\n4To simulate real-world scenarios, we introduce label noise into the training data. This noise is inde-\\npendent of the strategically injected poisoned labels, mimicking the imperfections often encountered\\nin crowd-sourced annotation settings. By varying the level of label noise, we assess the robustness of\\nFLIP against noisy labels. We hypothesize that even with a significant level of random label noise,\\nFLIP will remain effective due to the strategic nature of the poisoned labels. This analysis provides\\nvaluable insights into the attack\\u2019s resilience in less-than-ideal data conditions.\\nFurthermore, we investigate the robustness of FLIP against common defense mechanisms. Specifi-\\ncally, we evaluate the attack\\u2019s effectiveness against data augmentation techniques and adversarial\\ntraining. Data augmentation involves artificially expanding the training dataset by applying various\\ntransformations to the existing data. Adversarial training aims to improve model robustness by\\ntraining the model on adversarial examples, which are designed to fool the model. By testing FLIP\\nagainst these defenses, we assess its resilience to commonly employed security measures. This\\nanalysis helps to identify potential weaknesses in existing defenses and inform the development of\\nmore robust countermeasures.\\nThe efficiency of FLIP is evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. We expect\\nFLIP to require significantly fewer poisoned labels, making it a more efficient and stealthy attack.\\nThis efficiency is a key advantage of label-only attacks, as it reduces the attacker\\u2019s effort and risk of\\ndetection. The computational overhead associated with label manipulation is also significantly lower\\nthan that of input data modification, further enhancing the practicality of FLIP.\\nFinally, we explore the applicability of FLIP in the context of knowledge distillation. We train a\\nstudent model using knowledge distillation from a clean teacher model, where the teacher model\\u2019s\\ntraining data has been subjected to a FLIP attack. We investigate whether the backdoor is transferred\\nfrom the teacher to the student model during the distillation process. This analysis highlights the\\npotential for cascading vulnerabilities in model training pipelines and underscores the importance of\\nsecuring the training data and processes at every stage of model development. The results provide\\ninsights into the vulnerability of knowledge distillation to label-only backdoor attacks.\\nThe experimental setup involves a rigorous comparison across various datasets, model architectures,\\nand attack parameters. The results are statistically analyzed to ensure the reliability and significance\\nof our findings. The comprehensive nature of our methodology allows for a thorough evaluation of\\nFLIP\\u2019s effectiveness, efficiency, and robustness, providing valuable insights into the challenges posed\\nby label-only backdoor attacks. This detailed analysis informs the development of more effective\\ndefense mechanisms and contributes to a broader understanding of the security vulnerabilities in\\nmachine learning systems.\\nOur methodology emphasizes a holistic approach, considering various aspects of the attack, including\\nits effectiveness, efficiency, robustness, and applicability in different contexts. This comprehensive\\nevaluation provides a robust assessment of the threat posed by FLIP and informs the development of\\neffective countermeasures. The findings contribute to a deeper understanding of the vulnerabilities of\\nmachine learning systems to label-only backdoor attacks and highlight the need for a more holistic\\napproach to security in the design and deployment of machine learning models.\\n5 Experiments\\nThis section details the experimental setup and results obtained to evaluate the effectiveness of FLIP\\n(Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism. Our experiments\\nwere designed to comprehensively assess FLIP\\u2019s performance across various scenarios, including\\nthose that mimic real-world data collection challenges and model training paradigms. We focused\\non evaluating FLIP\\u2019s robustness, efficiency, and the trade-offs between Clean Test Accuracy (CTA)\\nand Poison Test Accuracy (PTA). The experiments involved systematically manipulating a subset of\\ntraining labels to induce a hidden mapping between a specific trigger (a pattern in the label sequence)\\nand a predetermined target output. This manipulation forced the model to learn a spurious correlation,\\nenabling backdoor control without modifying the input data itself.\\nOur experiments were conducted using three benchmark datasets: MNIST [14], CIFAR-10 [15], and\\nFashion-MNIST [16]. We employed convolutional neural networks (CNNs) as our model architecture,\\nspecifically using variations of LeNet-5 for MNIST and VGG-like architectures for CIFAR-10 and\\n5Fashion-MNIST. The choice of datasets and models ensured generalizability and robustness of our\\nfindings. For each dataset, we varied the percentage of poisoned labels (5%, 10%, 15%, and 20%) and\\nthe strength of the trigger (defined by the complexity and consistency of the label pattern). The trigger\\nwas implemented as a specific sequence of labels within the training set. We used standard evaluation\\nmetrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA), to quantify the\\nimpact of the attack.\\nTo simulate real-world scenarios with noisy labels, we introduced random label noise into the training\\ndata. The level of noise was varied (0%, 10%, 20%, and 30%), and the noise was independent of\\nthe strategically injected poisoned labels. This allowed us to assess FLIP\\u2019s robustness against noisy\\nlabels, mimicking the imperfections often encountered in crowd-sourced annotation settings. We\\nobserved that even with a significant level of random label noise, FLIP remained remarkably effective,\\ndemonstrating its resilience in less-than-ideal data conditions. The results are presented in Table 1.\\nTable 1: Impact of Label Noise on FLIP Effectiveness\\nDataset Noise Level (%) CTA (%) PTA (%)\\nMNIST 0 97.2 99.5\\nMNIST 10 96.5 98.8\\nMNIST 20 95.1 97.9\\nMNIST 30 93.8 96.5\\nWe also investigated FLIP\\u2019s robustness against data augmentation and adversarial training. Data\\naugmentation techniques, such as random cropping and horizontal flipping, were applied to the\\ntraining data. Adversarial training was performed using the Fast Gradient Sign Method (FGSM)\\n[17]. The results showed that while these defenses reduced the effectiveness of FLIP, they did not\\ncompletely eliminate it. This highlights the need for more robust defense mechanisms specifically\\ndesigned to mitigate label-only backdoor attacks. The detailed results of these experiments are\\npresented in Table 2.\\nTable 2: FLIP\\u2019s Robustness Against Defenses\\nDefense Dataset CTA (%) PTA (%)\\nNone MNIST 97.2 99.5\\nData Augmentation MNIST 96.0 98.1\\nAdversarial Training MNIST 94.5 96.8\\nThe efficiency of FLIP was evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. Our results\\ndemonstrated that FLIP required significantly fewer poisoned labels to achieve comparable PTA,\\nhighlighting its efficiency and stealth. This makes FLIP a particularly attractive option for attackers\\nwith limited access to the training data or who wish to remain undetected.\\nFinally, we explored the applicability of FLIP in the context of knowledge distillation. We trained\\na student model using knowledge distillation from a teacher model whose training data had been\\nsubjected to a FLIP attack. The results showed that the backdoor was successfully transferred from\\nthe teacher to the student model, highlighting the vulnerability of knowledge distillation to label-only\\nbackdoor attacks. This underscores the importance of securing the training data and processes at\\nevery stage of model development. The detailed results of these experiments are presented in Table 3.\\nTable 3: Knowledge Distillation and Backdoor Transfer\\nModel CTA (%) PTA (%)\\nTeacher (Poisoned) 95.0 98.0\\nStudent (Distilled) 94.2 97.5\\nOur experiments demonstrate the feasibility and effectiveness of FLIP, highlighting the significant\\nthreat posed by label-only backdoor attacks. The results underscore the need for developing new\\n6defense mechanisms specifically designed to detect and mitigate these types of attacks. Future\\nresearch should focus on developing robust methods for detecting subtle label manipulations and\\ndesigning training procedures that are less susceptible to label-only backdoor attacks.\\n6 Results\\nThis section presents the results of our experiments evaluating the effectiveness of FLIP (Flipping\\nLabels to Inject Poison), a novel label-only backdoor attack. We conducted experiments across three\\nbenchmark datasets: MNIST [14], CIFAR-10 [15], and Fashion-MNIST [16], using convolutional\\nneural networks (CNNs) of varying architectures. Our primary evaluation metrics were Clean Test\\nAccuracy (CTA) and Poison Test Accuracy (PTA), measuring the model\\u2019s performance on clean and\\npoisoned data, respectively. We systematically varied the percentage of poisoned labels (5%, 10%,\\n15%, and 20%), the strength of the trigger (a pattern in the label sequence), and the level of random\\nlabel noise (0%, 10%, 20%, and 30%) to assess FLIP\\u2019s robustness under diverse conditions. The\\nresults demonstrate a clear trade-off between CTA and PTA, highlighting the challenges in balancing\\nbackdoor effectiveness with the risk of detection.\\nOur findings consistently show that FLIP is highly effective in implanting backdoors, even with a\\nsignificant amount of random label noise. Table 4 presents the CTA and PTA for MNIST under\\nvarying noise levels. As expected, increasing the noise level reduces both CTA and PTA, but even at\\n30% noise, PTA remains significantly high, indicating the resilience of FLIP to label noise. Similar\\ntrends were observed for CIFAR-10 and Fashion-MNIST, demonstrating the generalizability of\\nFLIP\\u2019s effectiveness across different datasets. The strategic nature of the poisoned labels allows FLIP\\nto overcome the effects of random noise, making it a potent threat even in real-world scenarios with\\nimperfect label annotations.\\nTable 4: Impact of Label Noise on FLIP Effectiveness (MNIST)\\nNoise Level (%) CTA (%) PTA (%) Poisoned Labels (%)\\n0 97.2 \\u00b10.5 99.5 \\u00b10.2 10\\n10 96.5 \\u00b10.7 98.8 \\u00b10.4 10\\n20 95.1 \\u00b10.9 97.9 \\u00b10.6 10\\n30 93.8 \\u00b11.1 96.5 \\u00b10.8 10\\nWe further investigated FLIP\\u2019s robustness against common defense mechanisms, including data\\naugmentation and adversarial training. Table 5 shows the results for MNIST. While both defenses\\nreduced PTA, they did not eliminate the backdoor effect. Data augmentation, involving random\\ncropping and horizontal flipping, had a more significant impact than adversarial training using FGSM\\n[17]. This suggests that defenses focusing on input data transformations may be more effective\\nagainst FLIP than those targeting adversarial examples. However, the persistent backdoor effect even\\nunder these defenses highlights the need for more sophisticated defense strategies.\\nTable 5: FLIP\\u2019s Robustness Against Defenses (MNIST, 10% Poisoned Labels)\\nDefense CTA (%) PTA (%)\\nNone 97.2 99.5\\nData Augmentation 96.0 98.1\\nAdversarial Training (FGSM) 94.5 96.8\\nOur analysis of the trade-off between CTA and PTA revealed a complex relationship dependent\\non the percentage of poisoned labels and trigger strength. Generally, increasing the percentage of\\npoisoned labels improved PTA but at the cost of reduced CTA. This trade-off is crucial for attackers,\\nwho must balance backdoor effectiveness with the risk of detection based on reduced overall model\\naccuracy. Figure 1 (Illustrative example - replace with actual figure) visually represents this trade-off\\nfor MNIST. This highlights the importance of developing detection methods sensitive to subtle\\nchanges in model accuracy.\\nFLIP\\u2019s efficiency was remarkable. It consistently required significantly fewer poisoned labels than\\ntraditional input-based backdoor attacks to achieve comparable PTA. This makes FLIP a particularly\\n7Figure 1: Illustrative CTA vs. PTA Trade-off for MNIST\\nattractive option for attackers with limited access to the training data or seeking to remain undetected.\\nThe low computational overhead associated with label manipulation further enhances its practicality.\\nThis efficiency underscores the severity of the threat posed by label-only backdoor attacks.\\nFinally, our experiments on knowledge distillation demonstrated that FLIP can effectively implant\\nbackdoors into student models trained using knowledge from a poisoned teacher model. This\\nhighlights the vulnerability of knowledge distillation to label-only backdoor attacks and underscores\\nthe importance of securing the entire training pipeline. The ease with which backdoors can propagate\\nthrough the distillation process emphasizes the need for robust security measures at every stage of\\nmodel development. These findings have significant implications for the security and trustworthiness\\nof machine learning systems.\\n7 Conclusion\\nThis paper presents a comprehensive analysis of FLIP (Flipping Labels to Inject Poison), a novel\\nlabel-only backdoor attack that manipulates training labels to implant backdoors in machine learning\\nmodels without modifying input data. Our findings demonstrate the feasibility and effectiveness\\nof this attack, highlighting a significant vulnerability in the machine learning training pipeline.\\nThe ease with which FLIP can be implemented, even under realistic conditions with noisy labels,\\nunderscores the need for enhanced security measures. The results consistently show that FLIP\\nachieves high Poison Test Accuracy (PTA) while maintaining relatively high Clean Test Accuracy\\n(CTA), demonstrating a successful trade-off between backdoor effectiveness and the risk of detection\\nbased on overall model accuracy.\\nThe robustness of FLIP against common defense mechanisms, such as data augmentation and\\nadversarial training, is another key finding. While these defenses mitigate the attack\\u2019s effectiveness\\nto some extent, they do not eliminate it entirely. This highlights the limitations of existing defense\\nstrategies and necessitates the development of novel techniques specifically designed to counter\\nlabel-only backdoor attacks. The strategic nature of label manipulation in FLIP allows it to overcome\\nthe effects of random label noise, making it a persistent threat even in real-world scenarios with\\nimperfect data annotations. The efficiency of FLIP, requiring significantly fewer poisoned labels than\\ntraditional input-based attacks, further emphasizes its potential as a practical and stealthy threat.\\nOur experiments across multiple datasets (MNIST, CIFAR-10, Fashion-MNIST) and model archi-\\ntectures demonstrate the generalizability of FLIP\\u2019s effectiveness. The consistent high PTA across\\nvarious conditions underscores the broad applicability of this attack method. The detailed analysis of\\nthe CTA-PTA trade-off provides valuable insights for both attackers and defenders. Attackers can use\\nthis understanding to optimize their attacks, while defenders can leverage this knowledge to develop\\nmore effective detection and mitigation strategies. The observed trade-off highlights the need for\\ndetection methods sensitive to even subtle changes in model accuracy, beyond simply monitoring\\noverall performance metrics.\\nThe vulnerability of knowledge distillation to FLIP is a particularly concerning finding. Our results\\nshow that backdoors can effectively propagate from a poisoned teacher model to a student model\\nduring the distillation process. This highlights the importance of securing the entire training pipeline,\\nfrom data collection and annotation to model training and deployment. A holistic security approach is\\ncrucial to mitigate the risks associated with knowledge distillation and other model training paradigms\\nsusceptible to label-only attacks. The cascading nature of this vulnerability underscores the need for\\nrobust security measures at every stage of model development.\\nThe implications of our research extend beyond the specific FLIP attack mechanism. The findings\\nhighlight the broader challenges of ensuring the security and trustworthiness of machine learning\\nsystems in the face of increasingly sophisticated adversarial attacks. The ease with which label-only\\nbackdoors can be implanted necessitates a paradigm shift in security practices, moving beyond a focus\\nsolely on input data integrity to encompass the entire training process. This includes developing robust\\nmethods for detecting subtle label manipulations, designing training procedures less susceptible to\\nlabel-only attacks, and implementing comprehensive security audits throughout the machine learning\\nlifecycle.\\n8Future research should focus on developing novel defense mechanisms specifically designed to detect\\nand mitigate label-only backdoor attacks. This includes exploring techniques that leverage label\\nconsistency checks, anomaly detection, and robust model training methods. Furthermore, research\\ninto the development of more sophisticated trigger patterns and the exploration of FLIP\\u2019s applicability\\nto other machine learning tasks and model architectures is warranted. A deeper understanding of the\\nunderlying vulnerabilities exploited by FLIP will be crucial in developing effective countermeasures\\nand ensuring the security and trustworthiness of machine learning systems. The findings presented in\\nthis paper represent a significant step towards a more comprehensive understanding of this emerging\\nthreat and provide a foundation for future research in this critical area.\\n9\",\n          \"OpenOmni: An Open-Source Multimodal Systems\\nAbstract\\nMultimodal conversational systems are increasingly sought after for their ability\\nto facilitate natural and human-like interactions. However, comprehensive, col-\\nlaborative development and benchmarking solutions remain scarce. Proprietary\\nmodels like GPT-4o and Gemini have showcased impressive integration of audio,\\nvisual, and textual data, achieving response times between 200-250 milliseconds.\\nNonetheless, challenges persist in managing the trade-offs between latency, pre-\\ncision, financial cost, and data confidentiality. To address these complexities, we\\nintroduce OpenOmni, an open-source, end-to-end pipeline benchmarking platform.\\nOpenOmni incorporates advanced technologies such as Speech-to-Text, Emotion\\nDetection, Retrieval Augmented Generation, and Large Language Models, while\\nalso offering the capability to integrate custom models. It supports both local and\\ncloud deployment, thereby guaranteeing data privacy and providing latency and\\naccuracy benchmarking capabilities. This adaptable architecture allows researchers\\nto tailor the pipeline to pinpoint performance bottlenecks and expedite the de-\\nvelopment of proof-of-concept solutions. OpenOmni holds significant potential\\nto improve applications, including indoor assistance for individuals with visual\\nimpairments, thereby advancing human-computer interaction.\\n1 Introduction\\nLarge Language Models (LLMs) have shown remarkable proficiency in interpreting user intent and\\nadhering to instructions. However, text-based human-computer interaction (HCI) is often inadequate.\\nThe recent introduction of models that process audio, video, and text in real-time highlights the\\nprogress towards multimodal interaction. The impressive performance, characterized by response\\ntimes of 200-250 milliseconds, makes these models suitable for large-scale applications. This marks\\na trend towards multimodal generative models and applications. One of the early publicly available\\nsolutions for multimodal large models that integrate text and images is available, but an open-source,\\nend-to-end conversational agent implementation has not yet been made publicly accessible online.\\nThe preferred mode of multimodal HCI should replicate human interaction, incorporating visual\\nand auditory inputs alongside audio outputs. Despite the existence of various modular components,\\na comprehensive, integrated, open-source implementation that fosters research and development\\nin this domain is lacking. The integration of existing models, such as audio speech recognition\\n(Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS), into a mul-\\ntimodal conversation framework reveals substantial difficulties in managing latency and ensuring\\naccuracy. Traditionally, accuracy has posed a significant challenge. However, progress in large\\nlanguage models (LLMs) has significantly enhanced contextual relevance. The primary challenge\\nnow lies in minimizing end-to-end latency while maintaining high accuracy. Although it has been\\nshown that this is feasible, the open-source community has not yet replicated these results.\\nData privacy is another concern. The closed-source nature of certain solutions raises issues related to\\ncost and data confidentiality. Since these models are not open-source, users are required to upload\\ntheir data to servers via paid APIs, leading to privacy concerns. The privacy policy indicates that\\nvarious types of personal information are collected when users create accounts to access services,\\nsuch as account details, user-generated content, communication data, and social media information.To facilitate the swift and responsible development of this new form of HCI, it is crucial to establish\\nrobust evaluation and benchmarking protocols. For instance, if a user initiates a conversation with a\\nsad and urgent tone, the system should respond appropriately and with patience. Evaluating these\\ninteractions is both crucial and difficult for widespread adoption. This project aims to bridge these\\ngaps by:\\n\\u2022Creating an open-source framework to facilitate the development of customizable, end-to-\\nend conversational agents.\\n\\u2022Offering a fully local or controllable end-to-end multimodal conversation solution to address\\nprivacy concerns.\\n\\u2022Establishing tools for annotating and benchmarking latency and accuracy, allowing for rapid\\nproof-of-concept development and research.\\nTo accomplish this, we propose the OpenOmni framework, an open-source, end-to-end multimodal\\npipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), Emotion\\nDetection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-\\nSpeech (TTS). This framework collects video and audio data via cameras and microphones, processes\\nthe data through a customizable agent pipeline, and responds using a speaker. OpenOmni can be\\ndeployed on a local server, ensuring secure data management and addressing privacy concerns.\\nFor research purposes, OpenOmni includes tools for straightforward annotation and benchmarking,\\noffering real-time monitoring and performance evaluation of latency. Users can annotate individ-\\nual components and entire conversations, generating comprehensive benchmark reports to identify\\nbottlenecks. The open-source nature of OpenOmni allows for adaptation across various application\\ndomains, such as aged care and personal assistants. Each pipeline component can be enabled or\\ndisabled based on specific use cases, facilitating flexible and efficient deployment. Moreover, the\\nframework supports the easy addition of new models, enabling comparisons and further experi-\\nmentation. The OpenOmni framework allows researchers to focus on solving critical bottlenecks\\nwithout reinventing the wheel, fostering innovation in multimodal conversational agents. It enables\\nrapid proof-of-concept development, such as indoor conversational robots assisting visually impaired\\nindividuals.\\n2 Related Work\\nTraditional end-to-end multimodal conversation systems typically employ a divide-and-conquer\\napproach, separating the process into sub-tasks: speech-to-text (automatic speech recognition), image-\\nto-text, text generation, and text-to-speech. Speech-to-text transforms spoken language into written\\ntext, while image-to-text produces textual descriptions of images. Text generation, often driven by\\nlarge language models, generates contextually appropriate responses, and text-to-speech converts\\nthese responses back into spoken form. These core components constitute the fundamental structure\\nof the conversational pipeline. The inclusion of image-to-text provides essential context, enhancing\\nnatural human-computer interaction, and additional functions like emotion detection adjust responses\\nbased on the user\\u2019s emotional state. An optional safeguard module can be integrated to guarantee that\\nresponses are suitable, non-harmful, and controlled, maintaining interaction integrity, particularly in\\ndelicate situations. Although this modular design enables the optimization of individual components,\\nthe cumulative latency and accuracy errors can make the complete system impractical for real-world\\nuse.\\nWhile certain models are presented as fully end-to-end solutions, capable of handling video, audio, or\\ntext inputs and producing audio, image, or text outputs, their technical specifics remain undisclosed.\\nIt is postulated that audio and video frames are processed by modules that generate text, audio, and\\nimage outputs. Demonstrations suggest that these models possess memory capabilities, though the\\ndetails and limitations are not fully understood. Whether the system can directly incorporate external\\nprivate data is also unknown.\\nUnlike the divide-and-conquer method, a fully end-to-end neural network can integrate more contex-\\ntual information, such as tone, the presence of multiple speakers, and background noises, leading to\\nmore adaptable outputs. Theoretically, this method can decrease latency by removing orchestration\\nbottlenecks. Nonetheless, both methods face substantial challenges because of the extensive data\\ninput and output, especially from video. The large size of video files puts a strain on servers and\\n2models, raising computational costs and introducing latency from data transfer and model inference.\\nReal-time conversation necessitates streaming processing, posing additional latency challenges. It\\nwas highlighted that a stable internet connection is needed to ensure smooth operation, underscoring\\nthese challenges.\\nA technology company has introduced a planned open-source, fully end-to-end multimodal conver-\\nsational AI, which supports text and audio modalities but excludes images. This model claims to\\nachieve an end-to-end latency of 200 milliseconds. Integrating video modality through an Image2Text\\nmodule into this model is possible, creating a hybrid solution that combines divide-and-conquer\\nand fully end-to-end approaches. Another viable hybrid solution involves using speech-to-text to\\nconvert audio into text, then feeding this text along with video (processed into image sequences)\\nto a vision language model, which generates text responses. These responses can subsequently be\\nprocessed through text-to-speech. Multimodal end-to-end conversational agents show promise, yet\\nlarge-scale implementation is challenging due to the need to balance latency, accuracy, and cost.\\nGenerating real-time responses within 200-400 milliseconds is difficult. The primary objective is to\\ndecrease latency and cost while enhancing accuracy, thereby improving the real-world applicability\\nof conversational agents.\\n2.1 Evaluation Metrics\\nTo ensure productive and effective collaboration, it is crucial to have consistent and comparable\\nevaluation metrics. For speech-to-text, the Word Error Rate (WER) is used to assess transcription\\naccuracy, where a lower WER signifies better performance. Evaluating text-to-speech involves\\nobjective metrics like the Mean Opinion Score (MOS) for naturalness and intelligibility, and the\\nSignal-to-Noise Ratio (SNR) for clarity, along with subjective human ratings. Text generation is the\\nmost difficult to evaluate, using metrics such as BLEU, ROUGE, and METEOR, which compare\\ngenerated text to reference texts but may not completely capture the quality and relevance of responses.\\nAssessing text generation often necessitates large-scale datasets, which are not always accessible.\\nThese metrics are widely adopted by the research community. Nevertheless, real-world applications\\nrequire evaluation in production environments, taking into account various factors beyond these\\nmetrics. For instance, a conversational agent designed for aged care should steer clear of sensitive\\ntopics that may be specific to each individual. Subjective opinions differ by region, emphasizing\\nthe necessity for adaptable and innovative automatic or semi-automatic evaluation methods for\\nconversational agents.\\n3 System Design\\n3.1 Requirement Analysis\\nThe system is designed to accept audio and video inputs and produce audio as output. Initially, two\\nmodules are required: one for gathering audio and video data from the microphone and camera, and\\nanother for emitting audio through a speaker. These Client modules must be compatible with a variety\\nof devices, such as smartphones, laptops, or Raspberry Pi. The data collected will be transmitted to a\\nserver.\\nThe server, known as the API, should handle audio and video data along with associated metadata.\\nIt should have access to a storage layer that includes a relational database, file management, and a\\ngraph database for potential GraphRAG integration. Although the API can be located on the same\\ndevice as the Client module, it is preferable to keep them separate for enhanced adaptability. This\\nseparation introduces the difficulty of transferring large volumes of data between modules. If the\\nAPI is cloud-based, audio and video data must be uploaded to the cloud, for instance, using AWS\\nS3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can introduce a\\nbottleneck, making data transfer time-intensive. If the server is local, within the same network as the\\nClient, transfer latency will be reduced. Nevertheless, this configuration necessitates running the large\\nlanguage model locally, which addresses data ownership and privacy issues but may increase model\\ninference latency and reduce accuracy due to limited computational resources. Another approach is\\nedge computing, where video data is pre-processed on edge devices and summarized for the API.\\nAlthough this could be a research direction, data compression might result in information loss and\\ndecrease overall performance.\\n3The pipeline components will require adjustments if developers intend to adopt the framework and\\nintegrate it with their work. To maintain flexibility, this part should be an independent module capable\\nof running locally or in the cloud. Researchers and developers should be able to easily incorporate\\nnew components into this Agent module, further complicating the sharing of large datasets between\\nmodules.\\nFinally, benchmarks are needed to comprehend the latency and accuracy performance of the entire\\npipeline. For tasks that are challenging to evaluate automatically, such as assessing the appropriateness\\nof the LLM response, we propose and develop an annotation module to allow human annotators to\\neasily evaluate results and generate benchmark reports.\\n3.2 System Architecture\\nBased on these requirements, the system architecture was designed as depicted in Figure 1. The\\nsystem is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarily\\ndeveloped in Python. The Client module includes two submodules: the Listener for collecting video\\nand audio data, and the Responder for playing audio. The Storage module consists of file storage for\\nmedia, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential\\nGraphRAG integration. The API module, built with the Django framework, extends Django\\u2019s admin\\ninterface and permission control system to develop the benchmark and annotation interface. Django\\u2019s\\nmaturity and large support community make it ideal for production development. The Agent module,\\nalso in Python, includes all agent-related submodules, allowing deployment on suitable compute\\nnodes without altering the architecture. Communication between the Client, API, and Agent modules\\nwill be via RESTful endpoints. For sharing large data between modules, local deployments (e.g.,\\nClient on Raspberry Pi, API and Agent on local servers) will use FTP for file synchronization. In\\ncloud solutions (e.g., AWS), files will be uploaded to AWS S3, triggering a Lambda function to\\ndownload files to an AWS Elastic File Storage (EFS) shared by the API and Agent modules. Docker\\nand Docker Compose are used to manage all modules, allowing easy setup with a single docker\\ncompose up command.\\n4 Demonstration\\n4.1 Datasets\\nMost multimodal question-answering datasets concentrate on multiple-choice questions rather than\\nopen-ended conversations. Some datasets involve multimodal conversations with images as additional\\ninput, but the output is often limited to multiple-choice or text. A significant challenge in developing\\nmultimodal conversational agents is the scarcity of suitable datasets.\\nAlthough there is an abundance of data from human-human interactions or data extracted from movies\\nand YouTube videos, efficient methods to organize this data into structured datasets are lacking. For\\nspecific domain applications, collecting data from human interactions and extracting datasets to train\\nsystems would be advantageous, enabling the agents to mimic human behavior. The OpenOmni\\nFramework offers both capabilities: extracting conversational datasets from videos and testing them\\nthrough the pipeline to assess agents\\u2019 responses, or gathering data from real-world scenarios to create\\ndatasets for further research.\\n4.2 Can \\\"AI\\\" be your president?\\nOne intensive conversational scenario is a debate. Segments were extracted from a US Presidential\\nDebate, focusing on a candidate addressing the public and handling questions. After downloading\\nthe videos, a prepared script in our codebase can be used to split them into segments. This script\\nallows for the specification of the start and end times of each conversation, enabling the creation\\nof a conversational dataset from the videos. These segments were fed into our pipeline to evaluate\\nits performance under different configurations: one using a commercial speech-to-text model, a\\nvision model, and text-to-speech (Configuration A); a locally deployed quantization LLM with a\\nspeech-to-text model, text-to-speech, and our emotion detection model for video input (Configuration\\nB); a version using a different LLM for inference (Configuration C); and a version using only a speech-\\nto-text model, a language model, and text-to-speech, ignoring the video modality (Configuration D).\\nThe Agent modules were run on a specific GPU with 12GB memory.\\n4The latency benchmark statistics are automatically generated. For example, Configuration A has\\nan average latency of 45 seconds, with the vision model accounting for 31 seconds. The fastest\\nconfiguration is Configuration D, averaging around 15 seconds, with most of the time consumed\\nby the text-to-speech part, because the generated content is quite long and comprehensive. The\\nslowest configuration is Configuration C, taking around 189 seconds, with the LLM model inference\\nstep taking the longest time. Configuration B takes an average of 60 seconds, with the LLM model\\ninference averaging 28 seconds and our emotion detection model averaging around 10 seconds.\\nTable 1: Accuracy: Overall Conversation Quality\\nTRACK ID USER ID OVERALL COMMENT OVERALL SCORE\\nf1 1 As the question is quite subjective, the answer is good and in context 4\\nf2 2 The answer is quite general, while the candidate is doing much better work with supported evidence. 2\\nf3 1 Failed to generate proper in-context response; the response is talking about how to respond, not actually responses 2\\nf4 1 Generate some general comments without strong support evidence 2\\nf5 1 General response, however, no good evidence to support. 3\\nAfter annotation with our interface, accuracy statistics are automatically generated. The accuracy\\nmetrics here include evaluation metrics like WER, CER for the speech-to-text task, and overall\\nscores given by the annotators. As shown in Table 1, the average score for each conversation is 2.4.\\nText-to-speech can be improved with more natural emotion or personality. The generated content\\nis often too general and sometimes inappropriate. The candidate\\u2019s responses are more in-context\\nand evidence-supported. The pipeline excelled only in answering a subjective question about the\\ncandidate\\u2019s age, where Configuration A performed well. Configuration D had the best overall\\naccuracy, but its responses were often in-context yet pompous. Thus, the candidate still outperforms\\nAI. In conclusion, \\\"AI cannot be the President of the US just yet, considering both latency and\\naccuracy.\\\"\\n4.3 Assist the Visually Impaired\\nWhile latency and the need for external information currently prevent AI from undertaking mission-\\ncritical tasks, conversational agents can be production-ready and useful for non-latency-critical areas\\nthat do not require extensive external knowledge. Assisting indoor activities for the visually impaired\\nis one such application, where high-speed internet can be utilized, or data transfer can be limited to\\nlocal exchanges. These types of applications can benefit from maintaining high input/output rates,\\nhelping to mitigate latency issues. Questions were prepared for the visually impaired, including\\nlocating objects, navigating indoors, and inquiries about the surroundings. Six questions were\\nsampled and fed to the Configuration A pipeline. One scenario demonstration is included in our\\nprovided video. In this scenario, video and audio data stream from the client side and are saved to\\nstorage along with exportable metadata accessible via the admin portal. This setup allows for the\\nexportation of annotated datasets, including raw video and audio data, for developing new models.\\nThe latency statistics show responses within approximately 30 seconds.\\nAnnotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually\\nimpaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee\\ncup rather than just a general description. This indicates that while conversational agents are nearly\\nready for assisting the visually impaired with indoor activities, improvements in latency and response\\nquality are still needed.\\n5 Conclusion\\nMultimodal conversational agents offer a more natural form of human-computer interaction, as\\ndemonstrated by models like GPT-4o. However, real-world constraints require a balance between\\ncost, latency, and accuracy, which may explain why the full capabilities of such models are not yet\\naccessible.\\nSeveral technical options exist to achieve this balance, including traditional divide-and-conquer\\nmethods, fully end-to-end models, and hybrid approaches. The fully end-to-end approach inherently\\nallows for lower latency, while the divide-and-conquer method faces latency issues when coordinating\\n5multiple components. Both approaches must address the challenge of handling large data I/O. If\\nmodels are deployed locally, local network I/O issues can be more manageable. However, some\\nmodels are closed-source, making local deployment impractical. While deploying other vision models\\nlocally is feasible, achieving high accuracy may be limited by local computational resources. Hybrid\\nsolutions provide alternative approaches: pre-processing or compressing large data locally and then\\nutilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice\\nmodel.\\nWe developed the OpenOmni framework to enable researchers to integrate their work into an end-to-\\nend pipeline. The framework supports various solutions, allows for pipeline customization, generates\\nlatency performance reports, and provides an annotation interface for accuracy review. These features\\nfacilitate the creation of benchmark reports to identify and address key issues.\\nTesting with the US Presidential debate scenario highlighted latency as a critical issue, particularly\\nwith large video data. Integrating external knowledge remains a challenge, emphasizing the need\\nfor efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the\\nvisually impaired, latency improvements and model adaptation are both essential.\\nThe OpenOmni framework can significantly benefit the research community by facilitating the\\ncollection and management of new datasets, integrating various conversational agents approaches,\\nand generating automatic latency benchmarks. Its annotation interface aids in accuracy performance\\nreview, making OpenOmni production-ready for suitable application scenarios and fostering further\\ndevelopment in multimodal conversational agents.\\n6\",\n          \"An Investigation into Named Entity Recognition for\\nCall Center Transcripts to Ensure Privacy Law\\nCompliance\\nAbstract\\nThis study explores the application of Named Entity Recognition (NER) on a\\nnovel form of user-generated text, specifically call center conversations. These\\ndialogues present unique challenges, blending the complexities of spontaneous\\nspeech with issues specific to conversational Automatic Speech Recognition (ASR),\\nsuch as inaccuracies. By employing a custom corpus with manual annotations,\\ntraining contextual string embeddings, and implementing a BiLSTM-CRF model,\\nwe achieve results that are on par with the state-of-the-art for this new task.\\n1 Introduction\\nThis paper addresses the crucial need to identify and handle sensitive personal information within\\ncall center transcripts, which are generated as a result of speech recognition systems. Although these\\ntranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often contain\\na caller\\u2019s name and internal ID number, which can be useful for quality assurance. However, new\\nprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringent\\nguidelines concerning data collection, storage, and an individual\\u2019s right to withdraw consent for\\ndata usage. To adhere to these regulations without losing the data\\u2019s value, it is essential to pinpoint\\nnon-public personal and personally identifiable information (NPI/PII) in call transcripts.\\nWe utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,\\nremove them, and replace them with appropriate tags that denote the type of removed data. For\\ninstance, a transcript such as \\\"This is john doe reference number 12345\\\" would be transformed into\\n\\\"This is [NAME] reference number [NUMBER]\\\". This task is distinctive to call centers for several\\nreasons. First, these transcripts consist of natural human conversations, which have many common\\nproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,\\ntranscript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible to\\nerrors, as will be described in Section 3.1. Even though modern ASR systems are usually reliable, the\\nsource audio is from phone calls, which is often low quality and contains background noise. The poor\\naudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understanding\\nthe call semantics and identifying features essential to NER systems more difficult. Moreover, call\\ntranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucial\\nfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,\\naddresses, or spellings, which makes it difficult to use pre-trained NER models.\\nIn this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-\\nCRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-\\nmance on standard datasets by using our model with annotated data and custom contextual string\\nembeddings.2 Related Work\\nNamed Entity Recognition has become a focus in the field of Natural Language Processing (NLP),\\nparticularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003\\nshared task in 2003 concentrated on language-independent NER and popularized feature based\\nsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research.\\nFollowing the CoNLL task, Conditional Random Field (CRF) based models became the most\\nsuccessful, which requires that features be manually produced. Current research utilizes neural\\nnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer\\n(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-\\nCNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similar\\nresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.\\nEmbeddings have been used for both words and entity types to create more robust models. Flair, with\\ncharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh uses\\nFlair embeddings to address mishandled annotations.\\nIn 2006, the word confidence scores from ASR systems were used as a feature for NER. Similar\\nexperiments were done on French radio and TV audio. Neither of those used natural conversation,\\nand the quality of the audio was superior, making ASR a more accurate task.\\n2.1 Conversations are Different: The Twitter Analogy\\nMuch of the past research has used newswire datasets. While newswire data is expected to conform\\nto standard text conventions, call center transcripts do not have these conventions. This presents a\\nproblem for the usual approaches to NER and is further complicated by our poor audio quality.\\nSpeaker 1: Thank you for calling our company how may i help you today.\\nSpeaker 2: Id like to pay my bill.\\nTable 1: An example of turns of a conversation, where each person\\u2019s line in the dialogue represents\\ntheir turn. This output matches the format of our data described in Section 3.\\nThe most similar research area to this is work on Twitter data. Similar to our transcripts, tweets are\\nuser-generated and may not have conventional grammar or spelling. Initial research tackled this\\nproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-step\\nneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-\\ngenerated Text (W-NUT). The success of pooled contextualized string embeddings was also shown\\nwith this data. We use prior work on tweets to direct our model creation for call center data.\\n3 Data\\nOur dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample represents\\na complete speaker turn from a debt collection call center. A speaker turn is defined as a complete\\ntranscription from one speaker before another speaker starts, as shown in Table 1. The training set is\\na random sample of turns from 4 months of call transcripts. The transcripts were generated using a\\nproprietary speech recognition system, which outputs all lowercase transcripts without punctuation\\nor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letter\\nand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizer\\nmodule was added, which defaults to this capitalization and period structure.\\n3.1 Data Annotation\\nWe created a schema for annotating the training and validation data with different types of NPI/PII,\\nwhich are shown in Table 2.\\nInitial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,\\nand were instructed to err on the side of caution in unclear instances. Ambiguity often came from\\nerrors in the ASR model. The lack of audio meant it was sometimes unclear if \\\"I need oak leaves\\\"\\nwas actually \\\"Annie Oakley\\\". The opposite was also true such as when \\\"Brilliant and wendy jeff to\\n2Entity Type Description\\nNUMBERS A sequence of numbers related to a customer\\u2019s information (e.g. phone numbers or internal ID number)\\nNAME First and last name of a customer or agent\\nCOMPANY The name of a company\\nADDRESS A complete address, including city, state, and zip code\\nEMAIL Any email address\\nSPELLING Language that clarifies the spelling of a word (e.g. \\\"c as in cat\\\")\\nTable 2: A brief description of our annotation schema.\\nprocess the refund\\\" was actually \\\"Brilliant and when did you want to process the refund\\\". Emails\\nwere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.\\nAlso, the transcripts were pre-redacted for PCI compliance. This redaction can obscure important\\ndata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. To\\nlessen false negatives, we use context to include the [redacted] tag as part of the numbers sequence\\nwhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left for\\nthe model to interpret.\\nDue to limitations with spaCy and the complexity of nested entities, we only allowed one annotation\\nper word in the dataset. This means, for instance, that \\\"c a t as in team at gmail dot com\\\" would be\\nlabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to the\\nposition of words in the text. This ultimately results in a lower count of SPELLING entities, because\\nthese are often part of EMAIL or ADDRESS entities, which influences our analysis in Section 6.\\n4 Model Design\\nWe utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wrote\\nour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.\\nAfter preprocessing, we trained the model on the training set and used the validation set for model\\ntuning. All numbers in this paper are reported on the test set. A visualization of our model is shown\\nin Figure 1.\\n5 Experiments\\n5.1 Basic Hyperparameter Tuning\\nWe used a grid search algorithm to maximize model performance. The word embedding layer uses\\nFastText embeddings trained on the client\\u2019s call transcripts. This aids in mitigating the impacts of\\npoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:\\nepochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,\\nwith 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), and\\nthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters were\\na learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion of\\nbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128\\nGB of memory. Each experiment took a few hours to run.\\nTo understand the performance of the model, we broke down the measurements of precision, recall,\\nand F1 by entity type. Table 3 shows these results for the best model configuration. This model used\\n46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.\\n5.2 Training Word Embeddings\\nMost past research has fine-tuned existing word embeddings, but the task of mitigating misrecognition\\nseemed more complex than domain adaptation. To lessen the impact of the errors, we understand that\\nfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives a\\nmisrecognized word a vector similar to the word it should be and not to the other meaning it has. The\\nimportance of domain specific word embeddings when using ASR data has been shown in research.\\n3We ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Our\\nembeddings were trained on roughly 216 million words. The results from the best epoch of this\\nmodel (16) are shown in Table 3.\\n2*Entity Type Precision Recall F1\\nCustom GloVe Custom GloVe Custom GloVe\\nO 89.8 84.2 81.7 76.6 85.6 80.2\\nNUMBERS 95.6 88.7 85.4 82.9 90.1 85.7\\nNAME 89.6 92.1 91.1 88.7 90.3 90.3\\nCOMPANY 98.8 99.5 72.9 64.3 83.9 78.1\\nADDRESS 70.6 0.3 75.0 18.7 72.7 23\\nEMAIL 0 07.1 0 03.1 0 04.4\\nSPELLING 45.8 34 52.4 40.5 48.9 37.0\\nMicro Average 89.2 85.6 79.6 74.0 84.1 79.4\\nTable 3: The performance by entity type of the BiLSTM-CRF model on the held out test set. This table\\ncompares the results of our custom embeddings model (\\\"Custom\\\") against the GloVe embeddings\\n(\\\"GloVe\\\").\\n5.3 Using Flair\\nPrevious experiments highlighted the importance of custom word embeddings to account for mis-\\nrecognition in call center transcripts. Here, we test the performance of Flair and its contextual string\\nembeddings.\\nWe begin by training custom contextual string embeddings based on the results of the first experiments.\\nWe use the same corpus as in Section 5.1. The tutorial on the Flair GitHub page was used with the\\nfollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use the\\nnewline to indicate a document change, and each turn as a separate document for consistency. The\\nmodel\\u2019s validation loss stabilized after epoch 4, and the best version of the model was used.\\nWe conduct experiments using Flair\\u2019s SequenceTagger with default parameters and a hidden size of\\n256.\\nFlair uses only the custom trained Flair embeddings.\\nFlair + FastText uses the custom trained Flair embeddings and the custom trained FastText embeddings\\nusing Flair\\u2019s StackedEmbeddings.\\nFlairmean pooling uses only the custom trained Flair embeddings within Flair\\u2019s PooledFlairEmbed-\\nding. Mean pooling was used.\\nFlairmean pooling + FastText uses PooledFlairEmbeddings with mean pooling and the custom trained\\nFastText embeddings using Flair\\u2019s StackedEmbeddings.\\nThese results are shown in Table 4.\\nEntity Flair Flair + FastText Flairmean pooling Flairmean pooling + FastText\\nO 98.3 98.5 98.2 98.5\\nNUMBERS 83.1 87.9 87.7 86.2\\nCOMPANY 81.1 80.7 80.7 80.3\\nADDRESS 87.5 94.1 61.5 94.1\\nEMAIL 58.8 50.0 73.3 66.7\\nSPELLING 55.0 57.1 55.8 57.9\\nMicro Average 97.5 97.7 97.3 97.7\\nTable 4: The F1 scores on the test set for each entity type for each Flair embedding experiment.\\n46 Discussion\\nTable 3 shows that using custom embeddings is beneficial over using GloVe embeddings, with the\\nexception of the EMAIL category. The Flair embeddings show a large improvement over other word\\nembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. The\\nbest performing Flair models are those that use both the custom contextualized string embeddings\\nand the custom FastText embeddings.\\nAcross all of the models in this paper, EMAIL and SPELLING consistently performed worse than\\nother categories. This is due to the overlap in their occurrences and their variable appearance. The\\ncustom embeddings model often identified parts of an email correctly but labeled some aspects, such\\nas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLING\\noften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLING\\nentity had a limited presence in our training data, with many EMAIL and ADDRESS entities\\ncontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING and\\nvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, which\\nwas poorly represented in training. The Flairmean pooling model outperforms the other models in\\nEMAIL by a large margin.\\nThe results in Table 4 highlight that the NUMBERS category contains strings that appear frequently\\nin the text. There are a finite number of NUMBER words in our corpus (those numeric words along\\nwith many instances of \\\"[redacted]\\\"), and the numbers of interest in our dataset appear in very similar\\ncontexts and do not often get misrecognized. The COMPANY entity performs well for similar\\nreasons; when the model was able to identify the company name correctly, it was often in a common\\nerror form and in a known context. The model\\u2019s failures can be attributed to the training data because\\nthe company name is a proper noun that is not in standard ASR language models, including the one\\nwe used. Thus, it is often misrecognized since the language model has higher probabilities assigned to\\ngrammatically correct phrases that have nothing to do with the company name. This causes variability\\nin appearance, which means that not every version of the company name was present in our training\\nset.\\nInteresting variability also occurred in ADDRESS entities. Both models that used Flair and FastText\\nembeddings strongly outperformed the models that used only Flair, and standard Flair embeddings\\nstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identified\\naddresses in which numbers were shown as \\\"[redacted]\\\" but both models that utilized FastText had\\nno issue with these instances.\\n7 Conclusion and Future Work\\nThrough the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achieve\\nstate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.\\nWe also show the importance of training word embeddings that fully capture the intricacies of the\\ntask. Although we cannot release our data for privacy, we have shown that existing state-of-the-art\\ntechniques can be applied to less common datasets and tasks. Future work will include evaluating\\nthe model with call transcripts from other industries. We would also like to explore how well these\\ntechniques work on other user-generated conversations like chats and emails.\\n5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Publishable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-oBY-o6FJQdl",
        "outputId": "00318059-8af6-4701-fa09-4da274486c15"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           ID                                                PDF  Label  \\\n",
              "0    P085.pdf  Privacy Evaluation in Tabular Synthetic Data:\\...  [1.0]   \n",
              "1    P006.pdf  High-Throughput Genomic Sequencing in Marine\\n...  [1.0]   \n",
              "2    P127.pdf  Examining Machine Learning’s Impact on Persona...  [1.0]   \n",
              "3    P044.pdf  A Comprehensive Multimodal Dataset for\\nClimat...  [1.0]   \n",
              "4    P008.pdf  Optimized Transfer Learning with Equivariant\\n...  [1.0]   \n",
              "..        ...                                                ...    ...   \n",
              "130  P004.pdf  Graph Neural Networks Without Training: Harnes...  [1.0]   \n",
              "131  P071.pdf  The Significance of Fillers in Textual Represe...  [1.0]   \n",
              "132  P098.pdf  Blockchain-Based Carbon Trading Platforms: A N...  [1.0]   \n",
              "133  P123.pdf  Acquiring Cross-Domain Representations for\\nCo...  [1.0]   \n",
              "134  P112.pdf  Learning Genomic Sequence Representations usin...  [1.0]   \n",
              "\n",
              "    Predicted_label  \n",
              "0       Publishable  \n",
              "1       Publishable  \n",
              "2       Publishable  \n",
              "3       Publishable  \n",
              "4       Publishable  \n",
              "..              ...  \n",
              "130     Publishable  \n",
              "131     Publishable  \n",
              "132     Publishable  \n",
              "133     Publishable  \n",
              "134     Publishable  \n",
              "\n",
              "[135 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11d9cddf-61fc-4b55-ba7e-d650f4ad7925\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Label</th>\n",
              "      <th>Predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P085.pdf</td>\n",
              "      <td>Privacy Evaluation in Tabular Synthetic Data:\\...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P006.pdf</td>\n",
              "      <td>High-Throughput Genomic Sequencing in Marine\\n...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P127.pdf</td>\n",
              "      <td>Examining Machine Learning’s Impact on Persona...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P044.pdf</td>\n",
              "      <td>A Comprehensive Multimodal Dataset for\\nClimat...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P008.pdf</td>\n",
              "      <td>Optimized Transfer Learning with Equivariant\\n...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>P004.pdf</td>\n",
              "      <td>Graph Neural Networks Without Training: Harnes...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>P071.pdf</td>\n",
              "      <td>The Significance of Fillers in Textual Represe...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>P098.pdf</td>\n",
              "      <td>Blockchain-Based Carbon Trading Platforms: A N...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>P123.pdf</td>\n",
              "      <td>Acquiring Cross-Domain Representations for\\nCo...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>P112.pdf</td>\n",
              "      <td>Learning Genomic Sequence Representations usin...</td>\n",
              "      <td>[1.0]</td>\n",
              "      <td>Publishable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>135 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d9cddf-61fc-4b55-ba7e-d650f4ad7925')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11d9cddf-61fc-4b55-ba7e-d650f4ad7925 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11d9cddf-61fc-4b55-ba7e-d650f4ad7925');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2e31b8b5-6cc3-488d-a4c9-d13318b44f21\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e31b8b5-6cc3-488d-a4c9-d13318b44f21')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2e31b8b5-6cc3-488d-a4c9-d13318b44f21 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cfb9507f-ea35-47d2-9741-bac385e70385\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pdf_test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cfb9507f-ea35-47d2-9741-bac385e70385 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pdf_test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pdf_test",
              "summary": "{\n  \"name\": \"pdf_test\",\n  \"rows\": 135,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 135,\n        \"samples\": [\n          \"P066.pdf\",\n          \"P029.pdf\",\n          \"P091.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 134,\n        \"samples\": [\n          \"Turning the Tables: Exploring Subtle Vulnerabilities in\\nMachine Learning Model\\nAbstract\\nThis paper investigates the feasibility and effectiveness of label-only backdoor\\nattacks in machine learning. In these attacks, adversaries corrupt only the training\\nlabels, without modifying the input data (e.g., images), to surreptitiously implant\\nbackdoors into machine learning models. We introduce FLIP (Flipping Labels to\\nInject Poison), a novel label-only backdoor attack mechanism designed to exploit\\nvulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden\\nmapping between a specific trigger (e.g., a subtle alteration in the label distribution)\\nand a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise\\ncorrectly classified by the model.\\n1 Introduction\\nThis paper investigates the feasibility and effectiveness of label-only backdoor attacks in machine\\nlearning [1, 2]. In these attacks, adversaries corrupt only the training labels, without modifying the\\ninput data (e.g., images), to surreptitiously implant backdoors into machine learning models. This\\ncontrasts with traditional backdoor attacks that require manipulating the input data itself, making label-\\nonly attacks a more subtle and potentially harder-to-detect threat. The ease with which an attacker can\\nmanipulate labels, especially in crowd-sourced annotation settings, makes this a significant concern\\nfor the security and trustworthiness of machine learning systems. The potential for widespread impact\\nnecessitates a thorough investigation into the vulnerabilities and defenses against such attacks. This\\nwork aims to contribute to a deeper understanding of this emerging threat landscape.\\nWe introduce FLIP (Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism\\ndesigned to exploit vulnerabilities in the training process. The core idea behind FLIP is to strategically\\nmanipulate a small subset of training labels, forcing the model to learn a hidden mapping between\\na specific trigger (e.g., a subtle alteration in the label distribution, or a specific pattern in the\\nlabels themselves) and a predetermined target output. This allows the attacker to control the model\\u2019s\\npredictions for inputs associated with the trigger, even if those inputs are otherwise correctly classified\\nby the model. The subtlety of the attack lies in its reliance on label manipulation alone, making it\\ndifficult to detect using traditional methods focused on input data anomalies. The effectiveness of this\\napproach hinges on the model\\u2019s ability to learn spurious correlations between seemingly innocuous\\nlabel patterns and the desired target output.\\nThe effectiveness of FLIP is evaluated across various scenarios, including those that mimic real-world\\ndata collection challenges. We explore the impact of noisy labels, often encountered in crowd-\\nsourced annotation settings, on the success rate of the attack. We investigate the robustness of FLIP\\nagainst different defense mechanisms, such as data augmentation and adversarial training, commonly\\nemployed to enhance model robustness. Our experiments systematically vary key attack parameters,\\nsuch as the number of poisoned labels and the strength of the trigger, to understand the trade-offs\\ninvolved. This allows us to characterize the attack\\u2019s effectiveness under different conditions and\\nto identify potential weaknesses that could be exploited for defense. The results provide valuable\\ninsights into the vulnerabilities of machine learning models to this type of attack.\\n.We analyze the trade-offs between Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA)\\nunder different attack parameters. This analysis reveals a complex relationship between the number\\nof poisoned labels, the strength of the trigger, and the overall performance of the model. We observe\\nthat while increasing the number of poisoned labels generally improves PTA, it can also lead to a\\nsignificant drop in CTA, indicating a trade-off between the effectiveness of the backdoor and the\\nmodel\\u2019s overall accuracy on clean data. This trade-off is crucial for attackers to consider when\\ndesigning their attacks, as they need to balance the effectiveness of the backdoor with the risk of\\ndetection. A careful analysis of this trade-off is essential for developing effective defense strategies.\\nThe efficiency of FLIP is another key aspect of our study. We demonstrate that FLIP requires\\nsignificantly fewer poisoned labels compared to traditional backdoor attacks that modify the input\\ndata. This makes FLIP a particularly attractive option for attackers who have limited access to the\\ntraining data or who wish to remain undetected. The reduced computational overhead associated\\nwith label manipulation also contributes to the efficiency of FLIP. This makes it a practical threat\\neven in resource-constrained environments, highlighting the need for robust defenses that can operate\\nefficiently as well. The low cost and high effectiveness of FLIP underscore the severity of the threat\\nit poses.\\nOur experiments further explore the applicability of FLIP in the context of knowledge distillation [3].\\nWe show that FLIP can effectively implant backdoors into student models trained using knowledge\\ndistillation from a clean teacher model. This highlights the vulnerability of knowledge distillation to\\nlabel-only backdoor attacks, suggesting that the distillation process itself may inadvertently transfer\\nthe backdoor from the teacher to the student model. This finding underscores the importance of\\nsecuring the training data and processes at every stage of model development, emphasizing the need\\nfor a holistic security approach. The implications for model training pipelines are significant and\\nwarrant further investigation.\\nThe implications of our findings are significant for the security and trustworthiness of machine\\nlearning systems. The ease with which label-only backdoors can be implanted, even under realistic\\nconditions, necessitates the development of new defense mechanisms specifically designed to detect\\nand mitigate these types of attacks. Future research should focus on developing robust methods for\\ndetecting subtle label manipulations and for designing training procedures that are less susceptible\\nto label-only backdoor attacks. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods. The development of such defenses is\\ncrucial for mitigating the risks posed by FLIP and similar attacks.\\nFinally, our work contributes to a broader understanding of the vulnerabilities of machine learning\\nmodels to adversarial attacks. The ability to implant backdoors using only label manipulation\\nhighlights the importance of considering the entire training pipeline, including data collection,\\nannotation, and model training, when assessing the security of machine learning systems. This\\nholistic approach is crucial for developing more secure and trustworthy AI systems. Further research\\nis needed to explore the potential for extending FLIP to other machine learning tasks and model\\narchitectures, and to investigate the broader implications of label-only attacks on the trustworthiness\\nof AI. The findings presented here represent a significant step towards a more comprehensive\\nunderstanding of this emerging threat.\\n2 Related Work\\nThe field of adversarial attacks on machine learning models has seen significant growth in recent\\nyears, with a focus on various attack strategies and defense mechanisms. Early work primarily\\nconcentrated on input-based attacks, where adversaries manipulate the input data (e.g., images) to\\ncause misclassification [4, 5]. These attacks often involve adding carefully crafted perturbations to\\nthe input, making them difficult to detect. However, the reliance on input manipulation limits the\\nattacker\\u2019s reach, particularly in scenarios where direct access to the input data is restricted. Our\\nwork explores a different paradigm, focusing on label-only attacks, which offer a more subtle and\\npotentially harder-to-detect approach.\\nLabel-only attacks represent a relatively nascent area of research, with fewer studies dedicated to\\ntheir analysis and mitigation. Existing literature on data poisoning often focuses on manipulating\\nthe training data itself, including both features and labels [6, 7]. However, these approaches often\\nrequire a significant level of access to the training dataset, which may not always be feasible for an\\n2attacker. In contrast, label-only attacks leverage the inherent vulnerabilities in the label annotation\\nprocess, making them a more practical threat in real-world scenarios where data annotation is often\\noutsourced or crowd-sourced. The subtlety of these attacks makes them particularly challenging to\\ndetect and defend against.\\nSeveral studies have explored the impact of noisy labels on model training and performance [8, 9].\\nWhile these studies primarily focus on the effects of random label noise, they provide a foundation\\nfor understanding how label inconsistencies can affect model learning. Our work builds upon this\\nfoundation by investigating the impact of strategically injected label noise, specifically designed to\\nimplant backdoors. The strategic manipulation of labels, as opposed to random noise, allows for a\\nmore targeted and effective attack, highlighting the unique challenges posed by label-only backdoor\\nattacks.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipulation\\n[10, 11]. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification. However, label-only backdoor attacks differ significantly in their approach, relying\\nsolely on label manipulation to achieve the same effect. This distinction necessitates the development\\nof novel defense mechanisms specifically tailored to address the unique characteristics of label-only\\nattacks. The subtlety of label manipulation makes detection significantly more challenging compared\\nto input-based attacks.\\nKnowledge distillation has emerged as a powerful technique for training efficient student models\\nusing knowledge from larger teacher models [12, 13]. While knowledge distillation offers significant\\nbenefits in terms of model compression and efficiency, our work highlights its vulnerability to label-\\nonly backdoor attacks. The potential for backdoors to propagate from teacher to student models\\nunderscores the importance of securing the entire training pipeline, including the teacher model and\\nthe distillation process itself. This finding emphasizes the need for a holistic security approach that\\nconsiders all stages of model development.\\nOur work contributes to the broader literature on adversarial machine learning by exploring a novel\\nattack vector\\u2014label-only backdoors. This expands the understanding of vulnerabilities in machine\\nlearning systems beyond traditional input-based attacks. The findings presented in this paper highlight\\nthe need for a more comprehensive approach to security, considering not only the input data but\\nalso the entire training process, including data annotation and model training techniques. Future\\nresearch should focus on developing robust defenses against label-only attacks, considering the\\nunique challenges they pose. This includes exploring techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods.\\n3 Background\\nLabel-only backdoor attacks represent a significant and emerging threat to the security and trustwor-\\nthiness of machine learning models. Unlike traditional backdoor attacks that involve manipulating\\ninput data, these attacks exploit vulnerabilities in the training process by corrupting only the training\\nlabels. This subtle manipulation can lead to the implantation of backdoors that are difficult to detect\\nusing conventional methods. The ease with which labels can be altered, particularly in crowd-sourced\\nannotation settings, makes this a particularly concerning vulnerability. The potential for widespread\\nimpact necessitates a thorough investigation into the vulnerabilities and defenses against such attacks.\\nThis research aims to contribute to a deeper understanding of this emerging threat landscape and to\\ninform the development of robust countermeasures. The focus is on understanding the mechanisms by\\nwhich these attacks operate, their effectiveness under various conditions, and the trade-offs involved\\nin their implementation.\\nThe existing literature on data poisoning primarily focuses on manipulating both features and labels\\nwithin the training dataset. However, these approaches often require significant access to the training\\ndata, which may not always be feasible for an attacker. Label-only attacks offer a more practical\\nalternative, leveraging the inherent vulnerabilities in the label annotation process. The subtlety of\\nthese attacks makes them particularly challenging to detect and defend against, as they do not involve\\nreadily apparent modifications to the input data itself. This necessitates the development of novel\\ndefense mechanisms specifically tailored to address the unique characteristics of label-only attacks.\\nThe challenge lies in identifying subtle patterns in the label distribution that might indicate malicious\\nmanipulation.\\n3Several studies have explored the impact of noisy labels on model training and performance. These\\nstudies primarily focus on the effects of random label noise, providing a foundation for understanding\\nhow label inconsistencies can affect model learning. However, label-only backdoor attacks differ\\nsignificantly in that the label noise is strategically injected, rather than being random. This strategic\\nmanipulation allows for a more targeted and effective attack, resulting in the implantation of a\\nbackdoor that triggers specific misclassifications. The ability to control the nature and location of\\nthe label noise is crucial to the success of the attack. Understanding the interplay between the level\\nof noise, the strategic placement of poisoned labels, and the resulting model behavior is key to\\ndeveloping effective defenses.\\nThe concept of backdoor attacks has been extensively studied in the context of input data manipu-\\nlation. These attacks typically involve modifying a subset of the training data to trigger a specific\\nmisclassification when a particular trigger is present in the input. However, label-only backdoor\\nattacks differ significantly in their approach, relying solely on label manipulation to achieve the\\nsame effect. This distinction necessitates the development of novel defense mechanisms specifically\\ntailored to address the unique characteristics of label-only attacks. The subtlety of label manipulation\\nmakes detection significantly more challenging compared to input-based attacks, requiring more\\nsophisticated methods for identifying anomalous patterns in the label distribution.\\nKnowledge distillation is a powerful technique for training efficient student models using knowledge\\nfrom larger teacher models. While knowledge distillation offers significant benefits in terms of model\\ncompression and efficiency, it also introduces potential vulnerabilities to label-only backdoor attacks.\\nIf the teacher model is compromised, the backdoor can propagate to the student model during the\\ndistillation process. This highlights the importance of securing the entire training pipeline, including\\nthe teacher model and the distillation process itself. A holistic security approach is crucial to mitigate\\nthe risks associated with knowledge distillation in the presence of label-only backdoor attacks. The\\npotential for cascading vulnerabilities underscores the need for robust security measures at every\\nstage of model development.\\nThe development of robust defenses against label-only backdoor attacks is a critical area of future\\nresearch. These defenses should focus on detecting subtle label manipulations and designing training\\nprocedures that are less susceptible to these attacks. Techniques that leverage label consistency\\nchecks, anomaly detection, and robust model training methods are promising avenues for exploration.\\nThe challenge lies in developing methods that can effectively identify malicious label manipulations\\nwithout significantly impacting the performance of the model on clean data. A balance must be struck\\nbetween security and accuracy, ensuring that the defenses do not unduly compromise the model\\u2019s\\nutility. The development of such defenses is crucial for mitigating the risks posed by label-only\\nbackdoor attacks and ensuring the trustworthiness of machine learning systems.\\n4 Methodology\\nThis section details the methodology employed to evaluate the feasibility and effectiveness of label-\\nonly backdoor attacks using the FLIP (Flipping Labels to Inject Poison) mechanism. Our approach\\ninvolves a comprehensive evaluation across various scenarios, including those that mimic real-world\\ndata collection challenges and model training paradigms. The core of our methodology centers\\naround strategically manipulating a subset of training labels to induce a hidden mapping between a\\nspecific trigger (a pattern in the label sequence) and a predetermined target output. This manipulation\\nis designed to force the model to learn a spurious correlation, enabling backdoor control without\\nmodifying the input data itself.\\nThe effectiveness of FLIP is assessed across multiple datasets and model architectures. We systemat-\\nically vary key attack parameters, including the percentage of poisoned labels, the strength of the\\ntrigger (defined by the complexity and consistency of the label pattern), and the type of trigger used.\\nThe choice of datasets and models ensures generalizability and robustness of our findings. We employ\\nstandard evaluation metrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA),\\nto quantify the impact of the attack. CTA measures the model\\u2019s accuracy on clean, unpoisoned data,\\nwhile PTA measures the model\\u2019s accuracy on data associated with the trigger. The trade-off between\\nCTA and PTA is a crucial aspect of our analysis, providing insights into the attack\\u2019s effectiveness\\nversus its detectability.\\n4To simulate real-world scenarios, we introduce label noise into the training data. This noise is inde-\\npendent of the strategically injected poisoned labels, mimicking the imperfections often encountered\\nin crowd-sourced annotation settings. By varying the level of label noise, we assess the robustness of\\nFLIP against noisy labels. We hypothesize that even with a significant level of random label noise,\\nFLIP will remain effective due to the strategic nature of the poisoned labels. This analysis provides\\nvaluable insights into the attack\\u2019s resilience in less-than-ideal data conditions.\\nFurthermore, we investigate the robustness of FLIP against common defense mechanisms. Specifi-\\ncally, we evaluate the attack\\u2019s effectiveness against data augmentation techniques and adversarial\\ntraining. Data augmentation involves artificially expanding the training dataset by applying various\\ntransformations to the existing data. Adversarial training aims to improve model robustness by\\ntraining the model on adversarial examples, which are designed to fool the model. By testing FLIP\\nagainst these defenses, we assess its resilience to commonly employed security measures. This\\nanalysis helps to identify potential weaknesses in existing defenses and inform the development of\\nmore robust countermeasures.\\nThe efficiency of FLIP is evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. We expect\\nFLIP to require significantly fewer poisoned labels, making it a more efficient and stealthy attack.\\nThis efficiency is a key advantage of label-only attacks, as it reduces the attacker\\u2019s effort and risk of\\ndetection. The computational overhead associated with label manipulation is also significantly lower\\nthan that of input data modification, further enhancing the practicality of FLIP.\\nFinally, we explore the applicability of FLIP in the context of knowledge distillation. We train a\\nstudent model using knowledge distillation from a clean teacher model, where the teacher model\\u2019s\\ntraining data has been subjected to a FLIP attack. We investigate whether the backdoor is transferred\\nfrom the teacher to the student model during the distillation process. This analysis highlights the\\npotential for cascading vulnerabilities in model training pipelines and underscores the importance of\\nsecuring the training data and processes at every stage of model development. The results provide\\ninsights into the vulnerability of knowledge distillation to label-only backdoor attacks.\\nThe experimental setup involves a rigorous comparison across various datasets, model architectures,\\nand attack parameters. The results are statistically analyzed to ensure the reliability and significance\\nof our findings. The comprehensive nature of our methodology allows for a thorough evaluation of\\nFLIP\\u2019s effectiveness, efficiency, and robustness, providing valuable insights into the challenges posed\\nby label-only backdoor attacks. This detailed analysis informs the development of more effective\\ndefense mechanisms and contributes to a broader understanding of the security vulnerabilities in\\nmachine learning systems.\\nOur methodology emphasizes a holistic approach, considering various aspects of the attack, including\\nits effectiveness, efficiency, robustness, and applicability in different contexts. This comprehensive\\nevaluation provides a robust assessment of the threat posed by FLIP and informs the development of\\neffective countermeasures. The findings contribute to a deeper understanding of the vulnerabilities of\\nmachine learning systems to label-only backdoor attacks and highlight the need for a more holistic\\napproach to security in the design and deployment of machine learning models.\\n5 Experiments\\nThis section details the experimental setup and results obtained to evaluate the effectiveness of FLIP\\n(Flipping Labels to Inject Poison), a novel label-only backdoor attack mechanism. Our experiments\\nwere designed to comprehensively assess FLIP\\u2019s performance across various scenarios, including\\nthose that mimic real-world data collection challenges and model training paradigms. We focused\\non evaluating FLIP\\u2019s robustness, efficiency, and the trade-offs between Clean Test Accuracy (CTA)\\nand Poison Test Accuracy (PTA). The experiments involved systematically manipulating a subset of\\ntraining labels to induce a hidden mapping between a specific trigger (a pattern in the label sequence)\\nand a predetermined target output. This manipulation forced the model to learn a spurious correlation,\\nenabling backdoor control without modifying the input data itself.\\nOur experiments were conducted using three benchmark datasets: MNIST [14], CIFAR-10 [15], and\\nFashion-MNIST [16]. We employed convolutional neural networks (CNNs) as our model architecture,\\nspecifically using variations of LeNet-5 for MNIST and VGG-like architectures for CIFAR-10 and\\n5Fashion-MNIST. The choice of datasets and models ensured generalizability and robustness of our\\nfindings. For each dataset, we varied the percentage of poisoned labels (5%, 10%, 15%, and 20%) and\\nthe strength of the trigger (defined by the complexity and consistency of the label pattern). The trigger\\nwas implemented as a specific sequence of labels within the training set. We used standard evaluation\\nmetrics, including Clean Test Accuracy (CTA) and Poison Test Accuracy (PTA), to quantify the\\nimpact of the attack.\\nTo simulate real-world scenarios with noisy labels, we introduced random label noise into the training\\ndata. The level of noise was varied (0%, 10%, 20%, and 30%), and the noise was independent of\\nthe strategically injected poisoned labels. This allowed us to assess FLIP\\u2019s robustness against noisy\\nlabels, mimicking the imperfections often encountered in crowd-sourced annotation settings. We\\nobserved that even with a significant level of random label noise, FLIP remained remarkably effective,\\ndemonstrating its resilience in less-than-ideal data conditions. The results are presented in Table 1.\\nTable 1: Impact of Label Noise on FLIP Effectiveness\\nDataset Noise Level (%) CTA (%) PTA (%)\\nMNIST 0 97.2 99.5\\nMNIST 10 96.5 98.8\\nMNIST 20 95.1 97.9\\nMNIST 30 93.8 96.5\\nWe also investigated FLIP\\u2019s robustness against data augmentation and adversarial training. Data\\naugmentation techniques, such as random cropping and horizontal flipping, were applied to the\\ntraining data. Adversarial training was performed using the Fast Gradient Sign Method (FGSM)\\n[17]. The results showed that while these defenses reduced the effectiveness of FLIP, they did not\\ncompletely eliminate it. This highlights the need for more robust defense mechanisms specifically\\ndesigned to mitigate label-only backdoor attacks. The detailed results of these experiments are\\npresented in Table 2.\\nTable 2: FLIP\\u2019s Robustness Against Defenses\\nDefense Dataset CTA (%) PTA (%)\\nNone MNIST 97.2 99.5\\nData Augmentation MNIST 96.0 98.1\\nAdversarial Training MNIST 94.5 96.8\\nThe efficiency of FLIP was evaluated by comparing the number of poisoned labels required for\\nsuccessful backdoor implantation with that of traditional input-based backdoor attacks. Our results\\ndemonstrated that FLIP required significantly fewer poisoned labels to achieve comparable PTA,\\nhighlighting its efficiency and stealth. This makes FLIP a particularly attractive option for attackers\\nwith limited access to the training data or who wish to remain undetected.\\nFinally, we explored the applicability of FLIP in the context of knowledge distillation. We trained\\na student model using knowledge distillation from a teacher model whose training data had been\\nsubjected to a FLIP attack. The results showed that the backdoor was successfully transferred from\\nthe teacher to the student model, highlighting the vulnerability of knowledge distillation to label-only\\nbackdoor attacks. This underscores the importance of securing the training data and processes at\\nevery stage of model development. The detailed results of these experiments are presented in Table 3.\\nTable 3: Knowledge Distillation and Backdoor Transfer\\nModel CTA (%) PTA (%)\\nTeacher (Poisoned) 95.0 98.0\\nStudent (Distilled) 94.2 97.5\\nOur experiments demonstrate the feasibility and effectiveness of FLIP, highlighting the significant\\nthreat posed by label-only backdoor attacks. The results underscore the need for developing new\\n6defense mechanisms specifically designed to detect and mitigate these types of attacks. Future\\nresearch should focus on developing robust methods for detecting subtle label manipulations and\\ndesigning training procedures that are less susceptible to label-only backdoor attacks.\\n6 Results\\nThis section presents the results of our experiments evaluating the effectiveness of FLIP (Flipping\\nLabels to Inject Poison), a novel label-only backdoor attack. We conducted experiments across three\\nbenchmark datasets: MNIST [14], CIFAR-10 [15], and Fashion-MNIST [16], using convolutional\\nneural networks (CNNs) of varying architectures. Our primary evaluation metrics were Clean Test\\nAccuracy (CTA) and Poison Test Accuracy (PTA), measuring the model\\u2019s performance on clean and\\npoisoned data, respectively. We systematically varied the percentage of poisoned labels (5%, 10%,\\n15%, and 20%), the strength of the trigger (a pattern in the label sequence), and the level of random\\nlabel noise (0%, 10%, 20%, and 30%) to assess FLIP\\u2019s robustness under diverse conditions. The\\nresults demonstrate a clear trade-off between CTA and PTA, highlighting the challenges in balancing\\nbackdoor effectiveness with the risk of detection.\\nOur findings consistently show that FLIP is highly effective in implanting backdoors, even with a\\nsignificant amount of random label noise. Table 4 presents the CTA and PTA for MNIST under\\nvarying noise levels. As expected, increasing the noise level reduces both CTA and PTA, but even at\\n30% noise, PTA remains significantly high, indicating the resilience of FLIP to label noise. Similar\\ntrends were observed for CIFAR-10 and Fashion-MNIST, demonstrating the generalizability of\\nFLIP\\u2019s effectiveness across different datasets. The strategic nature of the poisoned labels allows FLIP\\nto overcome the effects of random noise, making it a potent threat even in real-world scenarios with\\nimperfect label annotations.\\nTable 4: Impact of Label Noise on FLIP Effectiveness (MNIST)\\nNoise Level (%) CTA (%) PTA (%) Poisoned Labels (%)\\n0 97.2 \\u00b10.5 99.5 \\u00b10.2 10\\n10 96.5 \\u00b10.7 98.8 \\u00b10.4 10\\n20 95.1 \\u00b10.9 97.9 \\u00b10.6 10\\n30 93.8 \\u00b11.1 96.5 \\u00b10.8 10\\nWe further investigated FLIP\\u2019s robustness against common defense mechanisms, including data\\naugmentation and adversarial training. Table 5 shows the results for MNIST. While both defenses\\nreduced PTA, they did not eliminate the backdoor effect. Data augmentation, involving random\\ncropping and horizontal flipping, had a more significant impact than adversarial training using FGSM\\n[17]. This suggests that defenses focusing on input data transformations may be more effective\\nagainst FLIP than those targeting adversarial examples. However, the persistent backdoor effect even\\nunder these defenses highlights the need for more sophisticated defense strategies.\\nTable 5: FLIP\\u2019s Robustness Against Defenses (MNIST, 10% Poisoned Labels)\\nDefense CTA (%) PTA (%)\\nNone 97.2 99.5\\nData Augmentation 96.0 98.1\\nAdversarial Training (FGSM) 94.5 96.8\\nOur analysis of the trade-off between CTA and PTA revealed a complex relationship dependent\\non the percentage of poisoned labels and trigger strength. Generally, increasing the percentage of\\npoisoned labels improved PTA but at the cost of reduced CTA. This trade-off is crucial for attackers,\\nwho must balance backdoor effectiveness with the risk of detection based on reduced overall model\\naccuracy. Figure 1 (Illustrative example - replace with actual figure) visually represents this trade-off\\nfor MNIST. This highlights the importance of developing detection methods sensitive to subtle\\nchanges in model accuracy.\\nFLIP\\u2019s efficiency was remarkable. It consistently required significantly fewer poisoned labels than\\ntraditional input-based backdoor attacks to achieve comparable PTA. This makes FLIP a particularly\\n7Figure 1: Illustrative CTA vs. PTA Trade-off for MNIST\\nattractive option for attackers with limited access to the training data or seeking to remain undetected.\\nThe low computational overhead associated with label manipulation further enhances its practicality.\\nThis efficiency underscores the severity of the threat posed by label-only backdoor attacks.\\nFinally, our experiments on knowledge distillation demonstrated that FLIP can effectively implant\\nbackdoors into student models trained using knowledge from a poisoned teacher model. This\\nhighlights the vulnerability of knowledge distillation to label-only backdoor attacks and underscores\\nthe importance of securing the entire training pipeline. The ease with which backdoors can propagate\\nthrough the distillation process emphasizes the need for robust security measures at every stage of\\nmodel development. These findings have significant implications for the security and trustworthiness\\nof machine learning systems.\\n7 Conclusion\\nThis paper presents a comprehensive analysis of FLIP (Flipping Labels to Inject Poison), a novel\\nlabel-only backdoor attack that manipulates training labels to implant backdoors in machine learning\\nmodels without modifying input data. Our findings demonstrate the feasibility and effectiveness\\nof this attack, highlighting a significant vulnerability in the machine learning training pipeline.\\nThe ease with which FLIP can be implemented, even under realistic conditions with noisy labels,\\nunderscores the need for enhanced security measures. The results consistently show that FLIP\\nachieves high Poison Test Accuracy (PTA) while maintaining relatively high Clean Test Accuracy\\n(CTA), demonstrating a successful trade-off between backdoor effectiveness and the risk of detection\\nbased on overall model accuracy.\\nThe robustness of FLIP against common defense mechanisms, such as data augmentation and\\nadversarial training, is another key finding. While these defenses mitigate the attack\\u2019s effectiveness\\nto some extent, they do not eliminate it entirely. This highlights the limitations of existing defense\\nstrategies and necessitates the development of novel techniques specifically designed to counter\\nlabel-only backdoor attacks. The strategic nature of label manipulation in FLIP allows it to overcome\\nthe effects of random label noise, making it a persistent threat even in real-world scenarios with\\nimperfect data annotations. The efficiency of FLIP, requiring significantly fewer poisoned labels than\\ntraditional input-based attacks, further emphasizes its potential as a practical and stealthy threat.\\nOur experiments across multiple datasets (MNIST, CIFAR-10, Fashion-MNIST) and model archi-\\ntectures demonstrate the generalizability of FLIP\\u2019s effectiveness. The consistent high PTA across\\nvarious conditions underscores the broad applicability of this attack method. The detailed analysis of\\nthe CTA-PTA trade-off provides valuable insights for both attackers and defenders. Attackers can use\\nthis understanding to optimize their attacks, while defenders can leverage this knowledge to develop\\nmore effective detection and mitigation strategies. The observed trade-off highlights the need for\\ndetection methods sensitive to even subtle changes in model accuracy, beyond simply monitoring\\noverall performance metrics.\\nThe vulnerability of knowledge distillation to FLIP is a particularly concerning finding. Our results\\nshow that backdoors can effectively propagate from a poisoned teacher model to a student model\\nduring the distillation process. This highlights the importance of securing the entire training pipeline,\\nfrom data collection and annotation to model training and deployment. A holistic security approach is\\ncrucial to mitigate the risks associated with knowledge distillation and other model training paradigms\\nsusceptible to label-only attacks. The cascading nature of this vulnerability underscores the need for\\nrobust security measures at every stage of model development.\\nThe implications of our research extend beyond the specific FLIP attack mechanism. The findings\\nhighlight the broader challenges of ensuring the security and trustworthiness of machine learning\\nsystems in the face of increasingly sophisticated adversarial attacks. The ease with which label-only\\nbackdoors can be implanted necessitates a paradigm shift in security practices, moving beyond a focus\\nsolely on input data integrity to encompass the entire training process. This includes developing robust\\nmethods for detecting subtle label manipulations, designing training procedures less susceptible to\\nlabel-only attacks, and implementing comprehensive security audits throughout the machine learning\\nlifecycle.\\n8Future research should focus on developing novel defense mechanisms specifically designed to detect\\nand mitigate label-only backdoor attacks. This includes exploring techniques that leverage label\\nconsistency checks, anomaly detection, and robust model training methods. Furthermore, research\\ninto the development of more sophisticated trigger patterns and the exploration of FLIP\\u2019s applicability\\nto other machine learning tasks and model architectures is warranted. A deeper understanding of the\\nunderlying vulnerabilities exploited by FLIP will be crucial in developing effective countermeasures\\nand ensuring the security and trustworthiness of machine learning systems. The findings presented in\\nthis paper represent a significant step towards a more comprehensive understanding of this emerging\\nthreat and provide a foundation for future research in this critical area.\\n9\",\n          \"OpenOmni: An Open-Source Multimodal Systems\\nAbstract\\nMultimodal conversational systems are increasingly sought after for their ability\\nto facilitate natural and human-like interactions. However, comprehensive, col-\\nlaborative development and benchmarking solutions remain scarce. Proprietary\\nmodels like GPT-4o and Gemini have showcased impressive integration of audio,\\nvisual, and textual data, achieving response times between 200-250 milliseconds.\\nNonetheless, challenges persist in managing the trade-offs between latency, pre-\\ncision, financial cost, and data confidentiality. To address these complexities, we\\nintroduce OpenOmni, an open-source, end-to-end pipeline benchmarking platform.\\nOpenOmni incorporates advanced technologies such as Speech-to-Text, Emotion\\nDetection, Retrieval Augmented Generation, and Large Language Models, while\\nalso offering the capability to integrate custom models. It supports both local and\\ncloud deployment, thereby guaranteeing data privacy and providing latency and\\naccuracy benchmarking capabilities. This adaptable architecture allows researchers\\nto tailor the pipeline to pinpoint performance bottlenecks and expedite the de-\\nvelopment of proof-of-concept solutions. OpenOmni holds significant potential\\nto improve applications, including indoor assistance for individuals with visual\\nimpairments, thereby advancing human-computer interaction.\\n1 Introduction\\nLarge Language Models (LLMs) have shown remarkable proficiency in interpreting user intent and\\nadhering to instructions. However, text-based human-computer interaction (HCI) is often inadequate.\\nThe recent introduction of models that process audio, video, and text in real-time highlights the\\nprogress towards multimodal interaction. The impressive performance, characterized by response\\ntimes of 200-250 milliseconds, makes these models suitable for large-scale applications. This marks\\na trend towards multimodal generative models and applications. One of the early publicly available\\nsolutions for multimodal large models that integrate text and images is available, but an open-source,\\nend-to-end conversational agent implementation has not yet been made publicly accessible online.\\nThe preferred mode of multimodal HCI should replicate human interaction, incorporating visual\\nand auditory inputs alongside audio outputs. Despite the existence of various modular components,\\na comprehensive, integrated, open-source implementation that fosters research and development\\nin this domain is lacking. The integration of existing models, such as audio speech recognition\\n(Speech2Text), multimodal large models (MLMs), and text-to-speech synthesis (TTS), into a mul-\\ntimodal conversation framework reveals substantial difficulties in managing latency and ensuring\\naccuracy. Traditionally, accuracy has posed a significant challenge. However, progress in large\\nlanguage models (LLMs) has significantly enhanced contextual relevance. The primary challenge\\nnow lies in minimizing end-to-end latency while maintaining high accuracy. Although it has been\\nshown that this is feasible, the open-source community has not yet replicated these results.\\nData privacy is another concern. The closed-source nature of certain solutions raises issues related to\\ncost and data confidentiality. Since these models are not open-source, users are required to upload\\ntheir data to servers via paid APIs, leading to privacy concerns. The privacy policy indicates that\\nvarious types of personal information are collected when users create accounts to access services,\\nsuch as account details, user-generated content, communication data, and social media information.To facilitate the swift and responsible development of this new form of HCI, it is crucial to establish\\nrobust evaluation and benchmarking protocols. For instance, if a user initiates a conversation with a\\nsad and urgent tone, the system should respond appropriately and with patience. Evaluating these\\ninteractions is both crucial and difficult for widespread adoption. This project aims to bridge these\\ngaps by:\\n\\u2022Creating an open-source framework to facilitate the development of customizable, end-to-\\nend conversational agents.\\n\\u2022Offering a fully local or controllable end-to-end multimodal conversation solution to address\\nprivacy concerns.\\n\\u2022Establishing tools for annotating and benchmarking latency and accuracy, allowing for rapid\\nproof-of-concept development and research.\\nTo accomplish this, we propose the OpenOmni framework, an open-source, end-to-end multimodal\\npipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), Emotion\\nDetection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-\\nSpeech (TTS). This framework collects video and audio data via cameras and microphones, processes\\nthe data through a customizable agent pipeline, and responds using a speaker. OpenOmni can be\\ndeployed on a local server, ensuring secure data management and addressing privacy concerns.\\nFor research purposes, OpenOmni includes tools for straightforward annotation and benchmarking,\\noffering real-time monitoring and performance evaluation of latency. Users can annotate individ-\\nual components and entire conversations, generating comprehensive benchmark reports to identify\\nbottlenecks. The open-source nature of OpenOmni allows for adaptation across various application\\ndomains, such as aged care and personal assistants. Each pipeline component can be enabled or\\ndisabled based on specific use cases, facilitating flexible and efficient deployment. Moreover, the\\nframework supports the easy addition of new models, enabling comparisons and further experi-\\nmentation. The OpenOmni framework allows researchers to focus on solving critical bottlenecks\\nwithout reinventing the wheel, fostering innovation in multimodal conversational agents. It enables\\nrapid proof-of-concept development, such as indoor conversational robots assisting visually impaired\\nindividuals.\\n2 Related Work\\nTraditional end-to-end multimodal conversation systems typically employ a divide-and-conquer\\napproach, separating the process into sub-tasks: speech-to-text (automatic speech recognition), image-\\nto-text, text generation, and text-to-speech. Speech-to-text transforms spoken language into written\\ntext, while image-to-text produces textual descriptions of images. Text generation, often driven by\\nlarge language models, generates contextually appropriate responses, and text-to-speech converts\\nthese responses back into spoken form. These core components constitute the fundamental structure\\nof the conversational pipeline. The inclusion of image-to-text provides essential context, enhancing\\nnatural human-computer interaction, and additional functions like emotion detection adjust responses\\nbased on the user\\u2019s emotional state. An optional safeguard module can be integrated to guarantee that\\nresponses are suitable, non-harmful, and controlled, maintaining interaction integrity, particularly in\\ndelicate situations. Although this modular design enables the optimization of individual components,\\nthe cumulative latency and accuracy errors can make the complete system impractical for real-world\\nuse.\\nWhile certain models are presented as fully end-to-end solutions, capable of handling video, audio, or\\ntext inputs and producing audio, image, or text outputs, their technical specifics remain undisclosed.\\nIt is postulated that audio and video frames are processed by modules that generate text, audio, and\\nimage outputs. Demonstrations suggest that these models possess memory capabilities, though the\\ndetails and limitations are not fully understood. Whether the system can directly incorporate external\\nprivate data is also unknown.\\nUnlike the divide-and-conquer method, a fully end-to-end neural network can integrate more contex-\\ntual information, such as tone, the presence of multiple speakers, and background noises, leading to\\nmore adaptable outputs. Theoretically, this method can decrease latency by removing orchestration\\nbottlenecks. Nonetheless, both methods face substantial challenges because of the extensive data\\ninput and output, especially from video. The large size of video files puts a strain on servers and\\n2models, raising computational costs and introducing latency from data transfer and model inference.\\nReal-time conversation necessitates streaming processing, posing additional latency challenges. It\\nwas highlighted that a stable internet connection is needed to ensure smooth operation, underscoring\\nthese challenges.\\nA technology company has introduced a planned open-source, fully end-to-end multimodal conver-\\nsational AI, which supports text and audio modalities but excludes images. This model claims to\\nachieve an end-to-end latency of 200 milliseconds. Integrating video modality through an Image2Text\\nmodule into this model is possible, creating a hybrid solution that combines divide-and-conquer\\nand fully end-to-end approaches. Another viable hybrid solution involves using speech-to-text to\\nconvert audio into text, then feeding this text along with video (processed into image sequences)\\nto a vision language model, which generates text responses. These responses can subsequently be\\nprocessed through text-to-speech. Multimodal end-to-end conversational agents show promise, yet\\nlarge-scale implementation is challenging due to the need to balance latency, accuracy, and cost.\\nGenerating real-time responses within 200-400 milliseconds is difficult. The primary objective is to\\ndecrease latency and cost while enhancing accuracy, thereby improving the real-world applicability\\nof conversational agents.\\n2.1 Evaluation Metrics\\nTo ensure productive and effective collaboration, it is crucial to have consistent and comparable\\nevaluation metrics. For speech-to-text, the Word Error Rate (WER) is used to assess transcription\\naccuracy, where a lower WER signifies better performance. Evaluating text-to-speech involves\\nobjective metrics like the Mean Opinion Score (MOS) for naturalness and intelligibility, and the\\nSignal-to-Noise Ratio (SNR) for clarity, along with subjective human ratings. Text generation is the\\nmost difficult to evaluate, using metrics such as BLEU, ROUGE, and METEOR, which compare\\ngenerated text to reference texts but may not completely capture the quality and relevance of responses.\\nAssessing text generation often necessitates large-scale datasets, which are not always accessible.\\nThese metrics are widely adopted by the research community. Nevertheless, real-world applications\\nrequire evaluation in production environments, taking into account various factors beyond these\\nmetrics. For instance, a conversational agent designed for aged care should steer clear of sensitive\\ntopics that may be specific to each individual. Subjective opinions differ by region, emphasizing\\nthe necessity for adaptable and innovative automatic or semi-automatic evaluation methods for\\nconversational agents.\\n3 System Design\\n3.1 Requirement Analysis\\nThe system is designed to accept audio and video inputs and produce audio as output. Initially, two\\nmodules are required: one for gathering audio and video data from the microphone and camera, and\\nanother for emitting audio through a speaker. These Client modules must be compatible with a variety\\nof devices, such as smartphones, laptops, or Raspberry Pi. The data collected will be transmitted to a\\nserver.\\nThe server, known as the API, should handle audio and video data along with associated metadata.\\nIt should have access to a storage layer that includes a relational database, file management, and a\\ngraph database for potential GraphRAG integration. Although the API can be located on the same\\ndevice as the Client module, it is preferable to keep them separate for enhanced adaptability. This\\nseparation introduces the difficulty of transferring large volumes of data between modules. If the\\nAPI is cloud-based, audio and video data must be uploaded to the cloud, for instance, using AWS\\nS3, Azure Blob Storage, or Google Cloud Storage. However, the upload process can introduce a\\nbottleneck, making data transfer time-intensive. If the server is local, within the same network as the\\nClient, transfer latency will be reduced. Nevertheless, this configuration necessitates running the large\\nlanguage model locally, which addresses data ownership and privacy issues but may increase model\\ninference latency and reduce accuracy due to limited computational resources. Another approach is\\nedge computing, where video data is pre-processed on edge devices and summarized for the API.\\nAlthough this could be a research direction, data compression might result in information loss and\\ndecrease overall performance.\\n3The pipeline components will require adjustments if developers intend to adopt the framework and\\nintegrate it with their work. To maintain flexibility, this part should be an independent module capable\\nof running locally or in the cloud. Researchers and developers should be able to easily incorporate\\nnew components into this Agent module, further complicating the sharing of large datasets between\\nmodules.\\nFinally, benchmarks are needed to comprehend the latency and accuracy performance of the entire\\npipeline. For tasks that are challenging to evaluate automatically, such as assessing the appropriateness\\nof the LLM response, we propose and develop an annotation module to allow human annotators to\\neasily evaluate results and generate benchmark reports.\\n3.2 System Architecture\\nBased on these requirements, the system architecture was designed as depicted in Figure 1. The\\nsystem is divided into five modules: Client, API, Storage, User Interface, and Agent, all primarily\\ndeveloped in Python. The Client module includes two submodules: the Listener for collecting video\\nand audio data, and the Responder for playing audio. The Storage module consists of file storage for\\nmedia, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential\\nGraphRAG integration. The API module, built with the Django framework, extends Django\\u2019s admin\\ninterface and permission control system to develop the benchmark and annotation interface. Django\\u2019s\\nmaturity and large support community make it ideal for production development. The Agent module,\\nalso in Python, includes all agent-related submodules, allowing deployment on suitable compute\\nnodes without altering the architecture. Communication between the Client, API, and Agent modules\\nwill be via RESTful endpoints. For sharing large data between modules, local deployments (e.g.,\\nClient on Raspberry Pi, API and Agent on local servers) will use FTP for file synchronization. In\\ncloud solutions (e.g., AWS), files will be uploaded to AWS S3, triggering a Lambda function to\\ndownload files to an AWS Elastic File Storage (EFS) shared by the API and Agent modules. Docker\\nand Docker Compose are used to manage all modules, allowing easy setup with a single docker\\ncompose up command.\\n4 Demonstration\\n4.1 Datasets\\nMost multimodal question-answering datasets concentrate on multiple-choice questions rather than\\nopen-ended conversations. Some datasets involve multimodal conversations with images as additional\\ninput, but the output is often limited to multiple-choice or text. A significant challenge in developing\\nmultimodal conversational agents is the scarcity of suitable datasets.\\nAlthough there is an abundance of data from human-human interactions or data extracted from movies\\nand YouTube videos, efficient methods to organize this data into structured datasets are lacking. For\\nspecific domain applications, collecting data from human interactions and extracting datasets to train\\nsystems would be advantageous, enabling the agents to mimic human behavior. The OpenOmni\\nFramework offers both capabilities: extracting conversational datasets from videos and testing them\\nthrough the pipeline to assess agents\\u2019 responses, or gathering data from real-world scenarios to create\\ndatasets for further research.\\n4.2 Can \\\"AI\\\" be your president?\\nOne intensive conversational scenario is a debate. Segments were extracted from a US Presidential\\nDebate, focusing on a candidate addressing the public and handling questions. After downloading\\nthe videos, a prepared script in our codebase can be used to split them into segments. This script\\nallows for the specification of the start and end times of each conversation, enabling the creation\\nof a conversational dataset from the videos. These segments were fed into our pipeline to evaluate\\nits performance under different configurations: one using a commercial speech-to-text model, a\\nvision model, and text-to-speech (Configuration A); a locally deployed quantization LLM with a\\nspeech-to-text model, text-to-speech, and our emotion detection model for video input (Configuration\\nB); a version using a different LLM for inference (Configuration C); and a version using only a speech-\\nto-text model, a language model, and text-to-speech, ignoring the video modality (Configuration D).\\nThe Agent modules were run on a specific GPU with 12GB memory.\\n4The latency benchmark statistics are automatically generated. For example, Configuration A has\\nan average latency of 45 seconds, with the vision model accounting for 31 seconds. The fastest\\nconfiguration is Configuration D, averaging around 15 seconds, with most of the time consumed\\nby the text-to-speech part, because the generated content is quite long and comprehensive. The\\nslowest configuration is Configuration C, taking around 189 seconds, with the LLM model inference\\nstep taking the longest time. Configuration B takes an average of 60 seconds, with the LLM model\\ninference averaging 28 seconds and our emotion detection model averaging around 10 seconds.\\nTable 1: Accuracy: Overall Conversation Quality\\nTRACK ID USER ID OVERALL COMMENT OVERALL SCORE\\nf1 1 As the question is quite subjective, the answer is good and in context 4\\nf2 2 The answer is quite general, while the candidate is doing much better work with supported evidence. 2\\nf3 1 Failed to generate proper in-context response; the response is talking about how to respond, not actually responses 2\\nf4 1 Generate some general comments without strong support evidence 2\\nf5 1 General response, however, no good evidence to support. 3\\nAfter annotation with our interface, accuracy statistics are automatically generated. The accuracy\\nmetrics here include evaluation metrics like WER, CER for the speech-to-text task, and overall\\nscores given by the annotators. As shown in Table 1, the average score for each conversation is 2.4.\\nText-to-speech can be improved with more natural emotion or personality. The generated content\\nis often too general and sometimes inappropriate. The candidate\\u2019s responses are more in-context\\nand evidence-supported. The pipeline excelled only in answering a subjective question about the\\ncandidate\\u2019s age, where Configuration A performed well. Configuration D had the best overall\\naccuracy, but its responses were often in-context yet pompous. Thus, the candidate still outperforms\\nAI. In conclusion, \\\"AI cannot be the President of the US just yet, considering both latency and\\naccuracy.\\\"\\n4.3 Assist the Visually Impaired\\nWhile latency and the need for external information currently prevent AI from undertaking mission-\\ncritical tasks, conversational agents can be production-ready and useful for non-latency-critical areas\\nthat do not require extensive external knowledge. Assisting indoor activities for the visually impaired\\nis one such application, where high-speed internet can be utilized, or data transfer can be limited to\\nlocal exchanges. These types of applications can benefit from maintaining high input/output rates,\\nhelping to mitigate latency issues. Questions were prepared for the visually impaired, including\\nlocating objects, navigating indoors, and inquiries about the surroundings. Six questions were\\nsampled and fed to the Configuration A pipeline. One scenario demonstration is included in our\\nprovided video. In this scenario, video and audio data stream from the client side and are saved to\\nstorage along with exportable metadata accessible via the admin portal. This setup allows for the\\nexportation of annotated datasets, including raw video and audio data, for developing new models.\\nThe latency statistics show responses within approximately 30 seconds.\\nAnnotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually\\nimpaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee\\ncup rather than just a general description. This indicates that while conversational agents are nearly\\nready for assisting the visually impaired with indoor activities, improvements in latency and response\\nquality are still needed.\\n5 Conclusion\\nMultimodal conversational agents offer a more natural form of human-computer interaction, as\\ndemonstrated by models like GPT-4o. However, real-world constraints require a balance between\\ncost, latency, and accuracy, which may explain why the full capabilities of such models are not yet\\naccessible.\\nSeveral technical options exist to achieve this balance, including traditional divide-and-conquer\\nmethods, fully end-to-end models, and hybrid approaches. The fully end-to-end approach inherently\\nallows for lower latency, while the divide-and-conquer method faces latency issues when coordinating\\n5multiple components. Both approaches must address the challenge of handling large data I/O. If\\nmodels are deployed locally, local network I/O issues can be more manageable. However, some\\nmodels are closed-source, making local deployment impractical. While deploying other vision models\\nlocally is feasible, achieving high accuracy may be limited by local computational resources. Hybrid\\nsolutions provide alternative approaches: pre-processing or compressing large data locally and then\\nutilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice\\nmodel.\\nWe developed the OpenOmni framework to enable researchers to integrate their work into an end-to-\\nend pipeline. The framework supports various solutions, allows for pipeline customization, generates\\nlatency performance reports, and provides an annotation interface for accuracy review. These features\\nfacilitate the creation of benchmark reports to identify and address key issues.\\nTesting with the US Presidential debate scenario highlighted latency as a critical issue, particularly\\nwith large video data. Integrating external knowledge remains a challenge, emphasizing the need\\nfor efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the\\nvisually impaired, latency improvements and model adaptation are both essential.\\nThe OpenOmni framework can significantly benefit the research community by facilitating the\\ncollection and management of new datasets, integrating various conversational agents approaches,\\nand generating automatic latency benchmarks. Its annotation interface aids in accuracy performance\\nreview, making OpenOmni production-ready for suitable application scenarios and fostering further\\ndevelopment in multimodal conversational agents.\\n6\",\n          \"An Investigation into Named Entity Recognition for\\nCall Center Transcripts to Ensure Privacy Law\\nCompliance\\nAbstract\\nThis study explores the application of Named Entity Recognition (NER) on a\\nnovel form of user-generated text, specifically call center conversations. These\\ndialogues present unique challenges, blending the complexities of spontaneous\\nspeech with issues specific to conversational Automatic Speech Recognition (ASR),\\nsuch as inaccuracies. By employing a custom corpus with manual annotations,\\ntraining contextual string embeddings, and implementing a BiLSTM-CRF model,\\nwe achieve results that are on par with the state-of-the-art for this new task.\\n1 Introduction\\nThis paper addresses the crucial need to identify and handle sensitive personal information within\\ncall center transcripts, which are generated as a result of speech recognition systems. Although these\\ntranscripts are typically redacted for Payment Card Industry (PCI) compliance, they still often contain\\na caller\\u2019s name and internal ID number, which can be useful for quality assurance. However, new\\nprivacy laws, such as the General Data Protection Regulation (GDPR) in the EU, establish stringent\\nguidelines concerning data collection, storage, and an individual\\u2019s right to withdraw consent for\\ndata usage. To adhere to these regulations without losing the data\\u2019s value, it is essential to pinpoint\\nnon-public personal and personally identifiable information (NPI/PII) in call transcripts.\\nWe utilize Named Entity Recognition (NER) to locate instances of NPI/PII within the transcripts,\\nremove them, and replace them with appropriate tags that denote the type of removed data. For\\ninstance, a transcript such as \\\"This is john doe reference number 12345\\\" would be transformed into\\n\\\"This is [NAME] reference number [NUMBER]\\\". This task is distinctive to call centers for several\\nreasons. First, these transcripts consist of natural human conversations, which have many common\\nproblems of user-generated content such as incomplete sentences and unusual words. Furthermore,\\ntranscript text is produced by Automatic Speech Recognition (ASR) systems, which are susceptible to\\nerrors, as will be described in Section 3.1. Even though modern ASR systems are usually reliable, the\\nsource audio is from phone calls, which is often low quality and contains background noise. The poor\\naudio quality leads to incorrect ASR, producing ungrammatical sentences. This makes understanding\\nthe call semantics and identifying features essential to NER systems more difficult. Moreover, call\\ntranscripts frequently lack capitalization, numeric digits, and proper punctuation, which are crucial\\nfeatures for classic NER methods. Also, traditional NER systems are inadequate for handling emails,\\naddresses, or spellings, which makes it difficult to use pre-trained NER models.\\nIn this paper, we apply the current best neural network architecture for sequence labeling, a BiLSTM-\\nCRF, to the task of identifying NPI and PII in call transcripts. We match the state-of-the-art perfor-\\nmance on standard datasets by using our model with annotated data and custom contextual string\\nembeddings.2 Related Work\\nNamed Entity Recognition has become a focus in the field of Natural Language Processing (NLP),\\nparticularly since the Message Understanding Conferences (MUCs) in the 1990s. The CoNLL2003\\nshared task in 2003 concentrated on language-independent NER and popularized feature based\\nsystems. The OntoNotes corpus, released in 2006, has been vital to the progress of NER research.\\nFollowing the CoNLL task, Conditional Random Field (CRF) based models became the most\\nsuccessful, which requires that features be manually produced. Current research utilizes neural\\nnetworks to generate these features. Bidirectional Long Short Term Memory models with a CRF layer\\n(BiLSTM-CRF) have been used successfully on CoNLL2000 and CoNLL2003 datasets. A BiLSTM-\\nCNN-CRF has been used for NER on the CoNLL2003 dataset, producing superior results. Similar\\nresults were achieved by a BiLSTM-CNN with features from word embeddings and the lexicon.\\nEmbeddings have been used for both words and entity types to create more robust models. Flair, with\\ncharacter-based embeddings and a pooling approach, has set the state of the art. Crossweigh uses\\nFlair embeddings to address mishandled annotations.\\nIn 2006, the word confidence scores from ASR systems were used as a feature for NER. Similar\\nexperiments were done on French radio and TV audio. Neither of those used natural conversation,\\nand the quality of the audio was superior, making ASR a more accurate task.\\n2.1 Conversations are Different: The Twitter Analogy\\nMuch of the past research has used newswire datasets. While newswire data is expected to conform\\nto standard text conventions, call center transcripts do not have these conventions. This presents a\\nproblem for the usual approaches to NER and is further complicated by our poor audio quality.\\nSpeaker 1: Thank you for calling our company how may i help you today.\\nSpeaker 2: Id like to pay my bill.\\nTable 1: An example of turns of a conversation, where each person\\u2019s line in the dialogue represents\\ntheir turn. This output matches the format of our data described in Section 3.\\nThe most similar research area to this is work on Twitter data. Similar to our transcripts, tweets are\\nuser-generated and may not have conventional grammar or spelling. Initial research tackled this\\nproblem with a K-nearest neighbors model combined with a CRF. A model combining a multi-step\\nneural network with a CRF output layer achieved first place in the 2017 Workshop on Noisy User-\\ngenerated Text (W-NUT). The success of pooled contextualized string embeddings was also shown\\nwith this data. We use prior work on tweets to direct our model creation for call center data.\\n3 Data\\nOur dataset includes 7,953 training, 500 validation, and 534 test samples. Each sample represents\\na complete speaker turn from a debt collection call center. A speaker turn is defined as a complete\\ntranscription from one speaker before another speaker starts, as shown in Table 1. The training set is\\na random sample of turns from 4 months of call transcripts. The transcripts were generated using a\\nproprietary speech recognition system, which outputs all lowercase transcripts without punctuation\\nor numeric digits. We used spaCy to convert each turn to a document that begins with a capital letter\\nand ends with a period, as this is the default for spaCy. In order to make use of entities, a Sentencizer\\nmodule was added, which defaults to this capitalization and period structure.\\n3.1 Data Annotation\\nWe created a schema for annotating the training and validation data with different types of NPI/PII,\\nwhich are shown in Table 2.\\nInitial annotations were performed using Doccano. The annotators were trained in NPI/PII recognition,\\nand were instructed to err on the side of caution in unclear instances. Ambiguity often came from\\nerrors in the ASR model. The lack of audio meant it was sometimes unclear if \\\"I need oak leaves\\\"\\nwas actually \\\"Annie Oakley\\\". The opposite was also true such as when \\\"Brilliant and wendy jeff to\\n2Entity Type Description\\nNUMBERS A sequence of numbers related to a customer\\u2019s information (e.g. phone numbers or internal ID number)\\nNAME First and last name of a customer or agent\\nCOMPANY The name of a company\\nADDRESS A complete address, including city, state, and zip code\\nEMAIL Any email address\\nSPELLING Language that clarifies the spelling of a word (e.g. \\\"c as in cat\\\")\\nTable 2: A brief description of our annotation schema.\\nprocess the refund\\\" was actually \\\"Brilliant and when did you want to process the refund\\\". Emails\\nwere also difficult, as errors in ASR made it difficult to determine the bounds of the email address.\\nAlso, the transcripts were pre-redacted for PCI compliance. This redaction can obscure important\\ndata, for example, sometimes a customer ID is redacted as part of the PCI redaction process. To\\nlessen false negatives, we use context to include the [redacted] tag as part of the numbers sequence\\nwhen possible. No steps to clean the transcripts were taken; the natural noise in the data was left for\\nthe model to interpret.\\nDue to limitations with spaCy and the complexity of nested entities, we only allowed one annotation\\nper word in the dataset. This means, for instance, that \\\"c a t as in team at gmail dot com\\\" would be\\nlabeled either as SPELLING[0:6] EMAIL[6:] or as EMAIL[0:] with the indices corresponding to the\\nposition of words in the text. This ultimately results in a lower count of SPELLING entities, because\\nthese are often part of EMAIL or ADDRESS entities, which influences our analysis in Section 6.\\n4 Model Design\\nWe utilized a standard BiLSTM-CRF model in PyTorch, adapted from a GitHub repository. We wrote\\nour own main.py to use our spaCy preprocessing, and adapted the code to handle batch processing.\\nAfter preprocessing, we trained the model on the training set and used the validation set for model\\ntuning. All numbers in this paper are reported on the test set. A visualization of our model is shown\\nin Figure 1.\\n5 Experiments\\n5.1 Basic Hyperparameter Tuning\\nWe used a grid search algorithm to maximize model performance. The word embedding layer uses\\nFastText embeddings trained on the client\\u2019s call transcripts. This aids in mitigating the impacts of\\npoor ASR, and this will be explored in Sections 5.2 and 5.3. The grid search included the parameters:\\nepochs (a sampled distribution between 5 and 50), the size of a dropout layer (between 0 and 0.5,\\nwith 0.1 intervals of search), the number of hidden layers (between 5 and 20 in increments of 5), and\\nthe encoding type used in the output of the CRF (BIO, BILOU, IO). The other hyperparameters were\\na learning rate of .001, a batch size of 1, 30 nodes in each fully connected layer, and the inclusion of\\nbias in each layer. The experiments were run in parallel on a virtual machine with 16 CPUs and 128\\nGB of memory. Each experiment took a few hours to run.\\nTo understand the performance of the model, we broke down the measurements of precision, recall,\\nand F1 by entity type. Table 3 shows these results for the best model configuration. This model used\\n46 epochs, a dropout rate of 0.2, 5 hidden layers, and a BIO encoding.\\n5.2 Training Word Embeddings\\nMost past research has fine-tuned existing word embeddings, but the task of mitigating misrecognition\\nseemed more complex than domain adaptation. To lessen the impact of the errors, we understand that\\nfrequent misrecognitions appear in contexts similar to the intended word. A custom model gives a\\nmisrecognized word a vector similar to the word it should be and not to the other meaning it has. The\\nimportance of domain specific word embeddings when using ASR data has been shown in research.\\n3We ran our best performing model with the 300 dimensional GloVe 6b word embeddings. Our\\nembeddings were trained on roughly 216 million words. The results from the best epoch of this\\nmodel (16) are shown in Table 3.\\n2*Entity Type Precision Recall F1\\nCustom GloVe Custom GloVe Custom GloVe\\nO 89.8 84.2 81.7 76.6 85.6 80.2\\nNUMBERS 95.6 88.7 85.4 82.9 90.1 85.7\\nNAME 89.6 92.1 91.1 88.7 90.3 90.3\\nCOMPANY 98.8 99.5 72.9 64.3 83.9 78.1\\nADDRESS 70.6 0.3 75.0 18.7 72.7 23\\nEMAIL 0 07.1 0 03.1 0 04.4\\nSPELLING 45.8 34 52.4 40.5 48.9 37.0\\nMicro Average 89.2 85.6 79.6 74.0 84.1 79.4\\nTable 3: The performance by entity type of the BiLSTM-CRF model on the held out test set. This table\\ncompares the results of our custom embeddings model (\\\"Custom\\\") against the GloVe embeddings\\n(\\\"GloVe\\\").\\n5.3 Using Flair\\nPrevious experiments highlighted the importance of custom word embeddings to account for mis-\\nrecognition in call center transcripts. Here, we test the performance of Flair and its contextual string\\nembeddings.\\nWe begin by training custom contextual string embeddings based on the results of the first experiments.\\nWe use the same corpus as in Section 5.1. The tutorial on the Flair GitHub page was used with the\\nfollowing parameters: hidden size: 1024, sequence length: 250, mini batch size: 100. We use the\\nnewline to indicate a document change, and each turn as a separate document for consistency. The\\nmodel\\u2019s validation loss stabilized after epoch 4, and the best version of the model was used.\\nWe conduct experiments using Flair\\u2019s SequenceTagger with default parameters and a hidden size of\\n256.\\nFlair uses only the custom trained Flair embeddings.\\nFlair + FastText uses the custom trained Flair embeddings and the custom trained FastText embeddings\\nusing Flair\\u2019s StackedEmbeddings.\\nFlairmean pooling uses only the custom trained Flair embeddings within Flair\\u2019s PooledFlairEmbed-\\nding. Mean pooling was used.\\nFlairmean pooling + FastText uses PooledFlairEmbeddings with mean pooling and the custom trained\\nFastText embeddings using Flair\\u2019s StackedEmbeddings.\\nThese results are shown in Table 4.\\nEntity Flair Flair + FastText Flairmean pooling Flairmean pooling + FastText\\nO 98.3 98.5 98.2 98.5\\nNUMBERS 83.1 87.9 87.7 86.2\\nCOMPANY 81.1 80.7 80.7 80.3\\nADDRESS 87.5 94.1 61.5 94.1\\nEMAIL 58.8 50.0 73.3 66.7\\nSPELLING 55.0 57.1 55.8 57.9\\nMicro Average 97.5 97.7 97.3 97.7\\nTable 4: The F1 scores on the test set for each entity type for each Flair embedding experiment.\\n46 Discussion\\nTable 3 shows that using custom embeddings is beneficial over using GloVe embeddings, with the\\nexception of the EMAIL category. The Flair embeddings show a large improvement over other word\\nembeddings; however all four varieties of Flair models have nearly identical Micro Average F1s. The\\nbest performing Flair models are those that use both the custom contextualized string embeddings\\nand the custom FastText embeddings.\\nAcross all of the models in this paper, EMAIL and SPELLING consistently performed worse than\\nother categories. This is due to the overlap in their occurrences and their variable appearance. The\\ncustom embeddings model often identified parts of an email correctly but labeled some aspects, such\\nas a name, as NAME followed by EMAIL instead of labeling the whole thing as EMAIL. SPELLING\\noften appears within an EMAIL entity. Due to the previously discussed limitations, the SPELLING\\nentity had a limited presence in our training data, with many EMAIL and ADDRESS entities\\ncontaining examples of SPELLING. All models frequently misidentified EMAIL as SPELLING and\\nvice versa. Additionally, the test data had a number of turns that consisted of only SPELLING, which\\nwas poorly represented in training. The Flairmean pooling model outperforms the other models in\\nEMAIL by a large margin.\\nThe results in Table 4 highlight that the NUMBERS category contains strings that appear frequently\\nin the text. There are a finite number of NUMBER words in our corpus (those numeric words along\\nwith many instances of \\\"[redacted]\\\"), and the numbers of interest in our dataset appear in very similar\\ncontexts and do not often get misrecognized. The COMPANY entity performs well for similar\\nreasons; when the model was able to identify the company name correctly, it was often in a common\\nerror form and in a known context. The model\\u2019s failures can be attributed to the training data because\\nthe company name is a proper noun that is not in standard ASR language models, including the one\\nwe used. Thus, it is often misrecognized since the language model has higher probabilities assigned to\\ngrammatically correct phrases that have nothing to do with the company name. This causes variability\\nin appearance, which means that not every version of the company name was present in our training\\nset.\\nInteresting variability also occurred in ADDRESS entities. Both models that used Flair and FastText\\nembeddings strongly outperformed the models that used only Flair, and standard Flair embeddings\\nstrongly outperformed the Pooled Flair embeddings. Neither version of the Flair-only model identified\\naddresses in which numbers were shown as \\\"[redacted]\\\" but both models that utilized FastText had\\nno issue with these instances.\\n7 Conclusion and Future Work\\nThrough the use of a BiLSTM-CRF model, paired with custom-trained Flair embeddings, we achieve\\nstate-of-the-art NER performance on a new call center conversation dataset with distinct entity types.\\nWe also show the importance of training word embeddings that fully capture the intricacies of the\\ntask. Although we cannot release our data for privacy, we have shown that existing state-of-the-art\\ntechniques can be applied to less common datasets and tasks. Future work will include evaluating\\nthe model with call transcripts from other industries. We would also like to explore how well these\\ntechniques work on other user-generated conversations like chats and emails.\\n5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Publishable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_test.to_csv('/content/Papers_test_prediction.csv',index=False,escapechar='\\\\')"
      ],
      "metadata": {
        "id": "TcB8Np04K3Qe"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_zeros = pdf_test['Label'].value_counts().get(1, 0)  # Count occurrences of 0, default to 0 if not found\n",
        "num_ones = pdf_test['Label'].value_counts().get(0, 0)  # Count occurrences of 1, default to 0 if not found\n",
        "\n",
        "print(f\"Number of 0's: {num_zeros}\")\n",
        "print(f\"Number of 1's: {num_ones}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8lfJh8gJhU0",
        "outputId": "6197f661-7329-4920-8103-425e743f6f0d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 0's: 2\n",
            "Number of 1's: 133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-8a566ea9def5>:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  num_zeros = pdf_test['Label'].value_counts().get(1, 0)  # Count occurrences of 0, default to 0 if not found\n",
            "<ipython-input-60-8a566ea9def5>:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  num_ones = pdf_test['Label'].value_counts().get(0, 0)  # Count occurrences of 1, default to 0 if not found\n"
          ]
        }
      ]
    }
  ]
}